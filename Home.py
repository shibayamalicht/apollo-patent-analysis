import streamlit as st
import pandas as pd
import numpy as np
import datetime
import warnings
import unicodedata
import re
import traceback

from sentence_transformers import SentenceTransformer
from janome.tokenizer import Tokenizer
from sklearn.feature_extraction.text import TfidfVectorizer

# è­¦å‘Šã‚’éè¡¨ç¤º
warnings.filterwarnings('ignore')

# ==================================================================
# --- 1. ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° & ãƒªã‚½ãƒ¼ã‚¹ãƒ­ãƒ¼ãƒ‰ ---
# ==================================================================

@st.cache_resource
def load_sbert_model():
    """SBERTãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ãŠã‚ˆã³ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
    return SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

@st.cache_resource
def load_tokenizer():
    """Janome Tokenizerã‚’ãƒ­ãƒ¼ãƒ‰ãŠã‚ˆã³ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
    return Tokenizer()

# Tokenizerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
t = load_tokenizer()

# åˆ†æç”¨ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰å®šç¾©
STOP_WORDS = {
    "ã™ã‚‹","ã‚ã‚‹","ãªã‚‹","ãŸã‚","ã“ã¨","ã‚ˆã†","ã‚‚ã®","ã“ã‚Œ","ãã‚Œ","ã‚ã‚Œ","ã“ã“","ãã“","ã©ã‚Œ","ã©ã®","ã“ã®","ãã®","å½“è©²","è©²","ãŠã‚ˆã³","åŠã³","ã¾ãŸã¯","ã¾ãŸ","ä¾‹ãˆã°","ä¾‹ãˆã°ã¯","ã«ãŠã„ã¦","ã«ã‚ˆã‚Š","ã«å¯¾ã—ã¦","ã«é–¢ã—ã¦","ã«ã¤ã„ã¦","ã¨ã—ã¦","ã¨ã—ã¦ã¯","å ´åˆ","ä¸€æ–¹","ä»–æ–¹","ã•ã‚‰ã«","ãã—ã¦","ãŸã ã—","ãªãŠ","ç­‰","ãªã©","ç­‰ã€…","ã„ã‚ã‚†ã‚‹","æ‰€è¬‚","åŒæ§˜","åŒæ™‚","å‰è¨˜","æœ¬","åŒ","å„","å„ç¨®","æ‰€å®š","æ‰€æœ›","ä¸€ä¾‹","ä»–","ä¸€éƒ¨","ä¸€ã¤","è¤‡æ•°","å°‘ãªãã¨ã‚‚","å°‘ãªãã¨ã‚‚ä¸€ã¤","ä¸Šè¨˜","ä¸‹è¨˜","å‰è¿°","å¾Œè¿°","æ—¢è¿°","é–¢ã™ã‚‹","åŸºã¥ã","ç”¨ã„ã‚‹","ä½¿ç”¨","åˆ©ç”¨","æœ‰ã™ã‚‹","å«ã‚€","å‚™ãˆã‚‹","è¨­ã‘ã‚‹","ã™ãªã‚ã¡","å¾“ã£ã¦","ã—ã‹ã—ãªãŒã‚‰","æ¬¡ã«","ç‰¹ã«","å…·ä½“çš„ã«","è©³ç´°ã«","ã„ãšã‚Œ","ã†ã¡","ãã‚Œãã‚Œ","ã¨ã","ã‹ã‹ã‚‹","ã‹ã‚ˆã†ãª","ã‹ã‹ã‚‹å ´åˆ","æœ¬ä»¶","æœ¬é¡˜","æœ¬å‡ºé¡˜","æœ¬æ˜ç´°æ›¸",
    "æœ¬ç™ºæ˜","ç™ºæ˜","å®Ÿæ–½ä¾‹","å®Ÿæ–½å½¢æ…‹","å¤‰å½¢ä¾‹","è«‹æ±‚","è«‹æ±‚é …","å›³","å›³é¢","ç¬¦å·","ç¬¦å·ã®èª¬æ˜","å›³é¢ã®ç°¡å˜ãªèª¬æ˜","ç™ºæ˜ã®è©³ç´°ãªèª¬æ˜","æŠ€è¡“åˆ†é‡","èƒŒæ™¯æŠ€è¡“","å¾“æ¥æŠ€è¡“","ç™ºæ˜ãŒè§£æ±ºã—ã‚ˆã†ã¨ã™ã‚‹èª²é¡Œ","èª²é¡Œ","è§£æ±ºæ‰‹æ®µ","åŠ¹æœ","è¦ç´„","ç™ºæ˜ã®åŠ¹æœ","ç›®çš„","æ‰‹æ®µ","æ§‹æˆ","æ§‹é€ ","å·¥ç¨‹","å‡¦ç†","æ–¹æ³•","æ‰‹æ³•","æ–¹å¼","ã‚·ã‚¹ãƒ†ãƒ ","ãƒ—ãƒ­ã‚°ãƒ©ãƒ ","è¨˜æ†¶åª’ä½“","ç‰¹å¾´","ç‰¹å¾´ã¨ã™ã‚‹","ç‰¹å¾´éƒ¨","ã‚¹ãƒ†ãƒƒãƒ—","ãƒ•ãƒ­ãƒ¼","ã‚·ãƒ¼ã‚±ãƒ³ã‚¹","å®šç¾©","é–¢ä¿‚","å¯¾å¿œ","æ•´åˆ", "å®Ÿæ–½ã®å½¢æ…‹","å®Ÿæ–½ã®æ…‹æ§˜","æ…‹æ§˜","å¤‰å½¢","ä¿®æ­£ä¾‹","å›³ç¤º","å›³ç¤ºä¾‹","å›³ç¤ºã—ãªã„","å‚ç…§","å‚ç…§ç¬¦å·","æ®µè½","è©³ç´°èª¬æ˜","è¦æ—¨","ä¸€å®Ÿæ–½å½¢æ…‹","ä»–ã®å®Ÿæ–½å½¢æ…‹","ä¸€å®Ÿæ–½ä¾‹","åˆ¥ã®å´é¢","ä»˜è¨˜","é©ç”¨ä¾‹","ç”¨èªã®å®šç¾©","é–‹ç¤º","æœ¬é–‹ç¤º","é–‹ç¤ºå†…å®¹",
    "ä¸Šéƒ¨","ä¸‹éƒ¨","å†…éƒ¨","å¤–éƒ¨","å†…å´","å¤–å´","è¡¨é¢","è£é¢","å´é¢","ä¸Šé¢","ä¸‹é¢","ç«¯é¢","å…ˆç«¯","åŸºç«¯","å¾Œç«¯","ä¸€ç«¯","ä»–ç«¯","ä¸­å¿ƒ","ä¸­å¤®","å‘¨ç¸","å‘¨è¾º","è¿‘å‚","æ–¹å‘","ä½ç½®","ç©ºé–“","é ˜åŸŸ","ç¯„å›²","é–“éš”","è·é›¢","å½¢çŠ¶","å½¢æ…‹","çŠ¶æ…‹","ç¨®é¡","å±¤","è†œ","éƒ¨","éƒ¨æ","éƒ¨ä½","éƒ¨å“","æ©Ÿæ§‹","è£…ç½®","å®¹å™¨","çµ„æˆ","ææ–™","ç”¨é€”","é©ç”¨","é©ç”¨ä¾‹","ç‰‡å´","ä¸¡å´","å·¦å´","å³å´","å‰æ–¹","å¾Œæ–¹","ä¸Šæµ","ä¸‹æµ","éš£æ¥","è¿‘æ¥","é›¢é–“","é–“ç½®","ä»‹åœ¨","é‡ç•³","æ¦‚ã­","ç•¥","ç•¥ä¸­å¤®","å›ºå®šå´","å¯å‹•å´","ä¼¸é•·","åç¸®","ä¿‚åˆ","åµŒåˆ","å–ä»˜","é€£çµéƒ¨","æ”¯æŒä½“","æ”¯æŒéƒ¨","ã‚¬ã‚¤ãƒ‰éƒ¨",
    "ãƒ‡ãƒ¼ã‚¿","æƒ…å ±","ä¿¡å·","å‡ºåŠ›","å…¥åŠ›","åˆ¶å¾¡","æ¼”ç®—","å–å¾—","é€ä¿¡","å—ä¿¡","è¡¨ç¤º","é€šçŸ¥","è¨­å®š","å¤‰æ›´","æ›´æ–°","ä¿å­˜","å‰Šé™¤","è¿½åŠ ","å®Ÿè¡Œ","é–‹å§‹","çµ‚äº†","ç¶™ç¶š","åœæ­¢","åˆ¤å®š","åˆ¤æ–­","æ±ºå®š","é¸æŠ","ç‰¹å®š","æŠ½å‡º","æ¤œå‡º","æ¤œçŸ¥","æ¸¬å®š","è¨ˆæ¸¬","ç§»å‹•","å›è»¢","å¤‰ä½","å¤‰å½¢","å›ºå®š","é…ç½®","ç”Ÿæˆ","ä»˜ä¸","ä¾›çµ¦","é©ç”¨","ç…§åˆ","æ¯”è¼ƒ","ç®—å‡º","è§£æ","åŒå®š","åˆæœŸåŒ–","èª­å‡º","æ›¸è¾¼","ç™»éŒ²","è¨˜éŒ²","é…ä¿¡","é€£æº","åˆ‡æ›¿","èµ·å‹•","å¾©å¸°","ç›£è¦–","é€šçŸ¥å‡¦ç†","å–å¾—å‡¦ç†","æ¼”ç®—å‡¦ç†",
    "è‰¯å¥½","å®¹æ˜“","ç°¡ä¾¿","é©åˆ‡","æœ‰åˆ©","æœ‰ç”¨","æœ‰åŠ¹","åŠ¹æœçš„","é«˜ã„","ä½ã„","å¤§ãã„","å°ã•ã„","æ–°è¦","æ”¹è‰¯","æ”¹å–„","æŠ‘åˆ¶","å‘ä¸Š","ä½æ¸›","å‰Šæ¸›","å¢—åŠ ","æ¸›å°‘","å¯èƒ½","å¥½é©","å¥½ã¾ã—ã„","æœ›ã¾ã—ã„","å„ªã‚Œã‚‹","å„ªã‚ŒãŸ","é«˜æ€§èƒ½","é«˜åŠ¹ç‡","ä½ã‚³ã‚¹ãƒˆ","ã‚³ã‚¹ãƒˆ","ç°¡æ˜“","å®‰å®š","å®‰å®šæ€§","è€ä¹…","è€ä¹…æ€§","ä¿¡é ¼æ€§","ç°¡ç´ ","ç°¡ç•¥","å˜ç´”","æœ€é©","æœ€é©åŒ–","æ±ç”¨","æ±ç”¨æ€§","å®Ÿç¾","é”æˆ","ç¢ºä¿","ç¶­æŒ","é˜²æ­¢","å›é¿","ä¿ƒé€²","ä¸è¦","å¿…è¦","é«˜ç²¾åº¦","çœé›»åŠ›","çœè³‡æº","é«˜ä¿¡é ¼","ä½è² è·","é«˜ç´”åº¦","é«˜å¯†åº¦","é«˜æ„Ÿåº¦","è¿…é€Ÿ","å††æ»‘","ç°¡ç•¥åŒ–","ä½ä¾¡æ ¼","å®ŸåŠ¹çš„","å¯èƒ½åŒ–","æœ‰åŠ¹åŒ–","éå¿…é ˆ","é©åˆ","äº’æ›",
    "å‡ºé¡˜","å‡ºé¡˜äºº","å‡ºé¡˜ç•ªå·","å‡ºé¡˜æ—¥","å‡ºé¡˜æ›¸","å‡ºé¡˜å…¬é–‹","å…¬é–‹","å…¬é–‹ç•ªå·","å…¬é–‹å…¬å ±","å…¬å ±","å…¬å ±ç•ªå·","ç‰¹è¨±","ç‰¹è¨±ç•ªå·","ç‰¹è¨±æ–‡çŒ®","éç‰¹è¨±æ–‡çŒ®","å¼•ç”¨","å¼•ç”¨æ–‡çŒ®","å…ˆè¡ŒæŠ€è¡“","å¯©æŸ»","å¯©æŸ»å®˜","æ‹’çµ¶","æ„è¦‹æ›¸","è£œæ­£æ›¸","å„ªå…ˆ","å„ªå…ˆæ—¥","åˆ†å‰²å‡ºé¡˜","ç¶™ç¶šå‡ºé¡˜","å›½å†…ç§»è¡Œ","å›½éš›å‡ºé¡˜","å›½éš›å…¬é–‹","PCT","ç™»éŒ²","å…¬é–‹æ—¥","å¯©æŸ»è«‹æ±‚","æ‹’çµ¶ç†ç”±","è£œæ­£","è¨‚æ­£","ç„¡åŠ¹å¯©åˆ¤","ç•°è­°","å–æ¶ˆ","å–ä¸‹ã’","äº‹ä»¶ç•ªå·","ä»£ç†äºº","å¼ç†å£«","ä¿‚å±","çµŒé",
    "ç¬¬","ç¬¬ä¸€","ç¬¬äºŒ","ç¬¬ä¸‰","ç¬¬1","ç¬¬ï¼’","ç¬¬ï¼“","ç¬¬ï¼‘","ç¬¬ï¼’","ç¬¬ï¼“","ä¸€","äºŒ","ä¸‰","å››","äº”","å…­","ä¸ƒ","å…«","ä¹","é›¶","æ•°","è¤‡åˆ","å¤šæ•°","å°‘æ•°","å›³1","å›³2","å›³3","å›³4","å›³5","å›³6","å›³7","å›³8","å›³9","è¡¨1","è¡¨2","è¡¨3","å¼1","å¼2","å¼3",
    "%","ï¼…","wt%","vol%","è³ªé‡%","é‡é‡%","å®¹é‡%","mol","mol%","mol/L","M","mm","cm","m","nm","Î¼m","Î¼","rpm","Pa","kPa","MPa","GPa","N","W","V","A","mA","Hz","kHz","MHz","GHz","â„ƒ","Â°C","K","mL","L","g","kg","mg","wt","vol","h","hr","hrs","min","s","sec","ppm","ppb","bar","Î©","ohm","J","kJ","Wh","kWh",
    "æ ªå¼ä¼šç¤¾","æœ‰é™ä¼šç¤¾","åˆè³‡ä¼šç¤¾","åˆåä¼šç¤¾","åˆåŒä¼šç¤¾","Inc","Inc.","Ltd","Ltd.","Co","Co.","Corp","Corp.","LLC", "GmbH","AG","BV","B.V.","S.A.","S.p.A.","ï¼ˆæ ªï¼‰","ãˆ±","ï¼ˆæœ‰ï¼‰",
    "æº¶æ¶²","æº¶åª’","è§¦åª’","åå¿œ","ç”Ÿæˆç‰©","åŸæ–™","æˆåˆ†","å«æœ‰","å«æœ‰é‡","é…åˆ","æ··åˆ","æ··åˆç‰©","æ¿ƒåº¦","æ¸©åº¦","æ™‚é–“","å‰²åˆ","æ¯”ç‡","åŸº","å®˜èƒ½åŸº","åŒ–åˆç‰©","çµ„æˆç‰©","æ¨¹è„‚","ãƒãƒªãƒãƒ¼","ãƒ¢ãƒãƒãƒ¼","åŸºæ¿","åŸºæ","ãƒ•ã‚£ãƒ«ãƒ ","ã‚·ãƒ¼ãƒˆ","ç²’å­","ç²‰æœ«","æ¯”è¼ƒä¾‹","å‚è€ƒä¾‹","è©¦é¨“","è©¦æ–™","è©•ä¾¡","æ¡ä»¶","å®Ÿé¨“","å®Ÿé¨“ä¾‹","åå¿œæ¡ä»¶","åå¿œæ™‚é–“","åå¿œæ¸©åº¦",
    "å‡¦ç†è£…ç½®","ç«¯æœ«","ãƒ¦ãƒ‹ãƒƒãƒˆ","ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«","å›è·¯","ç´ å­","é›»æº","é›»åœ§","é›»æµ","ä¿¡å·ç·š","é…ç·š","ç«¯å­","ç«¯éƒ¨","æ¥ç¶š", "æ¥ç¶šéƒ¨","æ¼”ç®—éƒ¨","è¨˜æ†¶éƒ¨","è¨˜æ†¶è£…ç½®","è¨˜éŒ²åª’ä½“","ãƒ¦ãƒ¼ã‚¶","åˆ©ç”¨è€…","ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ","ã‚µãƒ¼ãƒ","ç”»é¢","UI","GUI","ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹","ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹","DB","ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯","é€šä¿¡","è¦æ±‚","å¿œç­”","ãƒªã‚¯ã‚¨ã‚¹ãƒˆ","ãƒ¬ã‚¹ãƒãƒ³ã‚¹","ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿","å¼•æ•°","å±æ€§","ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£","ãƒ•ãƒ©ã‚°","ID","ãƒ•ã‚¡ã‚¤ãƒ«","ãƒ‡ãƒ¼ã‚¿æ§‹é€ ","ãƒ†ãƒ¼ãƒ–ãƒ«","ãƒ¬ã‚³ãƒ¼ãƒ‰",
    "è»¸","ã‚·ãƒ£ãƒ•ãƒˆ","ã‚®ã‚¢","ãƒ¢ãƒ¼ã‚¿","ã‚¨ãƒ³ã‚¸ãƒ³","ã‚¢ã‚¯ãƒãƒ¥ã‚¨ãƒ¼ã‚¿","ã‚»ãƒ³ã‚µ","ãƒãƒ«ãƒ–","ãƒãƒ³ãƒ—","ç­ä½“","ãƒã‚¦ã‚¸ãƒ³ã‚°","ãƒ•ãƒ¬ãƒ¼ãƒ ","ã‚·ãƒ£ãƒ¼ã‚·","é§†å‹•","ä¼é”","æ”¯æŒ","é€£çµ"
}

def extract_ipc(text, delimiter=';'):
    """IPCã‚³ãƒ¼ãƒ‰ã‚’æ­£è¦åŒ–ã—ã¦ãƒªã‚¹ãƒˆã¨ã—ã¦æŠ½å‡º"""
    if not isinstance(text, str): return [] 
    text = unicodedata.normalize('NFKC', text).lower()
    text = re.sub(r'[\(ï¼ˆ][^)]*[\)ï¼‰]', ' ', text)
    ipc_codes = []
    parts = text.split(delimiter)
    for part in parts:
        part = part.strip()
        if not part: continue
        match = re.search(r'([a-z]\d{2}[a-z])\s*(\d{1,4}/\d{2,})', part)
        if match:
            ipc_code = match.group(1) + match.group(2)
            ipc_codes.append(ipc_code)
        else:
            match_main = re.search(r'\b([a-z]\d{2}[a-z])\b', part)
            if match_main:
                ipc_codes.append(match_main.group(1))
    return ipc_codes 

def advanced_tokenize(text):
    """Janomeã‚’ç”¨ã„ãŸé«˜åº¦ãªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºï¼ˆè¤‡åˆåè©æŠ½å‡ºãƒ»ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»ï¼‰"""
    if not isinstance(text, str): return ""
    text = unicodedata.normalize('NFKC', text).lower()
    text = re.sub(r'[\(ï¼ˆ][\w\s]+[\)ï¼‰]', ' ', text)
    text = re.sub(r'\b(å›³|fig|step|s)\s?\d+\b', ' ', text)
    text = re.sub(r'[!\"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~]', ' ', text)
    
    tokens = list(t.tokenize(text))
    processed_tokens = []
    i = 0
    while i < len(tokens):
        token1 = tokens[i]
        if token1.base_form in STOP_WORDS or len(token1.base_form) < 2:
            i += 1
            continue
        
        # è¤‡åˆåè©ã®çµåˆãƒ­ã‚¸ãƒƒã‚¯
        if (i + 1) < len(tokens):
            token2 = tokens[i+1]
            pos1 = token1.part_of_speech.split(',')[0]
            pos2 = token2.part_of_speech.split(',')[0]
            if pos1 == 'åè©' and pos2 == 'åè©' and token2.base_form not in STOP_WORDS:
                compound_word = token1.base_form + token2.base_form
                processed_tokens.append(compound_word)
                i += 2
                continue
        
        pos = token1.part_of_speech.split(',')[0]
        if pos in ['åè©']:
            processed_tokens.append(token1.base_form)
        i += 1
    return " ".join(processed_tokens)

def robust_parse_date(series):
    """
    å¤šæ§˜ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®æ—¥ä»˜æ–‡å­—åˆ—ã‚’ãƒ‘ãƒ¼ã‚¹ã™ã‚‹å¼·åŠ›ãªé–¢æ•°
    - æ¨™æº–å½¢å¼ (YYYY-MM-DD)
    - åŒºåˆ‡ã‚Šãªã— (YYYYMMDD)
    - å¹´ã®ã¿ (YYYY)
    - Excelã‚·ãƒªã‚¢ãƒ«å€¤
    """
    # 1. æ¨™æº–çš„ãªå¤‰æ›
    parsed = pd.to_datetime(series, errors='coerce')
    if parsed.notna().mean() > 0.5: return parsed
    
    # 2. åŒºåˆ‡ã‚Šæ–‡å­—ãªã— (YYYYMMDD)
    parsed = pd.to_datetime(series, format='%Y%m%d', errors='coerce')
    if parsed.notna().mean() > 0.5: return parsed
    
    # 3. å¹´ã®ã¿ (YYYY)
    parsed = pd.to_datetime(series, format='%Y', errors='coerce')
    if parsed.notna().mean() > 0.5: return parsed
    
    # 4. Excelã‚·ãƒªã‚¢ãƒ«å€¤
    try:
        numeric_series = pd.to_numeric(series, errors='coerce')
        if numeric_series.notna().sum() > 0 and numeric_series.mean() > 30000:
            parsed = pd.to_datetime(numeric_series, unit='D', origin='1899-12-30', errors='coerce')
            return parsed
    except:
        pass
        
    return parsed

# ==================================================================
# --- 3. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆæœŸåŒ– ---
# ==================================================================

def initialize_session_state():
    defaults = {
        "df_main": None,
        "sbert_model": None,
        "sbert_embeddings": None,
        "tfidf_matrix": None,
        "feature_names": None,
        "col_map": {},
        "delimiters": {
            'applicant': ';',
            'ipc': ';',
            'fterm': ';'
        },
        "preprocess_done": False
    }
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value

initialize_session_state()

# ==================================================================
# --- 4. Streamlit UIæ§‹æˆ ---
# ==================================================================

st.set_page_config(
    page_title="APOLLO | Mission Control", 
    page_icon="ğŸ›°ï¸", 
    layout="wide",
    initial_sidebar_state="expanded"
)

# ã‚«ã‚¹ã‚¿ãƒ CSSå®šç¾©
st.markdown("""
<style>
    /* ãƒ•ã‚©ãƒ³ãƒˆè¨­å®š */
    html, body { 
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif; 
    }
    
    /* ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ãƒ­ã‚´ã‚¿ã‚¤ãƒˆãƒ« (APOLLO) ã‚’å¤ªå­—ã«ã™ã‚‹ */
    [data-testid="stSidebar"] h1 {
        color: #003366;
        font-weight: 900 !important; /* Extra Bold */
        font-size: 2.5rem !important;
    }

    /* Main Page Title */
    h1 { 
        color: #003366;
        font-weight: 700; 
    }
    h2, h3 { 
        color: #333333; 
        font-weight: 500; 
        border-bottom: 2px solid #f0f0f0; 
        padding-bottom: 5px; 
    }
    
    /* æ¨™æº–ã®ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã‚’éè¡¨ç¤ºã«ã™ã‚‹ */
    [data-testid="stSidebarNav"] {
        display: none !important;
    }
    
    /* ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ä¸Šéƒ¨ä½™ç™½ã‚’èª¿æ•´ */
    [data-testid="stSidebar"] .block-container {
        padding-top: 2rem;
        padding-bottom: 1rem;
    }
    
    /* Main content area */
    .block-container { 
        padding-top: 2rem; 
        padding-bottom: 2rem; 
    }
    
    /* ãƒœã‚¿ãƒ³ã¨ã‚¿ãƒ–ã®ã‚¹ã‚¿ã‚¤ãƒ« */
    .stButton>button {
        font-weight: 600;
    }
    .stTabs [data-baseweb="tab-list"] {
        gap: 8px;
    }
    .stTabs [data-baseweb="tab"] {
        background-color: #f0f2f6;
        border-radius: 8px 8px 0 0;
        padding: 10px 15px;
    }
    .stTabs [aria-selected="true"] {
        background-color: #ffffff;
        border-bottom: 2px solid #003366;
    }
</style>
""", unsafe_allow_html=True)


# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ ---
with st.sidebar:
    st.title("APOLLO") 
    st.markdown("Advanced Patent & Overall Landscape-analytics Logic Orbiter")
    st.markdown("---")
    
    st.subheader("Home")
    st.page_link("Home.py", label="Mission Control", icon="ğŸ›°ï¸")
    
    st.subheader("Modules")
    st.page_link("pages/1_ğŸŒ_ATLAS.py", label="ATLAS", icon="ğŸŒ")
    st.page_link("pages/2_ğŸ’¡_CORE.py", label="CORE", icon="ğŸ’¡")
    st.page_link("pages/3_ğŸš€_Saturn_V.py", label="Saturn V", icon="ğŸš€")
    st.page_link("pages/4_ğŸ“ˆ_MEGA.py", label="MEGA", icon="ğŸ“ˆ")
    st.page_link("pages/5_ğŸ§­_Explorer.py", label="Explorer", icon="ğŸ§­")
    
    st.markdown("---")
    
    st.caption("ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³:")
    st.caption("1. Mission Control ã§ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€å‰å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
    st.caption("2. ä¸Šã®ãƒªã‚¹ãƒˆã‹ã‚‰åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é¸æŠã—ã¾ã™ã€‚")
    
    st.markdown("---")
    st.caption("Â© 2025 ã—ã°ã‚„ã¾")


# --- ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ ---
st.title("ğŸ›°ï¸ Mission Control") 
st.markdown("ã“ã“ã¯ã€å…¨åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§å…±é€šã®ãƒ‡ãƒ¼ã‚¿æº–å‚™ã‚’è¡Œã†ã€ŒãƒŸãƒƒã‚·ãƒ§ãƒ³ãƒ»ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒãƒ–ï¼‰ã€ã§ã™ã€‚")

st.markdown("---")
st.subheader("åˆ†æè¨­å®š")

container = st.container() 

with container:
    tab1, tab2, tab3 = st.tabs([
        "ãƒ•ã‚§ãƒ¼ã‚º 1: ãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆ", 
        "ãƒ•ã‚§ãƒ¼ã‚º 2: ã‚«ãƒ©ãƒ ç´ä»˜ã‘", 
        "ãƒ•ã‚§ãƒ¼ã‚º 3: åˆ†æã‚¨ãƒ³ã‚¸ãƒ³èµ·å‹•"
    ])

    # --- A-1. ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ---
    with tab1:
        st.markdown("##### åˆ†æå¯¾è±¡ã®ç‰¹è¨±ãƒªã‚¹ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦ãã ã•ã„ã€‚")
        uploaded_file = st.file_uploader(
            "åˆ†æãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (CSV ã¾ãŸã¯ Excel)", 
            type=["csv", "xlsx", "xls"],
            label_visibility="collapsed"
        )
        
        if uploaded_file is not None:
            try:
                if uploaded_file.name.lower().endswith('.csv'):
                    try:
                        df = pd.read_csv(uploaded_file, dtype=str)
                    except UnicodeDecodeError:
                        df = pd.read_csv(uploaded_file, dtype=str, encoding='shift_jis')
                else:
                    df = pd.read_excel(uploaded_file, dtype=str)
                
                st.session_state.df_main = df
                st.session_state.preprocess_done = False 
                st.success(f"ãƒ•ã‚¡ã‚¤ãƒ« '{uploaded_file.name}' ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº† ({len(df)}è¡Œ)ã€‚")
                st.dataframe(df.head())
            except Exception as e:
                st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
                st.session_state.df_main = None
                
    # --- A-2. ã‚«ãƒ©ãƒ ç´ä»˜ã‘ ---
    with tab2:
        if st.session_state.df_main is not None:
            df = st.session_state.df_main
            columns_with_none = [None] + list(df.columns)
            
            # ä¿å­˜ã•ã‚ŒãŸå€¤ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¨ã—ã¦ä½¿ç”¨
            current_col_map = st.session_state.col_map
            current_delimiters = st.session_state.delimiters
            
            def get_index(key, options):
                val = current_col_map.get(key)
                if val in options:
                    return options.index(val)
                return 0

            col_map = {}
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.markdown("##### å¿…é ˆãƒ†ã‚­ã‚¹ãƒˆé …ç›®")
                col_map['title'] = st.selectbox("ç™ºæ˜ã®åç§°:", columns_with_none, index=get_index('title', columns_with_none), key="col_title")
                col_map['abstract'] = st.selectbox("è¦ç´„:", columns_with_none, index=get_index('abstract', columns_with_none), key="col_abstract")
                col_map['claim'] = st.selectbox("è«‹æ±‚é …:", columns_with_none, index=get_index('claim', columns_with_none), key="col_claim")
            with col2:
                st.markdown("##### å¿…é ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿é …ç›®")
                col_map['app_num'] = st.selectbox("å‡ºé¡˜ç•ªå·:", columns_with_none, index=get_index('app_num', columns_with_none), key="col_app_num")
                col_map['date'] = st.selectbox("å‡ºé¡˜æ—¥:", columns_with_none, index=get_index('date', columns_with_none), key="col_date")
                col_map['applicant'] = st.selectbox("å‡ºé¡˜äºº:", columns_with_none, index=get_index('applicant', columns_with_none), key="col_applicant")
                applicant_delimiter = st.text_input("å‡ºé¡˜äººåŒºåˆ‡ã‚Šæ–‡å­—:", value=current_delimiters.get('applicant', ';'), key="del_applicant")
            with col3:
                st.markdown("##### åˆ†æè»¸é …ç›®")
                col_map['ipc'] = st.selectbox("IPC:", columns_with_none, index=get_index('ipc', columns_with_none), key="col_ipc")
                ipc_delimiter = st.text_input("IPCåŒºåˆ‡ã‚Šæ–‡å­—:", value=current_delimiters.get('ipc', ';'), key="del_ipc")
                col_map['fterm'] = st.selectbox("Fã‚¿ãƒ¼ãƒ  (ä»»æ„):", columns_with_none, index=get_index('fterm', columns_with_none), key="col_fterm")
                fterm_delimiter = st.text_input("Fã‚¿ãƒ¼ãƒ åŒºåˆ‡ã‚Šæ–‡å­—:", value=current_delimiters.get('fterm', ';'), key="del_fterm") 
                
            # ãƒãƒƒãƒ”ãƒ³ã‚°æƒ…å ±ã‚’ä¿å­˜
            st.session_state.col_map = col_map
            st.session_state.delimiters = {
                'applicant': applicant_delimiter,
                'ipc': ipc_delimiter,
                'fterm': fterm_delimiter
            }
        else:
            st.info("ãƒ•ã‚§ãƒ¼ã‚º1ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹ã¨ã€ã‚«ãƒ©ãƒ ç´ä»˜ã‘è¨­å®šãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")

    # --- A-3. å‰å‡¦ç†å®Ÿè¡Œ ---
    with tab3:
        st.markdown("##### å…¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å…±é€šã®åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã‚’èµ·å‹•ã—ã¾ã™ã€‚")
        st.write("ãƒ‡ãƒ¼ã‚¿é‡ã«å¿œã˜ã¦æ•°åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚")

        if st.button("åˆ†æã‚¨ãƒ³ã‚¸ãƒ³èµ·å‹• (SBERT/TF-IDF)", type="primary", key="run_preprocess"):
            if st.session_state.df_main is None:
                st.error("ãƒ•ã‚§ãƒ¼ã‚º1ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
            # å¿…é ˆã‚«ãƒ©ãƒ ãƒã‚§ãƒƒã‚¯ (Fã‚¿ãƒ¼ãƒ ã¯é™¤å¤–)
            elif any(v is None for k, v in st.session_state.col_map.items() if k in ['title', 'abstract', 'claim', 'app_num', 'date', 'applicant', 'ipc']):
                missing = [k for k, v in st.session_state.col_map.items() if v is None and k in ['title', 'abstract', 'claim', 'app_num', 'date', 'applicant', 'ipc']]
                st.error(f"ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚§ãƒ¼ã‚º2ã®å¿…é ˆã‚«ãƒ©ãƒ ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“: {missing}")
            else:
                try:
                    status_area = st.empty() 
                    
                    with st.spinner("åˆ†æã‚¨ãƒ³ã‚¸ãƒ³èµ·å‹•ä¸­..."):
                        
                        df = st.session_state.df_main.copy() 
                        col_map = st.session_state.col_map
                        delimiters = st.session_state.delimiters
                        
                        status_area.write("2/7: SBERTãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
                        sbert_model = load_sbert_model()
                        st.session_state.sbert_model = sbert_model

                        status_area.write("3/7: SBERTç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’çµåˆä¸­...")
                        df['text_for_sbert'] = (
                            df[col_map['title']].fillna('') + ' ' +
                            df[col_map['abstract']].fillna('') + ' ' +
                            df[col_map['claim']].fillna('')
                        )

                        status_area.write("4. SBERTãƒ™ã‚¯ãƒˆãƒ«è¨ˆç®—ã‚’å®Ÿè¡Œä¸­...")
                        texts_for_sbert_list = df['text_for_sbert'].tolist()
                        
                        progress_bar = st.progress(0, "SBERTã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ä¸­...")
                        sbert_embeddings = sbert_model.encode(
                            texts_for_sbert_list,
                            show_progress_bar=False, 
                            batch_size=128
                        )
                        for i in range(1, 101):
                            progress_bar.progress(i / 100.0, text=f"SBERTã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ä¸­... {i}%")
                        progress_bar.progress(1.0, "SBERTã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰å®Œäº†")
                        st.session_state.sbert_embeddings = sbert_embeddings

                        status_area.write("5/7: TF-IDF (è‡ªå‹•ãƒ©ãƒ™ãƒªãƒ³ã‚°ç”¨) ã‚’è¨ˆç®—ä¸­...")
                        df['text_for_tfidf'] = df['text_for_sbert'].apply(advanced_tokenize)
                        
                        vectorizer = TfidfVectorizer(max_features=None, min_df=5, max_df=0.80)
                        st.session_state.tfidf_matrix = vectorizer.fit_transform(df['text_for_tfidf'])
                        st.session_state.feature_names = np.array(vectorizer.get_feature_names_out())

                        status_area.write("6/7: æ—¥ä»˜ã¨åˆ†æè»¸ï¼ˆIPC/Fã‚¿ãƒ¼ãƒ /å‡ºé¡˜äººï¼‰ã‚’æ­£è¦åŒ–ä¸­...")
                        
                        # æ—¥ä»˜è§£æ
                        raw_dates = df[col_map['date']].astype(str)
                        df['parsed_date'] = robust_parse_date(raw_dates)
                        
                        # è¨ºæ–­æƒ…å ±è¡¨ç¤º
                        valid_date_count = df['parsed_date'].notna().sum()
                        if valid_date_count == 0:
                            st.error(f"âš ï¸ ã‚¨ãƒ©ãƒ¼: '{col_map['date']}' ã‚«ãƒ©ãƒ ã‹ã‚‰æ—¥ä»˜ã‚’1ä»¶ã‚‚å¤‰æ›ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                            st.warning("å…ƒãƒ‡ãƒ¼ã‚¿ã®å½¢å¼: " + str(raw_dates.iloc[0] if len(df)>0 else 'N/A'))
                        else:
                            st.success(f"æ—¥ä»˜è§£ææˆåŠŸ: {valid_date_count}/{len(df)}ä»¶")
                            with st.expander("æ—¥ä»˜ãƒ‡ãƒ¼ã‚¿ã®å¤‰æ›ã‚µãƒ³ãƒ—ãƒ«ã‚’ç¢ºèª"):
                                debug_df = pd.DataFrame({
                                    'å…ƒãƒ‡ãƒ¼ã‚¿': raw_dates.head(5),
                                    'å¤‰æ›å¾Œ': df['parsed_date'].head(5)
                                })
                                st.dataframe(debug_df)

                        df['year'] = df['parsed_date'].dt.year
                        df['app_num_main'] = df[col_map['app_num']].astype(str).str.strip()

                        ipc_delimiter = delimiters['ipc']
                        df['ipc_normalized'] = df[col_map['ipc']].apply(lambda x: extract_ipc(x, ipc_delimiter))
                        
                        ipc_raw_list = df[col_map['ipc']].fillna('').astype(str).str.split(ipc_delimiter)
                        df['ipc_main_group'] = ipc_raw_list.apply(lambda terms: list(set([t.strip().split('/')[0].strip().upper() for t in terms if t.strip()])))

                        # Fã‚¿ãƒ¼ãƒ  (ä»»æ„)
                        if col_map['fterm']:
                            fterm_delimiter = delimiters['fterm']
                            fterm_raw_list = df[col_map['fterm']].fillna('').astype(str).str.split(fterm_delimiter)
                            df['fterm_main'] = fterm_raw_list.apply(lambda terms: list(set([t.strip()[:5].upper() for t in terms if t.strip() and len(t) >= 5])))
                        else:
                            df['fterm_main'] = [[] for _ in range(len(df))]

                        applicant_delimiter = delimiters['applicant']
                        applicant_raw_list = df[col_map['applicant']].fillna('').astype(str).str.split(applicant_delimiter)
                        df['applicant_main'] = applicant_raw_list.apply(lambda names: list(set([n.strip() for n in names if n.strip()])))
                        
                        status_area.write("7/7: ä¸­é–“ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­...")
                        df.drop(columns=['text_for_sbert', 'text_for_tfidf'], errors='ignore', inplace=True)
                        
                        st.session_state.df_main = df 
                        st.session_state.preprocess_done = True
                        
                    status_area.empty() 
                    progress_bar.empty()
                    st.success("åˆ†æã‚¨ãƒ³ã‚¸ãƒ³èµ·å‹•å®Œäº†ã€‚")
                    st.info("ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã‹ã‚‰åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é¸æŠã—ã€ãƒŸãƒƒã‚·ãƒ§ãƒ³ã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚")

                except Exception as e:
                    st.error(f"å‰å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                    st.exception(traceback.format_exc())