# ==================================================================
# --- 0. ç’°å¢ƒå¤‰æ•°ã®è¨­å®š (æœ€å„ªå…ˆ) ---
# ==================================================================
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ['OMP_NUM_THREADS'] = '1'

# ==================================================================
# --- 1. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
# ==================================================================
import streamlit as st
import pandas as pd
import numpy as np
import warnings
import traceback
import unicodedata
import re
import time
import datetime

from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import normalize
from janome.tokenizer import Tokenizer

warnings.filterwarnings('ignore')

# ==================================================================
# --- 2. ãƒšãƒ¼ã‚¸è¨­å®š ---
# ==================================================================
st.set_page_config(
    page_title="APOLLO | Mission Control", 
    page_icon="ğŸ›°ï¸", 
    layout="wide",
    initial_sidebar_state="expanded"
)

# ==================================================================
# --- 3. å®šæ•°ãƒ»å…±é€šé–¢æ•°å®šç¾© ---
# ==================================================================

STOP_WORDS = {
    "ã™ã‚‹","ã‚ã‚‹","ãªã‚‹","ãŸã‚","ã“ã¨","ã‚ˆã†","ã‚‚ã®","ã“ã‚Œ","ãã‚Œ","ã‚ã‚Œ","ã“ã“","ãã“","ã©ã‚Œ","ã©ã®",
    "ã“ã®","ãã®","å½“è©²","è©²","ãŠã‚ˆã³","åŠã³","ã¾ãŸã¯","ã¾ãŸ","ä¾‹ãˆã°","ä¾‹ãˆã°ã¯","ã«ãŠã„ã¦","ã«ã‚ˆã‚Š",
    "ã«å¯¾ã—ã¦","ã«é–¢ã—ã¦","ã«ã¤ã„ã¦","ã¨ã—ã¦","ã¨ã—ã¦ã¯","å ´åˆ","ä¸€æ–¹","ä»–æ–¹","ã•ã‚‰ã«","ãã—ã¦","ãŸã ã—",
    "ãªãŠ","ç­‰","ãªã©","ç­‰ã€…","ã„ã‚ã‚†ã‚‹","æ‰€è¬‚","åŒæ§˜","åŒæ™‚","å‰è¨˜","æœ¬","åŒ","å„","å„ç¨®","æ‰€å®š","æ‰€æœ›",
    "ä¸€ä¾‹","ä»–","ä¸€éƒ¨","ä¸€ã¤","è¤‡æ•°","å°‘ãªãã¨ã‚‚","å°‘ãªãã¨ã‚‚ä¸€ã¤","ä¸Šè¨˜","ä¸‹è¨˜","å‰è¿°","å¾Œè¿°","æ—¢è¿°",
    "é–¢ã™ã‚‹","åŸºã¥ã","ç”¨ã„ã‚‹","ä½¿ç”¨","åˆ©ç”¨","æœ‰ã™ã‚‹","å«ã‚€","å‚™ãˆã‚‹","è¨­ã‘ã‚‹","ã™ãªã‚ã¡","å¾“ã£ã¦",
    "ã—ã‹ã—ãªãŒã‚‰","æ¬¡ã«","ç‰¹ã«","å…·ä½“çš„ã«","è©³ç´°ã«","ã„ãšã‚Œ","ã†ã¡","ãã‚Œãã‚Œ","ã¨ã",
    "ã‹ã‹ã‚‹","ã‹ã‚ˆã†ãª","ã‹ã‹ã‚‹å ´åˆ","æœ¬ä»¶","æœ¬é¡˜","æœ¬å‡ºé¡˜","æœ¬æ˜ç´°æ›¸","ã“ã‚Œã‚‰","ãã‚Œã‚‰","å„ã€…","éšæ™‚","é©å®œ",
    "ä»»æ„","å¿…ãšã—ã‚‚","é€šå¸¸","ä¸€èˆ¬ã«","å…¸å‹çš„","ä»£è¡¨çš„",
    "æœ¬ç™ºæ˜","ç™ºæ˜","å®Ÿæ–½ä¾‹","å®Ÿæ–½å½¢æ…‹","å¤‰å½¢ä¾‹","è«‹æ±‚","è«‹æ±‚é …","å›³","å›³é¢","ç¬¦å·","ç¬¦å·ã®èª¬æ˜",
    "å›³é¢ã®ç°¡å˜ãªèª¬æ˜","ç™ºæ˜ã®è©³ç´°ãªèª¬æ˜","æŠ€è¡“åˆ†é‡","èƒŒæ™¯æŠ€è¡“","å¾“æ¥æŠ€è¡“","ç™ºæ˜ãŒè§£æ±ºã—ã‚ˆã†ã¨ã™ã‚‹èª²é¡Œ","èª²é¡Œ",
    "è§£æ±ºæ‰‹æ®µ","åŠ¹æœ","è¦ç´„","ç™ºæ˜ã®åŠ¹æœ","ç›®çš„","æ‰‹æ®µ","æ§‹æˆ","æ§‹é€ ","å·¥ç¨‹","å‡¦ç†","æ–¹æ³•","æ‰‹æ³•","æ–¹å¼",
    "ã‚·ã‚¹ãƒ†ãƒ ","ãƒ—ãƒ­ã‚°ãƒ©ãƒ ","è¨˜æ†¶åª’ä½“","ç‰¹å¾´","ç‰¹å¾´ã¨ã™ã‚‹","ç‰¹å¾´éƒ¨","ã‚¹ãƒ†ãƒƒãƒ—","ãƒ•ãƒ­ãƒ¼","ã‚·ãƒ¼ã‚±ãƒ³ã‚¹","å®šç¾©",
    "é–¢ä¿‚","å¯¾å¿œ","æ•´åˆ","å®Ÿæ–½ã®å½¢æ…‹","å®Ÿæ–½ã®æ…‹æ§˜","æ…‹æ§˜","å¤‰å½¢","ä¿®æ­£ä¾‹","å›³ç¤º","å›³ç¤ºä¾‹","å›³ç¤ºã—ãªã„",
    "å‚ç…§","å‚ç…§ç¬¦å·","æ®µè½","è©³ç´°èª¬æ˜","è¦æ—¨","ä¸€å®Ÿæ–½å½¢æ…‹","ä»–ã®å®Ÿæ–½å½¢æ…‹","ä¸€å®Ÿæ–½ä¾‹","åˆ¥ã®å´é¢","ä»˜è¨˜",
    "é©ç”¨ä¾‹","ç”¨èªã®å®šç¾©","é–‹ç¤º","æœ¬é–‹ç¤º","é–‹ç¤ºå†…å®¹","è¨˜è¼‰","è¨˜è¿°","æ²è¼‰","è¨€åŠ","å†…å®¹","è©³ç´°","èª¬æ˜","è¡¨è¨˜","è¡¨ç¾","ç®‡æ¡æ›¸ã","ä»¥ä¸‹ã®","ä»¥ä¸Šã®","å…¨ã¦ã®","ä»»æ„ã®","ç‰¹å®šã®",
    "ä¸Šéƒ¨","ä¸‹éƒ¨","å†…éƒ¨","å¤–éƒ¨","å†…å´","å¤–å´","è¡¨é¢","è£é¢","å´é¢","ä¸Šé¢","ä¸‹é¢","ç«¯é¢","å…ˆç«¯","åŸºç«¯","å¾Œç«¯","ä¸€ç«¯","ä»–ç«¯","ä¸­å¿ƒ","ä¸­å¤®","å‘¨ç¸","å‘¨è¾º",
    "è¿‘å‚","æ–¹å‘","ä½ç½®","ç©ºé–“","é ˜åŸŸ","ç¯„å›²","é–“éš”","è·é›¢","å½¢çŠ¶","å½¢æ…‹","çŠ¶æ…‹","ç¨®é¡","å±¤","è†œ","éƒ¨",
    "éƒ¨æ","éƒ¨ä½","éƒ¨å“","æ©Ÿæ§‹","è£…ç½®","å®¹å™¨","çµ„æˆ","ææ–™","ç”¨é€”","é©ç”¨","é©ç”¨ä¾‹","ç‰‡å´","ä¸¡å´","å·¦å´",
    "å³å´","å‰æ–¹","å¾Œæ–¹","ä¸Šæµ","ä¸‹æµ","éš£æ¥","è¿‘æ¥","é›¢é–“","é–“ç½®","ä»‹åœ¨","é‡ç•³","æ¦‚ã­","ç•¥","ç•¥ä¸­å¤®",
    "å›ºå®šå´","å¯å‹•å´","ä¼¸é•·","åç¸®","ä¿‚åˆ","åµŒåˆ","å–ä»˜","é€£çµéƒ¨","æ”¯æŒä½“","æ”¯æŒéƒ¨","ã‚¬ã‚¤ãƒ‰éƒ¨",
    "ãƒ‡ãƒ¼ã‚¿","æƒ…å ±","ä¿¡å·","å‡ºåŠ›","å…¥åŠ›","åˆ¶å¾¡","æ¼”ç®—","å–å¾—","é€ä¿¡","å—ä¿¡","è¡¨ç¤º","é€šçŸ¥","è¨­å®š","å¤‰æ›´",
    "æ›´æ–°","ä¿å­˜","å‰Šé™¤","è¿½åŠ ","å®Ÿè¡Œ","é–‹å§‹","çµ‚äº†","ç¶™ç¶š","åœæ­¢","åˆ¤å®š","åˆ¤æ–­","æ±ºå®š","é¸æŠ","ç‰¹å®š",
    "æŠ½å‡º","æ¤œå‡º","æ¤œçŸ¥","æ¸¬å®š","è¨ˆæ¸¬","ç§»å‹•","å›è»¢","å¤‰ä½","å¤‰å½¢","å›ºå®š","é…ç½®","ç”Ÿæˆ","ä»˜ä¸","ä¾›çµ¦",
    "é©ç”¨","ç…§åˆ","æ¯”è¼ƒ","ç®—å‡º","è§£æ","åŒå®š","åˆæœŸåŒ–","èª­å‡º","æ›¸è¾¼","ç™»éŒ²","è¨˜éŒ²","é…ä¿¡","é€£æº","åˆ‡æ›¿",
    "èµ·å‹•","å¾©å¸°","ç›£è¦–","é€šçŸ¥å‡¦ç†","å–å¾—å‡¦ç†","æ¼”ç®—å‡¦ç†","è‰¯å¥½","å®¹æ˜“","ç°¡ä¾¿","é©åˆ‡","æœ‰åˆ©","æœ‰ç”¨","æœ‰åŠ¹",
    "åŠ¹æœçš„","é«˜ã„","ä½ã„","å¤§ãã„","å°ã•ã„","æ–°è¦","æ”¹è‰¯","æ”¹å–„","æŠ‘åˆ¶","å‘ä¸Š","ä½æ¸›","å‰Šæ¸›","å¢—åŠ ",
    "æ¸›å°‘","å¯èƒ½","å¥½é©","å¥½ã¾ã—ã„","æœ›ã¾ã—ã„","å„ªã‚Œã‚‹","å„ªã‚ŒãŸ","é«˜æ€§èƒ½","é«˜åŠ¹ç‡","ä½ã‚³ã‚¹ãƒˆ","ã‚³ã‚¹ãƒˆ",
    "ç°¡æ˜“","å®‰å®š","å®‰å®šæ€§","è€ä¹…","è€ä¹…æ€§","ä¿¡é ¼æ€§","ç°¡ç´ ","ç°¡ç•¥","å˜ç´”","æœ€é©","æœ€é©åŒ–","æ±ç”¨","æ±ç”¨æ€§",
    "å®Ÿç¾","é”æˆ","ç¢ºä¿","ç¶­æŒ","é˜²æ­¢","å›é¿","ä¿ƒé€²","ä¸è¦","å¿…è¦","é«˜ç²¾åº¦","çœé›»åŠ›","çœè³‡æº","é«˜ä¿¡é ¼",
    "ä½è² è·","é«˜ç´”åº¦","é«˜å¯†åº¦","é«˜æ„Ÿåº¦","è¿…é€Ÿ","å††æ»‘","ç°¡ç•¥åŒ–","ä½ä¾¡æ ¼","å®ŸåŠ¹çš„","å¯èƒ½åŒ–","æœ‰åŠ¹åŒ–",
    "éå¿…é ˆ","é©åˆ","äº’æ›","å‡ºé¡˜","å‡ºé¡˜äºº","å‡ºé¡˜ç•ªå·","å‡ºé¡˜æ—¥","å‡ºé¡˜æ›¸","å‡ºé¡˜å…¬é–‹","å…¬é–‹","å…¬é–‹ç•ªå·",
    "å…¬é–‹å…¬å ±","å…¬å ±","å…¬å ±ç•ªå·","ç‰¹è¨±","ç‰¹è¨±ç•ªå·","ç‰¹è¨±æ–‡çŒ®","éç‰¹è¨±æ–‡çŒ®","å¼•ç”¨","å¼•ç”¨æ–‡çŒ®","å…ˆè¡ŒæŠ€è¡“",
    "å¯©æŸ»","å¯©æŸ»å®˜","æ‹’çµ¶","æ„è¦‹æ›¸","è£œæ­£æ›¸","å„ªå…ˆ","å„ªå…ˆæ—¥","åˆ†å‰²å‡ºé¡˜","ç¶™ç¶šå‡ºé¡˜","å›½å†…ç§»è¡Œ","å›½éš›å‡ºé¡˜",
    "å›½éš›å…¬é–‹","PCT","ç™»éŒ²","å…¬é–‹æ—¥","å¯©æŸ»è«‹æ±‚","æ‹’çµ¶ç†ç”±","è£œæ­£","è¨‚æ­£","ç„¡åŠ¹å¯©åˆ¤","ç•°è­°","å–æ¶ˆ","å–ä¸‹ã’",
    "äº‹ä»¶ç•ªå·","ä»£ç†äºº","å¼ç†å£«","ä¿‚å±","çµŒé",
    "ç¬¬","ç¬¬ä¸€","ç¬¬äºŒ","ç¬¬ä¸‰","ç¬¬1","ç¬¬ï¼’","ç¬¬ï¼“","ç¬¬ï¼‘","ç¬¬ï¼’","ç¬¬ï¼“","ï¼‘","ï¼’","ï¼“","ï¼”","ï¼•","ï¼–","ï¼—","ï¼˜","ï¼™","ï¼",
    "ä¸€","äºŒ","ä¸‰","å››","äº”","å…­","ä¸ƒ","å…«","ä¹","é›¶","æ•°","è¤‡åˆ","å¤šæ•°","å°‘æ•°","å›³1","å›³2","å›³3","å›³4","å›³5","å›³6","å›³7","å›³8","å›³9",
    "è¡¨1","è¡¨2","è¡¨3","å¼1","å¼2","å¼3","ï¼","ï¼‘","ï¼’","ï¼“","ï¼”","ï¼•","ï¼–","ï¼—","ï¼˜","ï¼™","%","ï¼…","wt%","vol%","è³ªé‡%","é‡é‡%","å®¹é‡%","mol","mol%","mol/L","M","mm","cm","m","nm","Î¼m","Î¼","rpm",
    "Pa","kPa","MPa","GPa","N","W","V","A","mA","Hz","kHz","MHz","GHz","â„ƒ","Â°C","K","mL","L","g","kg","mg","wt","vol",
    "h","hr","hrs","min","s","sec","ppm","ppb","bar","Î©","ohm","J","kJ","Wh","kWh",
    "æ ªå¼ä¼šç¤¾","æœ‰é™ä¼šç¤¾","åˆè³‡ä¼šç¤¾","åˆåä¼šç¤¾","åˆåŒä¼šç¤¾","Inc","Inc.","Ltd","Ltd.","Co","Co.","Corp","Corp.","LLC",
    "GmbH","AG","BV","B.V.","S.A.","S.p.A.","ï¼ˆæ ªï¼‰","ãˆ±","ï¼ˆæœ‰ï¼‰",
    "æº¶æ¶²","æº¶åª’","è§¦åª’","åå¿œ","ç”Ÿæˆç‰©","åŸæ–™","æˆåˆ†","å«æœ‰","å«æœ‰é‡","é…åˆ","æ··åˆ","æ··åˆç‰©","æ¿ƒåº¦","æ¸©åº¦","æ™‚é–“",
    "å‰²åˆ","æ¯”ç‡","åŸº","å®˜èƒ½åŸº","åŒ–åˆç‰©","çµ„æˆç‰©","æ¨¹è„‚","ãƒãƒªãƒãƒ¼","ãƒ¢ãƒãƒãƒ¼","åŸºæ¿","åŸºæ","ãƒ•ã‚£ãƒ«ãƒ ","ã‚·ãƒ¼ãƒˆ",
    "ç²’å­","ç²‰æœ«","æ¯”è¼ƒä¾‹","å‚è€ƒä¾‹","è©¦é¨“","è©¦æ–™","è©•ä¾¡","æ¡ä»¶","å®Ÿé¨“","å®Ÿé¨“ä¾‹","åå¿œæ¡ä»¶","åå¿œæ™‚é–“","åå¿œæ¸©åº¦",
    "å‡¦ç†è£…ç½®","ç«¯æœ«","ãƒ¦ãƒ‹ãƒƒãƒˆ","ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«","å›è·¯","ç´ å­","é›»æº","é›»åœ§","é›»æµ","ä¿¡å·ç·š","é…ç·š","ç«¯å­","ç«¯éƒ¨","æ¥ç¶š",
    "æ¥ç¶šéƒ¨","æ¼”ç®—éƒ¨","è¨˜æ†¶éƒ¨","è¨˜æ†¶è£…ç½®","è¨˜éŒ²åª’ä½“","ãƒ¦ãƒ¼ã‚¶","åˆ©ç”¨è€…","ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ","ã‚µãƒ¼ãƒ","ç”»é¢","UI","GUI",
    "ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹","ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹","DB","ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯","é€šä¿¡","è¦æ±‚","å¿œç­”","ãƒªã‚¯ã‚¨ã‚¹ãƒˆ","ãƒ¬ã‚¹ãƒãƒ³ã‚¹","ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿",
    "å¼•æ•°","å±æ€§","ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£","ãƒ•ãƒ©ã‚°","ID","ãƒ•ã‚¡ã‚¤ãƒ«","ãƒ‡ãƒ¼ã‚¿æ§‹é€ ","ãƒ†ãƒ¼ãƒ–ãƒ«","ãƒ¬ã‚³ãƒ¼ãƒ‰",
    "è»¸","ã‚·ãƒ£ãƒ•ãƒˆ","ã‚®ã‚¢","ãƒ¢ãƒ¼ã‚¿","ã‚¨ãƒ³ã‚¸ãƒ³","ã‚¢ã‚¯ãƒãƒ¥ã‚¨ãƒ¼ã‚¿","ã‚»ãƒ³ã‚µ","ãƒãƒ«ãƒ–","ãƒãƒ³ãƒ—","ç­ä½“","ãƒã‚¦ã‚¸ãƒ³ã‚°","ãƒ•ãƒ¬ãƒ¼ãƒ ",
    "ã‚·ãƒ£ãƒ¼ã‚·","é§†å‹•","ä¼é”","æ”¯æŒ","é€£çµ","è§£æ±º", "æº–å‚™", "æä¾›", "ç™ºç”Ÿ", "ä»¥ä¸Š", "ååˆ†",
    "ã§ãã‚‹", "ã„ã‚‹", "æ˜ç´°æ›¸"
}

@st.cache_resource
def load_sbert_model():
    return SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

@st.cache_resource
def load_tokenizer():
    return Tokenizer()

t = load_tokenizer()

def advanced_tokenize(text):
    if not isinstance(text, str): return ""
    text = unicodedata.normalize('NFKC', text).lower()
    text = re.sub(r'[\(ï¼ˆ][\w\s]+[\)ï¼‰]', ' ', text)
    text = re.sub(r'\b(å›³|fig|step|s)\s?\d+\b', ' ', text)
    text = re.sub(r'[!\"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~]', ' ', text)
    
    tokens = list(t.tokenize(text))
    processed_tokens = []
    i = 0
    while i < len(tokens):
        token1 = tokens[i]
        base_form = token1.base_form if token1.base_form != '*' else token1.surface
        
        if base_form in STOP_WORDS or len(base_form) < 2:
            i += 1
            continue
        
        if (i + 1) < len(tokens):
            token2 = tokens[i+1]
            base_form2 = token2.base_form if token2.base_form != '*' else token2.surface
            pos1 = token1.part_of_speech.split(',')[0]
            pos2 = token2.part_of_speech.split(',')[0]
            if pos1 == 'åè©' and pos2 == 'åè©' and base_form2 not in STOP_WORDS:
                compound_word = base_form + base_form2
                processed_tokens.append(compound_word)
                i += 2
                continue
        
        pos = token1.part_of_speech.split(',')[0]
        if pos == 'åè©':
            processed_tokens.append(base_form)
        i += 1
    return " ".join(processed_tokens)

def robust_parse_date(series):
    parsed = pd.to_datetime(series, errors='coerce')
    if parsed.notna().mean() > 0.5: return parsed
    
    parsed = pd.to_datetime(series, format='%Y%m%d', errors='coerce')
    if parsed.notna().mean() > 0.5: return parsed
    
    parsed = pd.to_datetime(series, format='%Y', errors='coerce')
    if parsed.notna().mean() > 0.5: return parsed
    
    try:
        numeric_series = pd.to_numeric(series, errors='coerce')
        if numeric_series.notna().sum() > 0 and numeric_series.mean() > 30000:
            parsed = pd.to_datetime(numeric_series, unit='D', origin='1899-12-30', errors='coerce')
            return parsed
    except:
        pass
    return parsed

def extract_ipc(text, delimiter=';'):
    if not isinstance(text, str): return [] 
    text = unicodedata.normalize('NFKC', text).lower()
    text = re.sub(r'[\(ï¼ˆ][^)]*[\)ï¼‰]', ' ', text)
    ipc_codes = []
    parts = text.split(delimiter)
    for part in parts:
        part = part.strip()
        if not part: continue
        match = re.search(r'([a-z]\d{2}[a-z])\s*(\d{1,4}/\d{2,})', part)
        if match:
            ipc_code = match.group(1) + match.group(2)
            ipc_codes.append(ipc_code)
        else:
            match_main = re.search(r'\b([a-z]\d{2}[a-z])\b', part)
            if match_main:
                ipc_codes.append(match_main.group(1))
    return ipc_codes 

def smart_map_index(current_value, options, keywords):
    """
    ã‚«ãƒ©ãƒ ç´ä»˜ã‘ã®è‡ªå‹•åŒ–ãƒ­ã‚¸ãƒƒã‚¯ (Fix: åˆæœŸçŠ¶æ…‹Noneã§ã‚‚æ¤œç´¢ã‚’å®Ÿè¡Œ)
    """
    if current_value is not None and current_value in options:
        return options.index(current_value)
    
    valid_cols = options[1:]
    
    for kw in keywords:
        for col in valid_cols:
            if kw == str(col):
                return options.index(col)
                
    for kw in keywords:
        for col in valid_cols:
            if kw in str(col):
                return options.index(col)
                
    return 0

# ==================================================================
# --- 4. ãƒ‡ã‚¶ã‚¤ãƒ³è¨­å®š ---
# ==================================================================

st.markdown("""
<style>
    html, body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif; }
    [data-testid="stSidebar"] h1 { color: #003366; font-weight: 900 !important; font-size: 2.5rem !important; }
    h1 { color: #003366; font-weight: 700; }
    h2, h3 { color: #333333; font-weight: 500; border-bottom: 2px solid #f0f0f0; padding-bottom: 5px; }
    [data-testid="stSidebarNav"] { display: none !important; }
    [data-testid="stSidebar"] .block-container { padding-top: 2rem; padding-bottom: 1rem; }
    .block-container { padding-top: 2rem; padding-bottom: 2rem; }
    .stButton>button { font-weight: 600; }
    .stTabs [data-baseweb="tab-list"] { gap: 8px; }
    .stTabs [data-baseweb="tab"] { background-color: #f0f2f6; border-radius: 8px 8px 0 0; padding: 10px 15px; }
    .stTabs [aria-selected="true"] { background-color: #ffffff; border-bottom: 2px solid #003366; }
</style>
""", unsafe_allow_html=True)

with st.sidebar:
    st.title("APOLLO") 
    st.markdown("Advanced Patent & Overall Landscape-analytics Logic Orbiter")
    st.markdown("**v.3**")
    st.markdown("---")
    st.subheader("Home"); st.page_link("Home.py", label="Mission Control", icon="ğŸ›°ï¸")
    st.subheader("Modules")
    st.page_link("pages/1_ğŸŒ_ATLAS.py", label="ATLAS", icon="ğŸŒ")
    st.page_link("pages/2_ğŸ’¡_CORE.py", label="CORE", icon="ğŸ’¡")
    st.page_link("pages/3_ğŸš€_Saturn_V.py", label="Saturn V", icon="ğŸš€")
    st.page_link("pages/4_ğŸ“ˆ_MEGA.py", label="MEGA", icon="ğŸ“ˆ")
    st.page_link("pages/5_ğŸ§­_Explorer.py", label="Explorer", icon="ğŸ§­")
    st.page_link("pages/6_ğŸ”—_CREW.py", label="CREW", icon="ğŸ”—")
    st.markdown("---")
    st.caption("ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³:\n1. Mission Control ã§ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€å‰å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\n2. ä¸Šã®ãƒªã‚¹ãƒˆã‹ã‚‰åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é¸æŠã—ã¾ã™ã€‚")
    st.markdown("---")
    st.caption("Â© 2025 ã—ã°ã‚„ã¾")

st.title("ğŸ›°ï¸ Mission Control") 
st.markdown("ã“ã“ã¯ã€å…¨åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§å…±é€šã®ãƒ‡ãƒ¼ã‚¿æº–å‚™ã‚’è¡Œã†ã€ŒãƒŸãƒƒã‚·ãƒ§ãƒ³ãƒ»ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒãƒ–ï¼‰ã€ã§ã™ã€‚")

# --- ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆæœŸåŒ– ---
def initialize_session_state():
    defaults = {
        "df_main": None,
        "shared_df": None,
        "filename": "No File",
        "sbert_model": None,
        "sbert_embeddings": None,
        "tfidf_matrix": None,
        "feature_names": None,
        "col_map": {},
        "delimiters": {'applicant': ';', 'inventor': ';', 'ipc': ';', 'fterm': ';'},
        "preprocess_done": False
    }
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value

initialize_session_state()

st.markdown("---")
st.subheader("åˆ†æè¨­å®š")

container = st.container() 

with container:
    tab1, tab2, tab3 = st.tabs([
        "ãƒ•ã‚§ãƒ¼ã‚º 1: ãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆ", 
        "ãƒ•ã‚§ãƒ¼ã‚º 2: ã‚«ãƒ©ãƒ ç´ä»˜ã‘", 
        "ãƒ•ã‚§ãƒ¼ã‚º 3: åˆ†æã‚¨ãƒ³ã‚¸ãƒ³èµ·å‹•"
    ])

    # A-1. ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    with tab1:
        st.markdown("##### åˆ†æå¯¾è±¡ã®ç‰¹è¨±ãƒªã‚¹ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦ãã ã•ã„ã€‚")
        uploaded_file = st.file_uploader(
            "åˆ†æãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (CSV ã¾ãŸã¯ Excel)", 
            type=["csv", "xlsx", "xls"],
            label_visibility="collapsed"
        )
        
        if uploaded_file is not None:
            try:
                if uploaded_file.name.lower().endswith('.csv'):
                    try:
                        df = pd.read_csv(uploaded_file, dtype=str)
                    except UnicodeDecodeError:
                        df = pd.read_csv(uploaded_file, dtype=str, encoding='shift_jis')
                else:
                    df = pd.read_excel(uploaded_file, dtype=str)
                
                st.session_state.df_main = df
                st.session_state.preprocess_done = False 
                st.session_state['shared_df'] = df  
                st.session_state['filename'] = uploaded_file.name

                st.success(f"ãƒ•ã‚¡ã‚¤ãƒ« '{uploaded_file.name}' ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº† ({len(df)}è¡Œ)ã€‚")
                st.dataframe(df.head())
                
            except Exception as e:
                st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
                st.session_state.df_main = None
                st.session_state.shared_df = None
                
    # A-2. ã‚«ãƒ©ãƒ ç´ä»˜ã‘
    with tab2:
        if st.session_state.df_main is not None:
            df = st.session_state.df_main
            columns_with_none = [None] + list(df.columns)
            
            kw_title = ['ç™ºæ˜ã®åç§°', 'åç§°', 'Title', 'Title of Invention']
            kw_abstract = ['è¦ç´„', 'è¦ç´„(æŠ„éŒ²)', 'Abstract']
            kw_claim = ['è«‹æ±‚é …', 'Claim']
            kw_app_num = ['å‡ºé¡˜ç•ªå·', 'Application Number', 'App No']
            kw_date = ['å‡ºé¡˜æ—¥', 'å‡ºé¡˜æ—¥ï¼ˆé¡åŠï¼‰', 'Date', 'Filing']
            kw_applicant = ['å‡ºé¡˜äºº', 'Applicant', 'Assignee']
            kw_inventor = ['ç™ºæ˜è€…', 'Inventor']
            kw_ipc = ['å›½éš›ç‰¹è¨±åˆ†é¡', 'å›½éš›ç‰¹è¨±åˆ†é¡(IPC)', 'IPC', 'Int. Cl']
            kw_fterm = ['Fã‚¿ãƒ¼ãƒ ', 'ãƒ†ãƒ¼ãƒã‚³ãƒ¼ãƒ‰', 'F-Term']

            col_map = {}
            col1, col2, col3 = st.columns(3)
            with col1:
                st.markdown("##### å¿…é ˆãƒ†ã‚­ã‚¹ãƒˆé …ç›®")
                col_map['title'] = st.selectbox("ç™ºæ˜ã®åç§°:", columns_with_none, index=smart_map_index(st.session_state.col_map.get('title'), columns_with_none, kw_title), key="col_title")
                col_map['abstract'] = st.selectbox("è¦ç´„:", columns_with_none, index=smart_map_index(st.session_state.col_map.get('abstract'), columns_with_none, kw_abstract), key="col_abstract")
                col_map['claim'] = st.selectbox("è«‹æ±‚é …:", columns_with_none, index=smart_map_index(st.session_state.col_map.get('claim'), columns_with_none, kw_claim), key="col_claim")
            with col2:
                st.markdown("##### å¿…é ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿é …ç›®")
                col_map['app_num'] = st.selectbox("å‡ºé¡˜ç•ªå·:", columns_with_none, index=smart_map_index(st.session_state.col_map.get('app_num'), columns_with_none, kw_app_num), key="col_app_num")
                col_map['date'] = st.selectbox("å‡ºé¡˜æ—¥:", columns_with_none, index=smart_map_index(st.session_state.col_map.get('date'), columns_with_none, kw_date), key="col_date")
                col_map['applicant'] = st.selectbox("å‡ºé¡˜äºº:", columns_with_none, index=smart_map_index(st.session_state.col_map.get('applicant'), columns_with_none, kw_applicant), key="col_applicant")
                applicant_delimiter = st.text_input("å‡ºé¡˜äººåŒºåˆ‡ã‚Šæ–‡å­—:", value=st.session_state.delimiters.get('applicant', ';'), key="del_applicant")
                
                col_map['inventor'] = st.selectbox("ç™ºæ˜è€… (ä»»æ„):", columns_with_none, index=smart_map_index(st.session_state.col_map.get('inventor'), columns_with_none, kw_inventor), key="col_inventor")
                inventor_delimiter = st.text_input("ç™ºæ˜è€…åŒºåˆ‡ã‚Šæ–‡å­—:", value=st.session_state.delimiters.get('inventor', ';'), key="del_inventor")

            with col3:
                st.markdown("##### åˆ†æè»¸é …ç›®")
                col_map['ipc'] = st.selectbox("IPC:", columns_with_none, index=smart_map_index(st.session_state.col_map.get('ipc'), columns_with_none, kw_ipc), key="col_ipc")
                ipc_delimiter = st.text_input("IPCåŒºåˆ‡ã‚Šæ–‡å­—:", value=st.session_state.delimiters.get('ipc', ';'), key="del_ipc")
                col_map['fterm'] = st.selectbox("Fã‚¿ãƒ¼ãƒ  (ä»»æ„):", columns_with_none, index=smart_map_index(st.session_state.col_map.get('fterm'), columns_with_none, kw_fterm), key="col_fterm")
                fterm_delimiter = st.text_input("Fã‚¿ãƒ¼ãƒ åŒºåˆ‡ã‚Šæ–‡å­—:", value=st.session_state.delimiters.get('fterm', ';'), key="del_fterm") 
                
            st.session_state.col_map = col_map
            st.session_state.delimiters = {
                'applicant': applicant_delimiter,
                'inventor': inventor_delimiter,
                'ipc': ipc_delimiter,
                'fterm': fterm_delimiter
            }
        else:
            st.info("ãƒ•ã‚§ãƒ¼ã‚º1ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹ã¨ã€ã‚«ãƒ©ãƒ ç´ä»˜ã‘è¨­å®šãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")

    # A-3. å‰å‡¦ç†å®Ÿè¡Œ
    with tab3:
        st.markdown("##### å…¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å…±é€šã®åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã‚’èµ·å‹•ã—ã¾ã™ã€‚")
        st.write("ãƒ‡ãƒ¼ã‚¿é‡ã«å¿œã˜ã¦æ•°åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚")

        if st.button("åˆ†æã‚¨ãƒ³ã‚¸ãƒ³èµ·å‹• (SBERT/TF-IDF)", type="primary", key="run_preprocess"):
            required_cols = ['title', 'abstract', 'claim', 'app_num', 'date', 'applicant', 'ipc']
            
            if st.session_state.df_main is None:
                st.error("ãƒ•ã‚§ãƒ¼ã‚º1ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
            elif any(v is None for k, v in st.session_state.col_map.items() if k in required_cols):
                missing = [k for k, v in st.session_state.col_map.items() if v is None and k in required_cols]
                st.error(f"ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚§ãƒ¼ã‚º2ã®å¿…é ˆã‚«ãƒ©ãƒ ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“: {missing}")
            else:
                try:
                    progress_bar = st.progress(0.0)
                    status_text = st.empty()
                    
                    start_time = time.time()
                    
                    phases = {
                        'init': 0.05,
                        'text': 0.05,
                        'sbert': 0.70,
                        'tfidf': 0.10,
                        'norm': 0.08,
                        'clean': 0.02
                    }

                    def update_progress(phase_key, phase_progress=0.0):
                        cumulative = 0.0
                        for k, w in phases.items():
                            if k == phase_key:
                                cumulative += w * phase_progress
                                break
                            else:
                                cumulative += w
                        
                        total_progress = min(0.99, cumulative)
                        
                        elapsed = time.time() - start_time
                        if total_progress > 0.01:
                            estimated_total = elapsed / total_progress
                            remaining = estimated_total - elapsed
                            eta_str = f"{int(remaining // 60):02}:{int(remaining % 60):02}"
                        else:
                            eta_str = "--:--"
                            
                        elapsed_str = f"{int(elapsed // 60):02}:{int(elapsed % 60):02}"
                        
                        progress_bar.progress(total_progress)
                        return elapsed_str, eta_str

                    # 1. ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ (Init)
                    status_text.markdown("ğŸ”„ **Phase 1/6: ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­...**")
                    update_progress('init', 0.5)
                    
                    df = st.session_state.df_main.copy() 
                    col_map = st.session_state.col_map
                    delimiters = st.session_state.delimiters
                    
                    sbert_model = load_sbert_model()
                    st.session_state.sbert_model = sbert_model
                    update_progress('init', 1.0)

                    # 2. ãƒ†ã‚­ã‚¹ãƒˆçµåˆ (Text)
                    status_text.markdown("ğŸ”„ **Phase 2/6: ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’çµåˆä¸­...**")
                    df['text_for_sbert'] = (
                        df[col_map['title']].fillna('') + ' ' +
                        df[col_map['abstract']].fillna('') + ' ' +
                        df[col_map['claim']].fillna('')
                    )
                    update_progress('text', 1.0)

                    # 3. SBERTã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ (SBERT)
                    texts_for_sbert_list = df['text_for_sbert'].tolist()
                    batch_size = 128
                    total_batches = (len(texts_for_sbert_list) + batch_size - 1) // batch_size
                    embeddings_list = []
                    
                    for i in range(total_batches):
                        batch_texts = texts_for_sbert_list[i*batch_size : (i+1)*batch_size]
                        batch_embeddings = sbert_model.encode(batch_texts, show_progress_bar=False)
                        embeddings_list.append(batch_embeddings)
                        
                        phase_prog = (i + 1) / total_batches
                        el_str, et_str = update_progress('sbert', phase_prog)
                        status_text.markdown(f"ğŸ”„ **Phase 3/6: AIãƒ™ã‚¯ãƒˆãƒ«åŒ– (SBERT) å®Ÿè¡Œä¸­...** (Batch {i+1}/{total_batches})\n\nâ±ï¸ çµŒé: {el_str} | â³ æ®‹ã‚Š: {et_str} (ç›®å®‰)")
                    
                    sbert_embeddings = np.vstack(embeddings_list)
                    sbert_embeddings = normalize(sbert_embeddings, norm='l2')
                    st.session_state.sbert_embeddings = sbert_embeddings

                    # 4. TF-IDF (TF-IDF)
                    status_text.markdown("ğŸ”„ **Phase 4/6: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º (TF-IDF) è¨ˆç®—ä¸­...**")
                    df['text_for_tfidf'] = df['text_for_sbert'].apply(advanced_tokenize)
                    vectorizer = TfidfVectorizer(max_features=None, min_df=5, max_df=0.80)
                    st.session_state.tfidf_matrix = vectorizer.fit_transform(df['text_for_tfidf'])
                    st.session_state.feature_names = np.array(vectorizer.get_feature_names_out())
                    update_progress('tfidf', 1.0)

                    # 5. æ­£è¦åŒ– (Norm)
                    status_text.markdown("ğŸ”„ **Phase 5/6: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ (æ—¥ä»˜ãƒ»IPCãƒ»å‡ºé¡˜äºº) æ­£è¦åŒ–ä¸­...**")
                    raw_dates = df[col_map['date']].astype(str)
                    df['parsed_date'] = robust_parse_date(raw_dates)
                    df['year'] = df['parsed_date'].dt.year
                    df['app_num_main'] = df[col_map['app_num']].astype(str).str.strip()

                    ipc_delimiter = delimiters['ipc']
                    df['ipc_normalized'] = df[col_map['ipc']].apply(lambda x: extract_ipc(x, ipc_delimiter))
                    ipc_raw_list = df[col_map['ipc']].fillna('').astype(str).str.split(ipc_delimiter)
                    df['ipc_main_group'] = ipc_raw_list.apply(lambda terms: list(set([t.strip().split('/')[0].strip().upper() for t in terms if t.strip()])))

                    if col_map['fterm']:
                        fterm_delimiter = delimiters['fterm']
                        fterm_raw_list = df[col_map['fterm']].fillna('').astype(str).str.split(fterm_delimiter)
                        df['fterm_main'] = fterm_raw_list.apply(lambda terms: list(set([t.strip()[:5].upper() for t in terms if t.strip() and len(t) >= 5])))
                    else:
                        df['fterm_main'] = [[] for _ in range(len(df))]

                    applicant_delimiter = delimiters['applicant']
                    applicant_raw_list = df[col_map['applicant']].fillna('').astype(str).str.split(applicant_delimiter)
                    df['applicant_main'] = applicant_raw_list.apply(lambda names: list(set([n.strip() for n in names if n.strip()])))
                    
                    if col_map['inventor'] and col_map['inventor'] in df.columns:
                        inventor_delimiter = delimiters['inventor']
                        def clean_inventors(val):
                            if pd.isna(val): return []
                            val = str(val).replace('â–²', '').replace('â–¼', '').replace('ã€€', '')
                            return list(set([n.strip() for n in val.split(inventor_delimiter) if n.strip()]))
                        df['inventor_main'] = df[col_map['inventor']].apply(clean_inventors)
                    else:
                        df['inventor_main'] = [[] for _ in range(len(df))]
                    update_progress('norm', 1.0)
                    
                    # 6. ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— (Clean)
                    status_text.markdown("ğŸ”„ **Phase 6/6: æœ€çµ‚å‡¦ç†ä¸­...**")
                    df.drop(columns=['text_for_sbert'], errors='ignore', inplace=True)
                    st.session_state.df_main = df 
                    st.session_state.shared_df = df 
                    st.session_state.preprocess_done = True
                    update_progress('clean', 1.0)
                    
                    # å®Œäº†
                    progress_bar.progress(1.0)
                    status_text.success(f"âœ… åˆ†æã‚¨ãƒ³ã‚¸ãƒ³èµ·å‹•å®Œäº† (æ‰€è¦æ™‚é–“: {int(time.time() - start_time)}ç§’)")
                    st.info("ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã‹ã‚‰åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é¸æŠã—ã€ãƒŸãƒƒã‚·ãƒ§ãƒ³ã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚")

                except Exception as e:
                    st.error(f"å‰å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                    import traceback
                    st.exception(traceback.format_exc())