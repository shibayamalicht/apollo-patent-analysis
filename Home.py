# ==================================================================
# --- ç’°å¢ƒè¨­å®š ---
# ==================================================================
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ['OMP_NUM_THREADS'] = '1'

# ==================================================================
# --- ãƒ©ã‚¤ãƒ–ãƒ©ãƒª ---
# ==================================================================
import streamlit as st
import pandas as pd
import numpy as np
import warnings
import traceback
import unicodedata
import re
import time
import datetime

from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import normalize
from janome.tokenizer import Tokenizer

warnings.filterwarnings('ignore')

# ==================================================================
# --- ãƒšãƒ¼ã‚¸è¨­å®š ---
# ==================================================================
st.set_page_config(
    page_title="APOLLO | Mission Control", 
    page_icon="ğŸ›°ï¸", 
    layout="wide",
    initial_sidebar_state="expanded"
)

# ==================================================================
# --- å®šæ•°ã¨ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ---
# ==================================================================

import io

import utils




@st.cache_resource
def load_sbert_model():
    return SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

@st.cache_resource
def load_tokenizer():
    return Tokenizer()

t = load_tokenizer()

def advanced_tokenize(text):
    # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚’å‹•çš„ã«å–å¾—
    if 'stopwords' in st.session_state and st.session_state['stopwords']:
        current_stopwords = st.session_state['stopwords']
    else:
        current_stopwords = utils.get_stopwords()

    if not isinstance(text, str): return ""
    text = unicodedata.normalize('NFKC', text).lower()
    text = re.sub(r'[\(ï¼ˆ][\w\s]+[\)ï¼‰]', ' ', text)
    text = re.sub(r'\b(å›³|fig|step|s)\s?\d+\b', ' ', text)
    text = re.sub(r'[!\"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~]', ' ', text)
    
    tokens = list(t.tokenize(text))
    processed_tokens = []
    i = 0
    while i < len(tokens):
        token1 = tokens[i]
        base_form = token1.base_form if token1.base_form != '*' else token1.surface
        
        if base_form in current_stopwords or len(base_form) < 2:
            i += 1
            continue
        
        if (i + 1) < len(tokens):
            token2 = tokens[i+1]
            base_form2 = token2.base_form if token2.base_form != '*' else token2.surface
            pos1 = token1.part_of_speech.split(',')[0]
            pos2 = token2.part_of_speech.split(',')[0]
            if pos1 == 'åè©' and pos2 == 'åè©' and base_form2 not in current_stopwords:
                compound_word = base_form + base_form2
                processed_tokens.append(compound_word)
                i += 2
                continue
        
        pos = token1.part_of_speech.split(',')[0]
        if pos == 'åè©':
            processed_tokens.append(base_form)
        i += 1
    return " ".join(processed_tokens)

def robust_parse_date(series):
    parsed = pd.to_datetime(series, errors='coerce')
    if parsed.notna().mean() > 0.5: return parsed
    
    parsed = pd.to_datetime(series, format='%Y%m%d', errors='coerce')
    if parsed.notna().mean() > 0.5: return parsed
    
    parsed = pd.to_datetime(series, format='%Y', errors='coerce')
    if parsed.notna().mean() > 0.5: return parsed
    
    try:
        numeric_series = pd.to_numeric(series, errors='coerce')
        if numeric_series.notna().sum() > 0 and numeric_series.mean() > 30000:
            parsed = pd.to_datetime(numeric_series, unit='D', origin='1899-12-30', errors='coerce')
            return parsed
    except:
        pass
    return parsed

def extract_ipc(text, delimiter=';'):
    if not isinstance(text, str): return [] 
    text = unicodedata.normalize('NFKC', text).lower()
    text = re.sub(r'[\(ï¼ˆ][^)]*[\)ï¼‰]', ' ', text)
    ipc_codes = []
    parts = text.split(delimiter)
    for part in parts:
        part = part.strip()
        if not part: continue
        match = re.search(r'([a-z]\d{2}[a-z])\s*(\d{1,4}/\d{2,})', part)
        if match:
            ipc_code = match.group(1) + match.group(2)
            ipc_codes.append(ipc_code)
        else:
            match_main = re.search(r'\b([a-z]\d{2}[a-z])\b', part)
            if match_main:
                ipc_codes.append(match_main.group(1))
    return ipc_codes 

def smart_map_index(current_value, options, keywords):
    """
    ã‚«ãƒ©ãƒ ç´ä»˜ã‘ã®è‡ªå‹•åŒ–ãƒ­ã‚¸ãƒƒã‚¯
    """
    if current_value is not None and current_value in options:
        return options.index(current_value)
    
    valid_cols = options[1:]
    
    for kw in keywords:
        for col in valid_cols:
            if kw == str(col):
                return options.index(col)
                
    for kw in keywords:
        for col in valid_cols:
            if kw in str(col):
                return options.index(col)
                
    return 0

# ==================================================================
# --- ãƒ¡ã‚¤ãƒ³ç”»é¢æç”» ---
# ==================================================================

utils.render_sidebar()

st.title("ğŸ›°ï¸ Mission Control") 
st.markdown("ã“ã“ã¯ã€å…¨åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§å…±é€šã®ãƒ‡ãƒ¼ã‚¿æº–å‚™ã‚’è¡Œã†ã€ŒãƒŸãƒƒã‚·ãƒ§ãƒ³ãƒ»ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒãƒ–ï¼‰ã€ã§ã™ã€‚")

# --- ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆæœŸåŒ– ---
def initialize_session_state():
    defaults = {
        "df_main": None,
        "shared_df": None,
        "filename": "No File",
        "sbert_model": None,
        "sbert_embeddings": None,
        "tfidf_matrix": None,
        "feature_names": None,
        "col_map": {},
        "delimiters": {'applicant': ';', 'inventor': ';', 'ipc': ';', 'fterm': ';'},
        "preprocess_done": False
    }
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value

initialize_session_state()


st.markdown("<h3 style='border: none; padding-bottom: 0;'>åˆ†æè¨­å®š</h3>", unsafe_allow_html=True)

container = st.container() 

with container:
    tab1, tab2, tab3, tab4 = st.tabs([
        "ãƒ•ã‚§ãƒ¼ã‚º 1: ãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆ", 
        "ãƒ•ã‚§ãƒ¼ã‚º 2: ã‚«ãƒ©ãƒ ç´ä»˜ã‘", 
        "ãƒ•ã‚§ãƒ¼ã‚º 3: ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ç®¡ç†",
        "ãƒ•ã‚§ãƒ¼ã‚º 4: åˆ†æã‚¨ãƒ³ã‚¸ãƒ³èµ·å‹•"
    ])

    # A-1. ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    with tab1:
        st.markdown("##### åˆ†æå¯¾è±¡ã®ç‰¹è¨±ãƒªã‚¹ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦ãã ã•ã„ã€‚")
        uploaded_file = st.file_uploader(
            "åˆ†æãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (CSV ã¾ãŸã¯ Excel)", 
            type=["csv", "xlsx", "xls"],
            label_visibility="collapsed"
        )
        
        if uploaded_file is not None:
            try:
                if uploaded_file.name.lower().endswith('.csv'):
                    try:
                        df = pd.read_csv(uploaded_file, dtype=str)
                    except UnicodeDecodeError:
                        df = pd.read_csv(uploaded_file, dtype=str, encoding='shift_jis')
                else:
                    df = pd.read_excel(uploaded_file, dtype=str)
                
                st.session_state.df_main = df
                st.session_state.preprocess_done = False 
                st.session_state['shared_df'] = df  
                st.session_state['filename'] = uploaded_file.name

                st.success(f"ãƒ•ã‚¡ã‚¤ãƒ« '{uploaded_file.name}' ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº† ({len(df)}è¡Œ)ã€‚")
                st.dataframe(df.head())
                
            except Exception as e:
                st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
                st.session_state.df_main = None
                st.session_state.shared_df = None
                
    # A-2. ã‚«ãƒ©ãƒ ç´ä»˜ã‘
    with tab2:
        if st.session_state.df_main is not None:
            df = st.session_state.df_main
            columns_with_none = [None] + list(df.columns)
            
            kw_title = ['ç™ºæ˜ã®åç§°', 'åç§°', 'Title', 'Title of Invention']
            kw_abstract = ['è¦ç´„', 'è¦ç´„(æŠ„éŒ²)', 'Abstract']
            kw_claim = ['è«‹æ±‚é …', 'Claim']
            kw_app_num = ['å‡ºé¡˜ç•ªå·', 'Application Number', 'App No']
            kw_date = ['å‡ºé¡˜æ—¥', 'å‡ºé¡˜æ—¥ï¼ˆé¡åŠï¼‰', 'Date', 'Filing']
            kw_applicant = ['å‡ºé¡˜äºº', 'Applicant', 'Assignee']
            kw_inventor = ['ç™ºæ˜è€…', 'Inventor']
            kw_ipc = ['å›½éš›ç‰¹è¨±åˆ†é¡', 'å›½éš›ç‰¹è¨±åˆ†é¡(IPC)', 'IPC', 'Int. Cl']
            kw_fterm = ['Fã‚¿ãƒ¼ãƒ ', 'ãƒ†ãƒ¼ãƒã‚³ãƒ¼ãƒ‰', 'F-Term']

            col_map = {}
            col1, col2, col3 = st.columns(3)
            with col1:
                st.markdown("##### å¿…é ˆãƒ†ã‚­ã‚¹ãƒˆé …ç›®")
                col_map['title'] = st.selectbox("ç™ºæ˜ã®åç§°:", columns_with_none, index=smart_map_index(st.session_state.col_map.get('title'), columns_with_none, kw_title), key="col_title")
                col_map['abstract'] = st.selectbox("è¦ç´„:", columns_with_none, index=smart_map_index(st.session_state.col_map.get('abstract'), columns_with_none, kw_abstract), key="col_abstract")
                col_map['claim'] = st.selectbox("è«‹æ±‚é …:", columns_with_none, index=smart_map_index(st.session_state.col_map.get('claim'), columns_with_none, kw_claim), key="col_claim")
            with col2:
                st.markdown("##### å¿…é ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿é …ç›®")
                col_map['app_num'] = st.selectbox("å‡ºé¡˜ç•ªå·:", columns_with_none, index=smart_map_index(st.session_state.col_map.get('app_num'), columns_with_none, kw_app_num), key="col_app_num")
                col_map['date'] = st.selectbox("å‡ºé¡˜æ—¥:", columns_with_none, index=smart_map_index(st.session_state.col_map.get('date'), columns_with_none, kw_date), key="col_date")
                col_map['applicant'] = st.selectbox("å‡ºé¡˜äºº:", columns_with_none, index=smart_map_index(st.session_state.col_map.get('applicant'), columns_with_none, kw_applicant), key="col_applicant")
                applicant_delimiter = st.text_input("å‡ºé¡˜äººåŒºåˆ‡ã‚Šæ–‡å­—:", value=st.session_state.delimiters.get('applicant', ';'), key="del_applicant")

                # IPC (Required)
                col_map['ipc'] = st.selectbox("å›½éš›ç‰¹è¨±åˆ†é¡ (IPC):", columns_with_none, index=smart_map_index(st.session_state.col_map.get('ipc'), columns_with_none, kw_ipc), key="col_ipc")
                ipc_delimiter = st.text_input("IPCåŒºåˆ‡ã‚Šæ–‡å­—:", value=st.session_state.delimiters.get('ipc', ';'), key="del_ipc")
                
            with col3:
                st.markdown("##### ä»»æ„ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿é …ç›®")
                
                # Inventor
                col_map['inventor'] = st.selectbox("ç™ºæ˜è€… (ä»»æ„):", columns_with_none, index=smart_map_index(st.session_state.col_map.get('inventor'), columns_with_none, kw_inventor), key="col_inventor")
                inventor_delimiter = st.text_input("ç™ºæ˜è€…åŒºåˆ‡ã‚Šæ–‡å­—:", value=st.session_state.delimiters.get('inventor', ';'), key="del_inventor")
                

                
                # F-term
                col_map['fterm'] = st.selectbox("Fã‚¿ãƒ¼ãƒ  (ä»»æ„):", columns_with_none, index=smart_map_index(st.session_state.col_map.get('fterm'), columns_with_none, kw_fterm), key="col_fterm")
                fterm_delimiter = st.text_input("Fã‚¿ãƒ¼ãƒ åŒºåˆ‡ã‚Šæ–‡å­—:", value=st.session_state.delimiters.get('fterm', ';'), key="del_fterm") 
                
                # Status
                col_map['status'] = st.selectbox("ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ (ä»»æ„):", columns_with_none, index=smart_map_index(st.session_state.col_map.get('status'), columns_with_none, ['ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹', 'Status', 'Legal Status', 'æ³•çš„çŠ¶æ…‹']), key="col_status") 
                
            st.session_state.col_map = col_map
            st.session_state.delimiters = {
                'applicant': applicant_delimiter,
                'inventor': inventor_delimiter,
                'ipc': ipc_delimiter,
                'fterm': fterm_delimiter
            }
        else:
            st.info("ãƒ•ã‚§ãƒ¼ã‚º1ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹ã¨ã€ã‚«ãƒ©ãƒ ç´ä»˜ã‘è¨­å®šãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")

    # A-3. ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ç®¡ç†
    with tab3:
        st.markdown("##### åˆ†æã‹ã‚‰é™¤å¤–ã™ã‚‹ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚’ç®¡ç†ã—ã¾ã™ã€‚")
        
        # åˆæœŸåŒ–
        if 'stopwords' not in st.session_state or not st.session_state['stopwords']:
            st.session_state['stopwords'] = utils.get_stopwords()
        
        # æ¤œç´¢æ©Ÿèƒ½
        search_query = st.text_input("ãƒªã‚¹ãƒˆå†…æ¤œç´¢ (æ­£è¦è¡¨ç¾ã‚‚å¯)", placeholder="æ¤œç´¢ã—ãŸã„å˜èªã‚’å…¥åŠ›...", key="sw_search")
        
        full_stopwords = sorted(list(st.session_state['stopwords']))
        
        if search_query:
            try:
                # æ­£è¦è¡¨ç¾æ¤œç´¢ã‚’è©¦ã¿ã‚‹
                filtered_stopwords = [w for w in full_stopwords if re.search(search_query, w)]
            except re.error:
                # æ­£è¦è¡¨ç¾ã‚¨ãƒ©ãƒ¼æ™‚ã¯å˜ç´”ãªéƒ¨åˆ†ä¸€è‡´
                filtered_stopwords = [w for w in full_stopwords if search_query in w]
            is_filtered = True
        else:
            filtered_stopwords = full_stopwords
            is_filtered = False
            
        stopwords_text = "\n".join(filtered_stopwords)
        
        c1, c2 = st.columns([2, 1])
        with c1:
            label_suffix = f" (è¡¨ç¤ºä¸­: {len(filtered_stopwords)} / å…¨ {len(full_stopwords)} èª)" if is_filtered else f" (å…¨ {len(full_stopwords)} èª)"
            if is_filtered:
                st.warning("âš ï¸ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ä¸­: ã“ã“ã§ã®ç·¨é›†ï¼ˆè¿½åŠ ãƒ»å‰Šé™¤ï¼‰ã¯ã€è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹å˜èªã«å¯¾ã—ã¦é©ç”¨ã•ã‚Œã€ãƒ¡ã‚¤ãƒ³ãƒªã‚¹ãƒˆã«ãƒãƒ¼ã‚¸ã•ã‚Œã¾ã™ã€‚")
            

            editor_key = f"stopwords_editor_{hash(search_query)}" 
            new_stopwords_text = st.text_area(f"ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ{label_suffix}", value=stopwords_text, height=300, key=editor_key)
            
            if st.button("å¤‰æ›´ã‚’é©ç”¨", key="apply_stopwords"):
                edited_lines = set([line.strip() for line in new_stopwords_text.split('\n') if line.strip()])
                
                if is_filtered:
                    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ™‚ã®ã‚¹ãƒãƒ¼ãƒˆãƒãƒ¼ã‚¸
                    # 1. æ¤œç´¢ãƒ’ãƒƒãƒˆã—ã¦ã„ãŸã¯ãšã®å…ƒã®å˜èªç¾¤ (å¤‰æ›´å‰)
                    original_matches = set(filtered_stopwords)
                    # 2. å‰Šé™¤ã•ã‚ŒãŸå˜èª = (å…ƒãƒ’ãƒƒãƒˆ) - (ç·¨é›†å¾Œ)
                    removed_words = original_matches - edited_lines
                    # 3. è¿½åŠ ã•ã‚ŒãŸå˜èª = (ç·¨é›†å¾Œ) - (å…ƒãƒ’ãƒƒãƒˆ)
                    added_words = edited_lines - original_matches
                    
                    # 4. ãƒ¡ã‚¤ãƒ³ãƒªã‚¹ãƒˆã‹ã‚‰å‰Šé™¤å¯¾è±¡ã‚’é™¤ãã€è¿½åŠ åˆ†ã‚’è¶³ã™
                    current_set = st.session_state['stopwords']
                    new_set = (current_set - removed_words) | added_words
                    st.session_state['stopwords'] = new_set
                    msg = f"æ›´æ–°å®Œäº†: {len(added_words)} èªã‚’è¿½åŠ , {len(removed_words)} èªã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚"
                else:
                    # å…¨é‡ç½®æ›
                    st.session_state['stopwords'] = edited_lines
                    msg = f"ãƒªã‚¹ãƒˆã‚’å…¨é‡æ›´æ–°ã—ã¾ã—ãŸ (è¨ˆ {len(edited_lines)} èª)ã€‚"
                
                st.success(msg)
                st.rerun()

        with c2:
            st.markdown("**ã‚¤ãƒ³ãƒãƒ¼ãƒˆ / ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ**")
            
            # ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
            sw_file = st.file_uploader("ãƒªã‚¹ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ (.txt, .csv)", type=['txt', 'csv'], key="sw_uploader")
            if sw_file:
                try:
                    stringio = io.StringIO(sw_file.getvalue().decode("utf-8"))
                    imported_lines = [line.strip() for line in stringio.read().split('\n') if line.strip()]
                    if st.button(f"è¿½åŠ ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ({len(imported_lines)}èª)", key="import_sw"):
                        st.session_state['stopwords'].update(imported_lines)
                        st.success("ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã—ãŸã€‚")
                        st.rerun()
                except Exception as e:
                    st.error(f"èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

            # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
            st.download_button(
                label="ãƒªã‚¹ãƒˆã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ (.txt)",
                data="\n".join(sorted(list(st.session_state['stopwords']))),
                file_name="apollo_stopwords.txt",
                mime="text/plain"
            )
            
            st.markdown("---")
            if st.button("ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«æˆ»ã™", key="reset_stopwords"):
                st.session_state['stopwords'] = utils.get_stopwords()
                st.rerun()

    # A-4. å‰å‡¦ç†å®Ÿè¡Œ
    with tab4:
        st.markdown("##### å…¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å…±é€šã®åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã‚’èµ·å‹•ã—ã¾ã™ã€‚")
        st.write("ãƒ‡ãƒ¼ã‚¿é‡ã«å¿œã˜ã¦æ•°åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚")

        if st.button("åˆ†æã‚¨ãƒ³ã‚¸ãƒ³èµ·å‹• (SBERT/TF-IDF)", type="primary", key="run_preprocess"):
            required_cols = ['title', 'abstract', 'claim', 'app_num', 'date', 'applicant', 'ipc']
            
            if st.session_state.df_main is None:
                st.error("ãƒ•ã‚§ãƒ¼ã‚º1ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
            elif any(v is None for k, v in st.session_state.col_map.items() if k in required_cols):
                missing = [k for k, v in st.session_state.col_map.items() if v is None and k in required_cols]
                st.error(f"ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚§ãƒ¼ã‚º2ã®å¿…é ˆã‚«ãƒ©ãƒ ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“: {missing}")
            else:
                try:
                    progress_bar = st.progress(0.0)
                    status_text = st.empty()
                    
                    start_time = time.time()
                    
                    phases = {
                        'init': 0.05,
                        'text': 0.05,
                        'sbert': 0.70,
                        'tfidf': 0.10,
                        'norm': 0.08,
                        'clean': 0.02
                    }

                    def update_progress(phase_key, phase_progress=0.0):
                        cumulative = 0.0
                        for k, w in phases.items():
                            if k == phase_key:
                                cumulative += w * phase_progress
                                break
                            else:
                                cumulative += w
                        
                        total_progress = min(0.99, cumulative)
                        
                        elapsed = time.time() - start_time
                        if total_progress > 0.01:
                            estimated_total = elapsed / total_progress
                            remaining = estimated_total - elapsed
                            eta_str = f"{int(remaining // 60):02}:{int(remaining % 60):02}"
                        else:
                            eta_str = "--:--"
                            
                        elapsed_str = f"{int(elapsed // 60):02}:{int(elapsed % 60):02}"
                        
                        progress_bar.progress(total_progress)
                        return elapsed_str, eta_str

                    # 1. ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ (Init)
                    status_text.markdown("ğŸ”„ **Phase 1/6: ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­...**")
                    update_progress('init', 0.5)
                    
                    df = st.session_state.df_main.copy() 
                    col_map = st.session_state.col_map
                    delimiters = st.session_state.delimiters
                    
                    sbert_model = load_sbert_model()
                    st.session_state.sbert_model = sbert_model
                    update_progress('init', 1.0)

                    # 2. ãƒ†ã‚­ã‚¹ãƒˆçµåˆ (Text)
                    status_text.markdown("ğŸ”„ **Phase 2/6: ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’çµåˆä¸­...**")
                    df['text_for_sbert'] = (
                        df[col_map['title']].fillna('') + ' ' +
                        df[col_map['abstract']].fillna('') + ' ' +
                        df[col_map['claim']].fillna('')
                    )
                    update_progress('text', 1.0)

                    # 3. SBERTã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ (SBERT)
                    texts_for_sbert_list = df['text_for_sbert'].tolist()
                    batch_size = 128
                    total_batches = (len(texts_for_sbert_list) + batch_size - 1) // batch_size
                    embeddings_list = []
                    
                    for i in range(total_batches):
                        batch_texts = texts_for_sbert_list[i*batch_size : (i+1)*batch_size]
                        batch_embeddings = sbert_model.encode(batch_texts, show_progress_bar=False)
                        embeddings_list.append(batch_embeddings)
                        
                        phase_prog = (i + 1) / total_batches
                        el_str, et_str = update_progress('sbert', phase_prog)
                        status_text.markdown(f"ğŸ”„ **Phase 3/6: AIãƒ™ã‚¯ãƒˆãƒ«åŒ– (SBERT) å®Ÿè¡Œä¸­...** (Batch {i+1}/{total_batches})\n\nâ±ï¸ çµŒé: {el_str} | â³ æ®‹ã‚Š: {et_str} (ç›®å®‰)")
                    
                    sbert_embeddings = np.vstack(embeddings_list)
                    sbert_embeddings = normalize(sbert_embeddings, norm='l2')
                    st.session_state.sbert_embeddings = sbert_embeddings

                    # 4. TF-IDF (TF-IDF)
                    status_text.markdown("ğŸ”„ **Phase 4/6: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º (TF-IDF) è¨ˆç®—ä¸­...**")
                    df['text_for_tfidf'] = df['text_for_sbert'].apply(advanced_tokenize)
                    vectorizer = TfidfVectorizer(max_features=None, min_df=5, max_df=0.80)
                    st.session_state.tfidf_matrix = vectorizer.fit_transform(df['text_for_tfidf'])
                    st.session_state.feature_names = np.array(vectorizer.get_feature_names_out())
                    update_progress('tfidf', 1.0)

                    # 5. æ­£è¦åŒ– (Norm)
                    status_text.markdown("ğŸ”„ **Phase 5/6: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ (æ—¥ä»˜ãƒ»IPCãƒ»å‡ºé¡˜äºº) æ­£è¦åŒ–ä¸­...**")
                    raw_dates = df[col_map['date']].astype(str)
                    df['parsed_date'] = robust_parse_date(raw_dates)
                    df['year'] = df['parsed_date'].dt.year
                    df['app_num_main'] = df[col_map['app_num']].astype(str).str.strip()

                    ipc_delimiter = delimiters['ipc']
                    df['ipc_normalized'] = df[col_map['ipc']].apply(lambda x: extract_ipc(x, ipc_delimiter))
                    ipc_raw_list = df[col_map['ipc']].fillna('').astype(str).str.split(ipc_delimiter)
                    df['ipc_main_group'] = ipc_raw_list.apply(lambda terms: list(set([t.strip().split('/')[0].strip().upper() for t in terms if t.strip()])))

                    if col_map['fterm']:
                        fterm_delimiter = delimiters['fterm']
                        fterm_raw_list = df[col_map['fterm']].fillna('').astype(str).str.split(fterm_delimiter)
                        df['fterm_main'] = fterm_raw_list.apply(lambda terms: list(set([t.strip()[:5].upper() for t in terms if t.strip() and len(t) >= 5])))
                    else:
                        df['fterm_main'] = [[] for _ in range(len(df))]

                    applicant_delimiter = delimiters['applicant']
                    applicant_raw_list = df[col_map['applicant']].fillna('').astype(str).str.split(applicant_delimiter)
                    df['applicant_main'] = applicant_raw_list.apply(lambda names: list(set([n.strip() for n in names if n.strip()])))
                    
                    if col_map['inventor'] and col_map['inventor'] in df.columns:
                        inventor_delimiter = delimiters['inventor']
                        def clean_inventors(val):
                            if pd.isna(val): return []
                            val = str(val).replace('â–²', '').replace('â–¼', '').replace('ã€€', '')
                            return list(set([n.strip() for n in val.split(inventor_delimiter) if n.strip()]))
                        df['inventor_main'] = df[col_map['inventor']].apply(clean_inventors)
                    else:
                        df['inventor_main'] = [[] for _ in range(len(df))]
                    update_progress('norm', 1.0)
                    
                    # 6. ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— (Clean)
                    status_text.markdown("ğŸ”„ **Phase 6/6: æœ€çµ‚å‡¦ç†ä¸­...**")
                    df.drop(columns=['text_for_sbert'], errors='ignore', inplace=True)
                    st.session_state.df_main = df 
                    st.session_state.shared_df = df 
                    st.session_state.preprocess_done = True
                    update_progress('clean', 1.0)
                    
                    # å®Œäº†
                    progress_bar.progress(1.0)
                    status_text.success(f"âœ… åˆ†æã‚¨ãƒ³ã‚¸ãƒ³èµ·å‹•å®Œäº† (æ‰€è¦æ™‚é–“: {int(time.time() - start_time)}ç§’)")
                    st.info("ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã‹ã‚‰åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é¸æŠã—ã€ãƒŸãƒƒã‚·ãƒ§ãƒ³ã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚")

                except Exception as e:
                    st.error(f"å‰å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                    import traceback
                    st.exception(traceback.format_exc())