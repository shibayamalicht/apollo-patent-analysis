# ==================================================================
# --- 1. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
# ==================================================================
import streamlit as st
import pandas as pd
import numpy as np
import io
import datetime
import warnings
import unicodedata
import re
import string
from collections import Counter
import os

# WordCloud / Janome
from wordcloud import WordCloud
from janome.tokenizer import Tokenizer

# çµ±è¨ˆãƒãƒƒãƒ—ç”¨
import matplotlib.pyplot as plt
import matplotlib.font_manager
import japanize_matplotlib # æ—¥æœ¬èªåŒ–

# è­¦å‘Šã‚’éè¡¨ç¤º
warnings.filterwarnings('ignore')

# ==================================================================
# --- 2. Explorerå°‚ç”¨ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ---
# ==================================================================

# Explorerã¯ç‹¬è‡ªã®Tokenizerã¨StopWords, n-gramã‚’æŒã¤
@st.cache_resource
def load_tokenizer_explorer():
    print("... Explorer: Janome Tokenizerã‚’ãƒ­ãƒ¼ãƒ‰ä¸­ ...")
    return Tokenizer()

t = load_tokenizer_explorer()

# Explorerå°‚ç”¨ã®ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰
stopwords = [
    "ã™ã‚‹","ã‚ã‚‹","ãªã‚‹","ãŸã‚","ã“ã¨","ã‚ˆã†","ã‚‚ã®","ã“ã‚Œ","ãã‚Œ","ã‚ã‚Œ","ã“ã“","ãã“","ã©ã‚Œ","ã©ã®",
    "ã“ã®","ãã®","å½“è©²","è©²","ãŠã‚ˆã³","åŠã³","ã¾ãŸã¯","ã¾ãŸ","ä¾‹ãˆã°","ä¾‹ãˆã°ã¯","ã«ãŠã„ã¦","ã«ã‚ˆã‚Š",
    "ã«å¯¾ã—ã¦","ã«é–¢ã—ã¦","ã«ã¤ã„ã¦","ã¨ã—ã¦","ã¨ã—ã¦ã¯","å ´åˆ","ä¸€æ–¹","ä»–æ–¹","ã•ã‚‰ã«","ãã—ã¦","ãŸã ã—",
    "ãªãŠ","ç­‰","ãªã©","ç­‰ã€…","ã„ã‚ã‚†ã‚‹","æ‰€è¬‚","åŒæ§˜","åŒæ™‚","å‰è¨˜","æœ¬","åŒ","å„","å„ç¨®","æ‰€å®š","æ‰€æœ›",
    "ä¸€ä¾‹","ä»–","ä¸€éƒ¨","ä¸€ã¤","è¤‡æ•°","å°‘ãªãã¨ã‚‚","å°‘ãªãã¨ã‚‚ä¸€ã¤","ä¸Šè¨˜","ä¸‹è¨˜","å‰è¿°","å¾Œè¿°","æ—¢è¿°",
    "é–¢ã™ã‚‹","åŸºã¥ã","ç”¨ã„ã‚‹","ä½¿ç”¨","åˆ©ç”¨","æœ‰ã™ã‚‹","å«ã‚€","å‚™ãˆã‚‹","è¨­ã‘ã‚‹","ã™ãªã‚ã¡","å¾“ã£ã¦",
    "ã—ã‹ã—ãªãŒã‚‰","æ¬¡ã«","ç‰¹ã«","å…·ä½“çš„ã«","è©³ç´°ã«","ã„ãšã‚Œ","ã†ã¡","ãã‚Œãã‚Œ","ã¨ã",
    "ã‹ã‹ã‚‹","ã‹ã‚ˆã†ãª","ã‹ã‹ã‚‹å ´åˆ","æœ¬ä»¶","æœ¬é¡˜","æœ¬å‡ºé¡˜","æœ¬æ˜ç´°æ›¸","ã“ã‚Œã‚‰","ãã‚Œã‚‰","å„ã€…","éšæ™‚","é©å®œ",
    "ä»»æ„","å¿…ãšã—ã‚‚","é€šå¸¸","ä¸€èˆ¬ã«","å…¸å‹çš„","ä»£è¡¨çš„",
    "æœ¬ç™ºæ˜","ç™ºæ˜","å®Ÿæ–½ä¾‹","å®Ÿæ–½å½¢æ…‹","å¤‰å½¢ä¾‹","è«‹æ±‚","è«‹æ±‚é …","å›³","å›³é¢","ç¬¦å·","ç¬¦å·ã®èª¬æ˜",
    "å›³é¢ã®ç°¡å˜ãªèª¬æ˜","ç™ºæ˜ã®è©³ç´°ãªèª¬æ˜","æŠ€è¡“åˆ†é‡","èƒŒæ™¯æŠ€è¡“","å¾“æ¥æŠ€è¡“","ç™ºæ˜ãŒè§£æ±ºã—ã‚ˆã†ã¨ã™ã‚‹èª²é¡Œ","èª²é¡Œ",
    "è§£æ±ºæ‰‹æ®µ","åŠ¹æœ","è¦ç´„","ç™ºæ˜ã®åŠ¹æœ","ç›®çš„","æ‰‹æ®µ","æ§‹æˆ","æ§‹é€ ","å·¥ç¨‹","å‡¦ç†","æ–¹æ³•","æ‰‹æ³•","æ–¹å¼",
    "ã‚·ã‚¹ãƒ†ãƒ ","ãƒ—ãƒ­ã‚°ãƒ©ãƒ ","è¨˜æ†¶åª’ä½“","ç‰¹å¾´","ç‰¹å¾´ã¨ã™ã‚‹","ç‰¹å¾´éƒ¨","ã‚¹ãƒ†ãƒƒãƒ—","ãƒ•ãƒ­ãƒ¼","ã‚·ãƒ¼ã‚±ãƒ³ã‚¹","å®šç¾©",
    "é–¢ä¿‚","å¯¾å¿œ","æ•´åˆ",
    "å®Ÿæ–½ã®å½¢æ…‹","å®Ÿæ–½ã®æ…‹æ§˜","æ…‹æ§˜","å¤‰å½¢","ä¿®æ­£ä¾‹","å›³ç¤º","å›³ç¤ºä¾‹","å›³ç¤ºã—ãªã„","å‚ç…§","å‚ç…§ç¬¦å·","æ®µè½",
    "è©³ç´°èª¬æ˜","è¦æ—¨","ä¸€å®Ÿæ–½å½¢æ…‹","ä»–ã®å®Ÿæ–½å½¢æ…‹","ä¸€å®Ÿæ–½ä¾‹","åˆ¥ã®å´é¢","ä»˜è¨˜","é©ç”¨ä¾‹","ç”¨èªã®å®šç¾©","é–‹ç¤º","æœ¬é–‹ç¤º","é–‹ç¤ºå†…å®¹",
    "ä¸Šéƒ¨","ä¸‹éƒ¨","å†…éƒ¨","å¤–éƒ¨","å†…å´","å¤–å´","è¡¨é¢","è£é¢","å´é¢","ä¸Šé¢","ä¸‹é¢","ç«¯é¢","å…ˆç«¯","åŸºç«¯","å¾Œç«¯",
    "ä¸€ç«¯","ä»–ç«¯","ä¸­å¿ƒ","ä¸­å¤®","å‘¨ç¸","å‘¨è¾º","è¿‘å‚","æ–¹å‘","ä½ç½®","ç©ºé–“","é ˜åŸŸ","ç¯„å›²","é–“éš”","è·é›¢","å½¢çŠ¶",
    "å½¢æ…‹","çŠ¶æ…‹","ç¨®é¡","å±¤","è†œ","éƒ¨","éƒ¨æ","éƒ¨ä½","éƒ¨å“","æ©Ÿæ§‹","è£…ç½®","å®¹å™¨","çµ„æˆ","ææ–™","ç”¨é€”","é©ç”¨",
    "é©ç”¨ä¾‹",
    "ç‰‡å´","ä¸¡å´","å·¦å´","å³å´","å‰æ–¹","å¾Œæ–¹","ä¸Šæµ","ä¸‹æµ","éš£æ¥","è¿‘æ¥","é›¢é–“","é–“ç½®","ä»‹åœ¨","é‡ç•³","æ¦‚ã­",
    "ç•¥","ç•¥ä¸­å¤®","å›ºå®šå´","å¯å‹•å´","ä¼¸é•·","åç¸®","ä¿‚åˆ","åµŒåˆ","å–ä»˜","é€£çµéƒ¨","æ”¯æŒä½“","æ”¯æŒéƒ¨","ã‚¬ã‚¤ãƒ‰éƒ¨",
    "ãƒ‡ãƒ¼ã‚¿","æƒ…å ±","ä¿¡å·","å‡ºåŠ›","å…¥åŠ›","åˆ¶å¾¡","æ¼”ç®—","å–å¾—","é€ä¿¡","å—ä¿¡","è¡¨ç¤º","é€šçŸ¥","è¨­å®š","å¤‰æ›´","æ›´æ–°",
    "ä¿å­˜","å‰Šé™¤","è¿½åŠ ","å®Ÿè¡Œ","é–‹å§‹","çµ‚äº†","ç¶™ç¶š","åœæ­¢","åˆ¤å®š","åˆ¤æ–­","æ±ºå®š","é¸æŠ","ç‰¹å®š","æŠ½å‡º","æ¤œå‡º",
    "æ¤œçŸ¥","æ¸¬å®š","è¨ˆæ¸¬","ç§»å‹•","å›è»¢","å¤‰ä½","å¤‰å½¢","å›ºå®š","é…ç½®",
    "ç”Ÿæˆ","ä»˜ä¸","ä¾›çµ¦","é©ç”¨","ç…§åˆ","æ¯”è¼ƒ","ç®—å‡º","è§£æ","åŒå®š","åˆæœŸåŒ–","èª­å‡º","æ›¸è¾¼","ç™»éŒ²","è¨˜éŒ²","é…ä¿¡",
    "é€£æº","åˆ‡æ›¿","èµ·å‹•","å¾©å¸°","ç›£è¦–","é€šçŸ¥å‡¦ç†","å–å¾—å‡¦ç†","æ¼”ç®—å‡¦ç†",
    "è‰¯å¥½","å®¹æ˜“","ç°¡ä¾¿","é©åˆ‡","æœ‰åˆ©","æœ‰ç”¨","æœ‰åŠ¹","åŠ¹æœçš„","é«˜ã„","ä½ã„","å¤§ãã„","å°ã•ã„","æ–°è¦","æ”¹è‰¯","æ”¹å–„",
    "æŠ‘åˆ¶","å‘ä¸Š","ä½æ¸›","å‰Šæ¸›","å¢—åŠ ","æ¸›å°‘","å¯èƒ½","å¥½é©","å¥½ã¾ã—ã„","æœ›ã¾ã—ã„","å„ªã‚Œã‚‹","å„ªã‚ŒãŸ","é«˜æ€§èƒ½",
    "é«˜åŠ¹ç‡","ä½ã‚³ã‚¹ãƒˆ","ã‚³ã‚¹ãƒˆ","ç°¡æ˜“","å®‰å®š","å®‰å®šæ€§","è€ä¹…","è€ä¹…æ€§","ä¿¡é ¼æ€§","ç°¡ç´ ","ç°¡ç•¥","å˜ç´”","æœ€é©",
    "æœ€é©åŒ–","æ±ç”¨","æ±ç”¨æ€§","å®Ÿç¾","é”æˆ","ç¢ºä¿","ç¶­æŒ","é˜²æ­¢","å›é¿","ä¿ƒé€²","ä¸è¦","å¿…è¦",
    "é«˜ç²¾åº¦","çœé›»åŠ›","çœè³‡æº","é«˜ä¿¡é ¼","ä½è² è·","é«˜ç´”åº¦","é«˜å¯†åº¦","é«˜æ„Ÿåº¦","è¿…é€Ÿ","å††æ»‘","ç°¡ç•¥åŒ–","ä½ä¾¡æ ¼","å®ŸåŠ¹çš„",
    "å¯èƒ½åŒ–","æœ‰åŠ¹åŒ–","éå¿…é ˆ","é©åˆ","äº’æ›",
    "å‡ºé¡˜","å‡ºé¡˜äºº","å‡ºé¡˜ç•ªå·","å‡ºé¡˜æ—¥","å‡ºé¡˜æ›¸","å‡ºé¡˜å…¬é–‹","å…¬é–‹","å…¬é–‹ç•ªå·","å…¬é–‹å…¬å ±","å…¬å ±","å…¬å ±ç•ªå·",
    "ç‰¹è¨±","ç‰¹è¨±ç•ªå·","ç‰¹è¨±æ–‡çŒ®","éç‰¹è¨±æ–‡çŒ®","å¼•ç”¨","å¼•ç”¨æ–‡çŒ®","å…ˆè¡ŒæŠ€è¡“","å¯©æŸ»","å¯©æŸ»å®˜","æ‹’çµ¶","æ„è¦‹æ›¸","è£œæ­£æ›¸",
    "å„ªå…ˆ","å„ªå…ˆæ—¥","åˆ†å‰²å‡ºé¡˜","ç¶™ç¶šå‡ºé¡˜","å›½å†…ç§»è¡Œ","å›½éš›å‡ºé¡˜","å›½éš›å…¬é–‹","PCT","ç™»éŒ²","å…¬é–‹æ—¥","å¯©æŸ»è«‹æ±‚",
    "æ‹’çµ¶ç†ç”±","è£œæ­£","è¨‚æ­£","ç„¡åŠ¹å¯©åˆ¤","ç•°è­°","å–æ¶ˆ","å–ä¸‹ã’","äº‹ä»¶ç•ªå·","ä»£ç†äºº","å¼ç†å£«","ä¿‚å±","çµŒé",
    "ç¬¬","ç¬¬ä¸€","ç¬¬äºŒ","ç¬¬ä¸‰","ç¬¬1","ç¬¬ï¼’","ç¬¬ï¼“","ç¬¬ï¼‘","ç¬¬ï¼’","ç¬¬ï¼“","ï¼‘","ï¼’","ï¼“","ï¼”","ï¼•","ï¼–","ï¼—","ï¼˜","ï¼™","ï¼",
    "ä¸€","äºŒ","ä¸‰","å››","äº”","å…­","ä¸ƒ","å…«","ä¹","é›¶","æ•°","è¤‡åˆ","å¤šæ•°","å°‘æ•°",
    "å›³1","å›³2","å›³3","å›³4","å›³5","å›³6","å›³7","å›³8","å›³9","è¡¨1","è¡¨2","è¡¨3","å¼1","å¼2","å¼3",
    "ï¼","ï¼‘","ï¼’","ï¼“","ï¼”","ï¼•","ï¼–","ï¼—","ï¼˜","ï¼™",
    "%","ï¼…","wt%","vol%","è³ªé‡%","é‡é‡%","å®¹é‡%","mol","mol%","mol/L","M","mm","cm","m","nm","Î¼m","Î¼","rpm",
    "Pa","kPa","MPa","GPa","N","W","V","A","mA","Hz","kHz","MHz","GHz","â„ƒ","Â°C","K","mL","L","g","kg","mg","wt","vol",
    "h","hr","hrs","min","s","sec","ppm","ppb","bar","Î©","ohm","J","kJ","Wh","kWh",
    "æ ªå¼ä¼šç¤¾","æœ‰é™ä¼šç¤¾","åˆè³‡ä¼šç¤¾","åˆåä¼šç¤¾","åˆåŒä¼šç¤¾","Inc","Inc.","Ltd","Ltd.","Co","Co.","Corp","Corp.","LLC",
    "GmbH","AG","BV","B.V.","S.A.","S.p.A.","ï¼ˆæ ªï¼‰","ãˆ±","ï¼ˆæœ‰ï¼‰",
    "æº¶æ¶²","æº¶åª’","è§¦åª’","åå¿œ","ç”Ÿæˆç‰©","åŸæ–™","æˆåˆ†","å«æœ‰","å«æœ‰é‡","é…åˆ","æ··åˆ","æ··åˆç‰©","æ¿ƒåº¦","æ¸©åº¦","æ™‚é–“",
    "å‰²åˆ","æ¯”ç‡","åŸº","å®˜èƒ½åŸº","åŒ–åˆç‰©","çµ„æˆç‰©","æ¨¹è„‚","ãƒãƒªãƒãƒ¼","ãƒ¢ãƒãƒãƒ¼","åŸºæ¿","åŸºæ","ãƒ•ã‚£ãƒ«ãƒ ","ã‚·ãƒ¼ãƒˆ",
    "ç²’å­","ç²‰æœ«","æ¯”è¼ƒä¾‹","å‚è€ƒä¾‹","è©¦é¨“","è©¦æ–™","è©•ä¾¡","æ¡ä»¶","å®Ÿé¨“","å®Ÿé¨“ä¾‹","åå¿œæ¡ä»¶","åå¿œæ™‚é–“","åå¿œæ¸©åº¦",
    "å‡¦ç†è£…ç½®","ç«¯æœ«","ãƒ¦ãƒ‹ãƒƒãƒˆ","ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«","å›è·¯","ç´ å­","é›»æº","é›»åœ§","é›»æµ","ä¿¡å·ç·š","é…ç·š","ç«¯å­","ç«¯éƒ¨","æ¥ç¶š",
    "æ¥ç¶šéƒ¨","æ¼”ç®—éƒ¨","è¨˜æ†¶éƒ¨","è¨˜æ†¶è£…ç½®","è¨˜éŒ²åª’ä½“","ãƒ¦ãƒ¼ã‚¶","åˆ©ç”¨è€…","ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ","ã‚µãƒ¼ãƒ","ç”»é¢","UI","GUI",
    "ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹","ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹","DB","ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯","é€šä¿¡","è¦æ±‚","å¿œç­”","ãƒªã‚¯ã‚¨ã‚¹ãƒˆ","ãƒ¬ã‚¹ãƒãƒ³ã‚¹","ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿",
    "å¼•æ•°","å±æ€§","ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£","ãƒ•ãƒ©ã‚°","ID","ãƒ•ã‚¡ã‚¤ãƒ«","ãƒ‡ãƒ¼ã‚¿æ§‹é€ ","ãƒ†ãƒ¼ãƒ–ãƒ«","ãƒ¬ã‚³ãƒ¼ãƒ‰",
    "è»¸","ã‚·ãƒ£ãƒ•ãƒˆ","ã‚®ã‚¢","ãƒ¢ãƒ¼ã‚¿","ã‚¨ãƒ³ã‚¸ãƒ³","ã‚¢ã‚¯ãƒãƒ¥ã‚¨ãƒ¼ã‚¿","ã‚»ãƒ³ã‚µ","ãƒãƒ«ãƒ–","ãƒãƒ³ãƒ—","ç­ä½“","ãƒã‚¦ã‚¸ãƒ³ã‚°","ãƒ•ãƒ¬ãƒ¼ãƒ ",
    "ã‚·ãƒ£ãƒ¼ã‚·","é§†å‹•","ä¼é”","æ”¯æŒ","é€£çµ",
    "è§£æ±º", "æº–å‚™", "æä¾›", "ç™ºç”Ÿ", "ä»¥ä¸Š", "ååˆ†"
]

@st.cache_data
def expand_stopwords_to_full_width(words):
    expanded = set(words)
    hankaku_chars = string.ascii_letters + string.digits
    zenkaku_chars = "ï½ï½‚ï½ƒï½„ï½…ï½†ï½‡ï½ˆï½‰ï½Šï½‹ï½Œï½ï½ï½ï½ï½‘ï½’ï½“ï½”ï½•ï½–ï½—ï½˜ï½™ï½šï¼¡ï¼¢ï¼£ï¼¤ï¼¥ï¼¦ï¼§ï¼¨ï¼©ï¼ªï¼«ï¼¬ï¼­ï¼®ï¼¯ï¼°ï¼±ï¼²ï¼³ï¼´ï¼µï¼¶ï¼·ï¼¸ï¼¹ï¼ºï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™"
    trans_table = str.maketrans(hankaku_chars, zenkaku_chars)
    for word in words:
        if any(c in hankaku_chars for c in word):
            expanded.add(word.translate(trans_table))
    return sorted(list(expanded))

stopwords = expand_stopwords_to_full_width(stopwords)
stopwords = sorted(list(set(stopwords)))

# n-gramå®šå‹å¥ãƒªã‚¹ãƒˆ (æ­£è¦è¡¨ç¾ãƒ•ã‚£ãƒ«ã‚¿)
_ngram_rows = [
    ("å‚ç…§ç¬¦å·ä»˜ãè¦ç´ ", r"[ä¸€-é¾¥ã-ã‚“ã‚¡-ãƒ³A-Za-z0-9ï¼\-ï¼‹ãƒ»]+?(?:éƒ¨|å±¤|é¢|ä½“|æ¿|å­”|æº|ç‰‡|éƒ¨æ|è¦ç´ |æ©Ÿæ§‹|è£…ç½®|æ‰‹æ®µ|é›»æ¥µ|ç«¯å­|é ˜åŸŸ|åŸºæ¿|å›è·¯|ææ–™|å·¥ç¨‹)\s*[ï¼ˆ(]\s*[0-9ï¼-ï¼™A-Za-z]+[A-Za-z]?\s*[ï¼‰)]", "regex", 1),
    ("å‚ç…§ç¬¦å·ä»˜ãè¦ç´ ", r"(?:ä¸Šè¨˜|å‰è¨˜)?[ä¸€-é¾¥ã-ã‚“ã‚¡-ãƒ³A-Za-z0-9ï¼\-ï¼‹ãƒ»]+?(?:éƒ¨|å±¤|é¢|ä½“|æ¿|å­”|æº|ç‰‡|éƒ¨æ|è¦ç´ |æ©Ÿæ§‹|è£…ç½®|æ‰‹æ®µ|é›»æ¥µ|ç«¯å­|é ˜åŸŸ|åŸºæ¿|å›è·¯|ææ–™|å·¥ç¨‹)\s*[0-9ï¼-ï¼™A-Za-z]+[A-Za-z]?", "regex", 1),
    ("å‚ç…§ç¬¦å·ä»˜ãè¦ç´ ", r"[A-Z]+[0-9]+", "regex", 1),
    ("è¦‹å‡ºã—ãƒ»ç« å¥","ä¸€å®Ÿæ–½å½¢æ…‹ã«ãŠã„ã¦","literal",1), ("è¦‹å‡ºã—ãƒ»ç« å¥","ä»–ã®å®Ÿæ–½å½¢æ…‹ã«ãŠã„ã¦","literal",1), ("è¦‹å‡ºã—ãƒ»ç« å¥","åˆ¥ã®å®Ÿæ–½å½¢æ…‹ã«ãŠã„ã¦","literal",1),
    ("è¦‹å‡ºã—ãƒ»ç« å¥","æœ¬æ˜ç´°æ›¸ã«ãŠã„ã¦","literal",1), ("è¦‹å‡ºã—ãƒ»ç« å¥","æœ¬æ˜ç´°æ›¸ã§ã¯","literal",1), ("è¦‹å‡ºã—ãƒ»ç« å¥","æœ¬ç™ºæ˜ã®ä¸€å´é¢","literal",1),
    ("è¦‹å‡ºã—ãƒ»ç« å¥","ä¸€å®Ÿæ–½ä¾‹ã«ãŠã„ã¦","literal",1), ("è¦‹å‡ºã—ãƒ»ç« å¥","ä»–ã®å®Ÿæ–½ä¾‹ã«ãŠã„ã¦","literal",1), ("è¦‹å‡ºã—ãƒ»ç« å¥","å¥½ã¾ã—ã„æ…‹æ§˜ã¨ã—ã¦","literal",2),
    ("è¦‹å‡ºã—ãƒ»ç« å¥","å¥½é©ã«ã¯","literal",2), ("è¦‹å‡ºã—ãƒ»ç« å¥","ç”¨èªã®å®šç¾©","literal",2), ("è¦‹å‡ºã—ãƒ»ç« å¥","å›³ç¤ºã—ãªã„","literal",2),
    ("å›³è¡¨å‚ç…§", r"å›³[ ã€€]*[ï¼-ï¼™0-9]+ã«ç¤ºã™", "regex", 1), ("å›³è¡¨å‚ç…§", r"è¡¨[ ã€€]*[ï¼-ï¼™0-9]+ã«ç¤ºã™", "regex", 1),
    ("å›³è¡¨å‚ç…§", r"å¼[ ã€€]*[ï¼-ï¼™0-9]+ã«ç¤ºã™", "regex", 1), ("å›³è¡¨å‚ç…§", r"è«‹æ±‚é …[ ã€€]*[ï¼-ï¼™0-9]+", "regex", 1),
    ("å›³è¡¨å‚ç…§", r"(?:ã€|\[)\s*[ï¼-ï¼™0-9]{4,5}\s*(?:ã€‘|\])", "regex", 1), ("å›³è¡¨å‚ç…§", r"[ï¼ˆ(][ï¼-ï¼™0-9]+[ï¼‰)]", "regex", 2),
    ("å›³è¡¨å‚ç…§", r"ç¬¬\s*[ï¼-ï¼™0-9]+ã®?å®Ÿæ–½å½¢æ…‹", "regex", 2), ("å›³è¡¨å‚ç…§", r"æ®µè½\s*[ï¼-ï¼™0-9]+", "regex", 2),
    ("å›³è¡¨å‚ç…§", r"å›³[ ã€€]*[ï¼-ï¼™0-9]+[A-Za-z]?", "regex", 2), ("å®šç¾©å°å…¥", r"ä»¥ä¸‹ã€[^ã€ã€‚]+ã‚’[^ã€ã€‚]+ã¨ç§°ã™ã‚‹", "regex", 1),
    ("å®šç¾©å°å…¥", r"ä»¥ä¸‹ã€[^ã€ã€‚]+ã‚’[^ã€ã€‚]+ã¨ã„ã†", "regex", 1), ("æ©Ÿèƒ½å¥","ã—ã¦ã‚‚ã‚ˆã„","literal",1), ("æ©Ÿèƒ½å¥","ã§ã‚ã£ã¦ã‚‚ã‚ˆã„","literal",1),
    ("æ©Ÿèƒ½å¥","ã™ã‚‹ã“ã¨ãŒã§ãã‚‹","literal",1), ("æ©Ÿèƒ½å¥","è¡Œã†ã“ã¨ãŒã§ãã‚‹","literal",1), ("æ©Ÿèƒ½å¥","ã«é™å®šã•ã‚Œãªã„","literal",1),
    ("æ©Ÿèƒ½å¥","ã«é™ã‚‰ã‚Œãªã„","literal",1), ("æ©Ÿèƒ½å¥","ä¸€ä¾‹ã¨ã—ã¦","literal",2), ("æ©Ÿèƒ½å¥","ä¾‹ç¤ºçš„ã«ã¯","literal",2),
    ("å‚ç…§å¥","å‰è¿°ã®ã¨ãŠã‚Š","literal",2), ("å‚ç…§å¥","å‰è¿°ã®é€šã‚Š","literal",2), ("å‚ç…§å¥","å¾Œè¿°ã™ã‚‹ã‚ˆã†ã«","literal",2),
    ("å‚ç…§å¥","å¾Œè¿°ã®ã¨ãŠã‚Š","literal",2), ("ç¯„å›²è¡¨ç¾", r"å°‘ãªãã¨ã‚‚(?:ä¸€|ï¼‘)ã¤", "regex", 2), ("ç¯„å›²è¡¨ç¾", "å°‘ãªãã¨ã‚‚ä¸€éƒ¨", "literal", 2),
    ("ç¯„å›²è¡¨ç¾", r"è¤‡æ•°ã®(?:å®Ÿæ–½å½¢æ…‹|æ§‹æˆ|è¦ç´ )", "regex", 3), ("èª²é¡Œå¥", r"(?:ä¸Šè¨˜|å‰è¨˜)ã®?èª²é¡Œ", "regex", 1),
    ("æ¥ç¶šãƒ»è«–ç†","ä¸€æ–¹ã§","literal",3), ("æ¥ç¶šãƒ»è«–ç†","ä»–æ–¹ã§","literal",3), ("æ¥ç¶šãƒ»è«–ç†","ã™ãªã‚ã¡","literal",3),
    ("æ¥ç¶šãƒ»è«–ç†","ã—ãŸãŒã£ã¦","literal",3), ("æ¥ç¶šãƒ»è«–ç†","ã—ã‹ã—ãªãŒã‚‰","literal",3), ("æ¥ç¶šãƒ»è«–ç†","ä¾‹ãˆã°","literal",3),
    ("æ¥ç¶šãƒ»è«–ç†","å…·ä½“çš„ã«ã¯","literal",3), ("è£œåŠ©å¥","ä»¥ä¸‹ã«èª¬æ˜ã™ã‚‹","literal",3), ("è£œåŠ©å¥","å‰è¨˜ã®ã¨ãŠã‚Š","literal",3),
    ("è£œåŠ©å¥","ã“ã‚Œã«ã‚ˆã‚Š","literal",3), ("è£œåŠ©å¥","ã“ã®ã‚ˆã†ã«","literal",3)
]

_ngram_rows = sorted(_ngram_rows, key=lambda x: (x[3], -len(x[1]) if x[2]=="literal" else -50))
_ngram_compiled = []
for cat, pat, ptype, pri in _ngram_rows:
    if ptype == "regex":
        _ngram_compiled.append((cat, re.compile(pat), ptype, pri))
    else:
        _ngram_compiled.append((cat, pat, ptype, pri))

# ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†é–¢æ•°
def normalize_text(text):
    if not isinstance(text, str): text = "" if pd.isna(text) else str(text)
    text = unicodedata.normalize("NFKC", text)
    text = text.replace("Âµ", "Î¼")
    text = re.sub(r"\s+", " ", text)
    return text

def apply_ngram_filters(text):
    for cat, pat, ptype, pri in _ngram_compiled:
        if ptype == "literal":
            if pat in text: text = text.replace(pat, "")
        else:
            text = pat.sub("", text)
    return text

@st.cache_data
def extract_compound_nouns(text):
    text = normalize_text(text)
    text = apply_ngram_filters(text)
    text = re.sub(r'ã€.*?ã€‘', '', text)

    tokens = t.tokenize(text)
    words, compound_word = [], ''
    for token in tokens:
        pos = token.part_of_speech.split(',')[0]
        if pos == 'åè©':
            compound_word += token.surface
        else:
            if (len(compound_word) > 1 and
                compound_word not in stopwords and
                not re.fullmatch(r'[\dï¼-ï¼™]+', compound_word) and
                not re.fullmatch(r'(å›³|è¡¨|å¼)[\dï¼-ï¼™]+', compound_word) and
                not re.match(r'^(ä¸Šè¨˜|å‰è¨˜|æœ¬é–‹ç¤º|å½“è©²|è©²)', compound_word) and
                not re.search(r'[0-9ï¼-ï¼™]+[)ï¼‰]?$', compound_word) and
                not re.match(r'[0-9ï¼-ï¼™]+[a-zA-Zï½-ï½šï¼¡-ï¼º]', compound_word)):
                words.append(compound_word)
            compound_word = ''

    if (len(compound_word) > 1 and
        compound_word not in stopwords and
        not re.fullmatch(r'[\dï¼-ï¼™]+', compound_word) and
        not re.fullmatch(r'(å›³|è¡¨|å¼)[\dï¼-ï¼™]+', compound_word) and
        not re.match(r'^(ä¸Šè¨˜|å‰è¨˜|æœ¬é–‹ç¤º|å½“è©²|è©²)', compound_word) and
        not re.search(r'[0-9ï¼-ï¼™]+[)ï¼‰]?$', compound_word) and
        not re.match(r'[0-9ï¼-ï¼™]+[a-zA-Zï½-ï½šï¼¡-ï¼º]', compound_word)):
        words.append(compound_word)

    return words

# ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆãƒ»è¡¨ç¤º
def generate_wordcloud_and_list(words, title, top_n=20, font_path=None):
    if not words:
        st.subheader(title)
        st.warning("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚ã€è¡¨ç¤ºã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")
        return None

    word_freq = Counter(words)

    try:
        wc = WordCloud(
            width=800, height=400, background_color='white',
            font_path=font_path, collocations=False,
            max_words=100
        ).generate_from_frequencies(word_freq)

        fig, ax = plt.subplots(figsize=(12, 6))
        ax.imshow(wc, interpolation='bilinear')
        ax.set_title(title, fontsize=20)
        ax.axis('off')
        st.pyplot(fig)
    except Exception as e:
        st.error(f"ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã®æç”»ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        st.error("Colabç’°å¢ƒä»¥å¤–ï¼ˆStreamlit Cloudãªã©ï¼‰ã§å®Ÿè¡Œã™ã‚‹å ´åˆã€ãƒ•ã‚©ãƒ³ãƒˆãƒ‘ã‚¹ï¼ˆfont_pathï¼‰ã®æŒ‡å®šãŒå¿…è¦ã§ã™ã€‚")

    st.subheader(f"--- {title} (ä¸Šä½{top_n}ä»¶) ---")
    if not word_freq:
        st.write("ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãªã—ï¼‰")
    else:
        list_data = { "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": [], "å‡ºç¾é »åº¦": [] }
        for word, freq in word_freq.most_common(top_n):
            list_data["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"].append(word)
            list_data["å‡ºç¾é »åº¦"].append(freq)
        st.dataframe(pd.DataFrame(list_data))

    return word_freq

# ç‰¹å¾´èªæŠ½å‡º
@st.cache_data
def get_characteristic_words(target_counter, other_counter1, other_counter2):
    char_words = {}
    total_target = sum(target_counter.values()) + 1
    total_other1 = sum(other_counter1.values()) + 1
    total_other2 = sum(other_counter2.values()) + 1

    for word, freq in target_counter.items():
        tf_target = freq / total_target
        tf_other1 = other_counter1.get(word, 0) / total_other1
        tf_other2 = other_counter2.get(word, 0) / total_other2
        score = tf_target / (tf_other1 + tf_other2 + 1e-9)
        if score > 1: char_words[word] = freq
    return list(Counter(char_words).elements())

# ==================================================================
# --- 3. Streamlit UI ---
# ==================================================================
st.set_page_config(
    page_title="APOLLO | Explorer",
    page_icon="ğŸ§­",
    layout="wide"
)

st.title("ğŸ§­ Explorer")
st.markdown("ç‰¹è¨±ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆè¤‡åˆåè©ï¼‰ã‚’æŠ½å‡ºã—ã€å…¨ä½“ãƒ»ç«¶åˆæ¯”è¼ƒãƒ»æ™‚ç³»åˆ—ã§ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚")

# ==================================================================
# --- 4. ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®ç¢ºèªã¨åˆæœŸåŒ– ---
# ==================================================================
if not st.session_state.get("preprocess_done", False):
    st.error("åˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    st.warning("å…ˆã«ã€ŒMission Controlã€ï¼ˆãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ï¼‰ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€ã€Œåˆ†æã‚¨ãƒ³ã‚¸ãƒ³èµ·å‹•ã€ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    st.stop()
else:
    df_main = st.session_state.df_main
    col_map = st.session_state.col_map
    delimiters = st.session_state.delimiters

# ==================================================================
# --- 5. Explorer ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ ---
# ==================================================================

# --- ãƒ•ã‚©ãƒ³ãƒˆãƒ‘ã‚¹ã®è¨­å®š (Colab / Streamlit Cloud) ---
font_path = '/usr/share/fonts/opentype/ipafont-gothic/ipagp.ttf'
if not os.path.exists(font_path):
    system_fonts = matplotlib.font_manager.findSystemFonts(fontpaths=None, fontext='ttf')
    jp_fonts = [f for f in system_fonts if 'ipagp' in f or 'notosanscjkjp' in f.lower() or 'hiragino' in f.lower()]
    if jp_fonts:
        font_path = jp_fonts[0]
        st.info(f"æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã¨ã—ã¦ {font_path} ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    else:
        st.warning("æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ãŒæ–‡å­—åŒ–ã‘ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        font_path = None


# --- UIè¨­å®š ---
st.subheader("åˆ†æãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š")

with st.container(border=True):
    st.markdown("##### ä¼æ¥­æ¯”è¼ƒåˆ†æã®è¨­å®š")

    applicant_list = ["(æŒ‡å®šãªã—)"]
    if col_map['applicant'] and col_map['applicant'] in df_main.columns:
        try:
            applicants = df_main[col_map['applicant']].fillna('').str.split(delimiters['applicant']).explode().str.strip()
            applicants = applicants[applicants != '']
            applicant_list = ["(æŒ‡å®šãªã—)"] + sorted(applicants.unique())
        except Exception as e:
            st.warning(f"å‡ºé¡˜äººãƒªã‚¹ãƒˆã®ç”Ÿæˆã«å¤±æ•—: {e}")

    col1, col2, col3 = st.columns(3)
    with col1:
        my_company = st.selectbox("è‡ªç¤¾å (MY_COMPANY):", applicant_list, key="exp_my_company")
    with col2:
        company_a = st.selectbox("ç«¶åˆA (COMPANY_A):", applicant_list, key="exp_comp_a")
    with col3:
        company_b = st.selectbox("ç«¶åˆB (COMPANY_B):", applicant_list, key="exp_comp_b")

    st.markdown("##### æ™‚ç³»åˆ—åˆ†æã®è¨­å®š")
    col1, col2, col3 = st.columns(3)
    with col1:
        enable_time_series = st.checkbox("æ™‚ç³»åˆ—åˆ†æã‚’æœ‰åŠ¹ã«ã™ã‚‹", value=True, key="exp_enable_time")
    with col2:
        date_column = col_map.get('date', None)
        st.text_input("æ—¥ä»˜ã‚«ãƒ©ãƒ  (è‡ªå‹•é¸æŠ):", value=date_column, disabled=True)
    with col3:
        time_slice_years = st.number_input("ä½•å¹´ã”ã¨ã«åŒºåˆ‡ã‚‹ã‹:", min_value=1, value=5, key="exp_time_slice")

    st.markdown("##### å‡ºåŠ›è¨­å®š")
    top_n_keywords = st.number_input("å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã§ä¸Šä½ä½•ä»¶ã¾ã§è¡¨ç¤ºã™ã‚‹ã‹:", min_value=5, value=20, key="exp_top_n")


# --- åˆ†æå®Ÿè¡Œ ---
st.markdown("---")
if st.button("Explorer ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æã‚’å®Ÿè¡Œ", type="primary", key="exp_run_analysis"):

    if not (col_map['title'] and col_map['abstract'] and col_map['applicant']):
        st.error("ã‚¨ãƒ©ãƒ¼: Mission Controlã§ã€Œç™ºæ˜ã®åç§°ã€ã€Œè¦ç´„ã€ã€Œå‡ºé¡˜äººã€ã®ã‚«ãƒ©ãƒ ã‚’æ­£ã—ãç´ä»˜ã‘ã¦ãã ã•ã„ã€‚")
        st.stop()
    if enable_time_series and not date_column:
        st.error("ã‚¨ãƒ©ãƒ¼: æ™‚ç³»åˆ—åˆ†æã‚’æœ‰åŠ¹ã«ã™ã‚‹å ´åˆã€Mission Controlã§ã€Œå‡ºé¡˜æ—¥ã€ã‚«ãƒ©ãƒ ã‚’ç´ä»˜ã‘ã¦ãã ã•ã„ã€‚")
        st.stop()

    try:
        df_main['text'] = df_main[col_map['title']].fillna('') + ' ' + df_main[col_map['abstract']].fillna('')
        df_main['æ¨©åˆ©è€…'] = df_main[col_map['applicant']].astype(str).str.split(delimiters['applicant'])
        df_exploded = df_main.explode('æ¨©åˆ©è€…')
        df_exploded['æ¨©åˆ©è€…'] = df_exploded['æ¨©åˆ©è€…'].str.strip()

        st.success("ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

        # --- åˆ†æ1: å…¨ä½“ã®ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ ---
        with st.container(border=True):
            st.header("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã®æŠ€è¡“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")
            with st.spinner("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºä¸­..."):
                all_words = []
                for text in df_main['text']:
                    all_words.extend(extract_compound_nouns(text))
                generate_wordcloud_and_list(all_words, "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã®æŠ€è¡“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", top_n_keywords, font_path)

        # --- åˆ†æ2: ä¼æ¥­æ¯”è¼ƒåˆ†æ ---
        target_companies = [c for c in [my_company, company_a, company_b] if c != "(æŒ‡å®šãªã—)"]

        if target_companies:
            with st.container(border=True):
                st.header(f"ä¼æ¥­æ¯”è¼ƒ: {', '.join(target_companies)}")
                company_words = {}

                with st.spinner("å„ä¼æ¥­ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºä¸­..."):
                    for company in target_companies:
                        company_df = df_exploded[df_exploded['æ¨©åˆ©è€…'] == company]
                        if company_df.empty:
                            st.warning(f"è­¦å‘Š: ä¼æ¥­ '{company}' ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                            company_words[company] = []
                            continue
                        words = []
                        for text in company_df['text']:
                            words.extend(extract_compound_nouns(text))
                        company_words[company] = words

                for company, words in company_words.items():
                    generate_wordcloud_and_list(words, f"'{company}'ã®æŠ€è¡“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", top_n_keywords, font_path)

                my_counter = Counter(company_words.get(my_company, []))
                a_counter = Counter(company_words.get(company_a, []))
                b_counter = Counter(company_words.get(company_b, []))

                if my_company != "(æŒ‡å®šãªã—)" and company_a != "(æŒ‡å®šãªã—)":
                    common_vs_a = list((my_counter & a_counter).elements())
                    generate_wordcloud_and_list(common_vs_a, f"'{my_company}' ã¨ '{company_a}' ã®å…±é€šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", top_n_keywords, font_path)

                if my_company != "(æŒ‡å®šãªã—)" and company_b != "(æŒ‡å®šãªã—)":
                    common_vs_b = list((my_counter & b_counter).elements())
                    generate_wordcloud_and_list(common_vs_b, f"'{my_company}' ã¨ '{company_b}' ã®å…±é€šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", top_n_keywords, font_path)

                st.markdown("---")
                st.subheader("ç‰¹å¾´/ç‹¬è‡ªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")

                my_char_words = get_characteristic_words(my_counter, a_counter, b_counter)
                generate_wordcloud_and_list(my_char_words, f"'{my_company}' ã®ç‰¹å¾´çš„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", top_n_keywords, font_path)

                a_char_words = get_characteristic_words(a_counter, my_counter, b_counter)
                generate_wordcloud_and_list(a_char_words, f"'{company_a}' ã®ç‰¹å¾´çš„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", top_n_keywords, font_path)

                b_char_words = get_characteristic_words(b_counter, my_counter, a_counter)
                generate_wordcloud_and_list(b_char_words, f"'{company_b}' ã®ç‰¹å¾´çš„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", top_n_keywords, font_path)

                my_set = set(word for word, freq in my_counter.most_common(500))
                a_set = set(word for word, freq in a_counter.most_common(500))
                b_set = set(word for word, freq in b_counter.most_common(500))

                my_unique = {k: my_counter[k] for k in my_set - a_set - b_set}
                generate_wordcloud_and_list(list(Counter(my_unique).elements()), f"'{my_company}' ã®ç‹¬è‡ªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", top_n_keywords, font_path)

                a_unique = {k: a_counter[k] for k in a_set - my_set - b_set}
                generate_wordcloud_and_list(list(Counter(a_unique).elements()), f"'{company_a}' ã®ç‹¬è‡ªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", top_n_keywords, font_path)

                b_unique = {k: b_counter[k] for k in b_set - my_set - a_set}
                generate_wordcloud_and_list(list(Counter(b_unique).elements()), f"'{company_b}' ã®ç‹¬è‡ªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", top_n_keywords, font_path)

        # --- åˆ†æ3: æ™‚ç³»åˆ—åˆ†æ ---
        if enable_time_series:
            with st.container(border=True):
                st.header(f"æŠ€è¡“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æ™‚ç³»åˆ—åˆ†æ ({time_slice_years}å¹´ã”ã¨)")
                try:
                    df_time = df_main.copy()
                    # 'year' ã‚«ãƒ©ãƒ ã¯ app.py ã§ 'parsed_date' ã‹ã‚‰æ—¢ã«ä½œæˆæ¸ˆã¿
                    df_time.dropna(subset=['year'], inplace=True)

                    min_year = int(df_time['year'].min())
                    max_year = int(df_time['year'].max())

                    with st.spinner("æ™‚ç³»åˆ—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºä¸­..."):
                        for start_year in range(min_year, max_year + 1, time_slice_years):
                            end_year = start_year + time_slice_years - 1
                            period_df = df_time[(df_time['year'] >= start_year) & (df_time['year'] <= end_year)]

                            if period_df.empty: continue

                            period_words = []
                            for text in period_df['text']:
                                period_words.extend(extract_compound_nouns(text))

                            title = f"æŠ€è¡“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å¤‰é· ({start_year} - {end_year})"
                            generate_wordcloud_and_list(period_words, title, top_n_keywords, font_path)

                except Exception as e:
                    st.error(f"æ™‚ç³»åˆ—åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                    st.warning(f"ãƒ’ãƒ³ãƒˆ: '{date_column}'åˆ—ã®æ—¥ä»˜å½¢å¼ãŒèªè­˜ã§ããªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚'YYYY-MM-DD'å½¢å¼ãªã©ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

        st.success("å…¨ã¦ã®åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

    except Exception as e:
        st.error(f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        st.exception(traceback.format_exc())

# --- å…±é€šã‚µã‚¤ãƒ‰ãƒãƒ¼ãƒ•ãƒƒã‚¿ãƒ¼ ---
st.sidebar.markdown("---") 
st.sidebar.caption("ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³:")
st.sidebar.caption("1. Mission Control ã§ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€å‰å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
st.sidebar.caption("2. å·¦ã®ãƒªã‚¹ãƒˆã‹ã‚‰åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é¸æŠã—ã¾ã™ã€‚")
st.sidebar.markdown("---")
st.sidebar.caption("Â© 2025 ã—ã°ã‚„ã¾")
