import streamlit as st
import pandas as pd
import numpy as np
import warnings
import re
import string
import os
import platform
import unicodedata
from collections import Counter
from itertools import combinations
import datetime

import plotly.graph_objects as go
import plotly.express as px
from wordcloud import WordCloud
from janome.tokenizer import Tokenizer
import networkx as nx
from networkx.algorithms import community
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import japanize_matplotlib
import utils

# ==================================================================
# --- 1. è¨­å®šãƒ»ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ---
# ==================================================================
warnings.filterwarnings('ignore')

st.set_page_config(
    page_title="APOLLO | Explorer",
    page_icon="ğŸ§­",
    layout="wide"
)



FONT_PATH = utils.get_japanese_font_path()
if FONT_PATH:
    try:
        prop = fm.FontProperties(fname=FONT_PATH)
        plt.rcParams['font.family'] = prop.get_name()
    except:
        pass





def update_fig_layout(fig, title, height=600, theme_config=None, show_legend=True):
    if theme_config:
        # Sanitize title
        if isinstance(title, str):
            title = re.sub(r'<[^>]+>', '', title)

        layout_params = dict(
            template=theme_config["plotly_template"],
            title=dict(text=title, font=dict(size=18, color=theme_config["text_color"], weight="normal")),
            paper_bgcolor=theme_config["bg_color"], plot_bgcolor=theme_config["bg_color"],
            font=dict(color=theme_config["text_color"], family="Helvetica Neue"),
            height=height, margin=dict(l=20, r=20, t=60, b=20)
        )
        if not show_legend:
            layout_params['showlegend'] = False
            
        fig.update_layout(**layout_params)
    return fig

# ==================================================================
# --- 2. ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç† ---
# ==================================================================
@st.cache_resource
def load_tokenizer_explorer(): return Tokenizer()
t = load_tokenizer_explorer()

if "stopwords" in st.session_state and st.session_state["stopwords"]:
    stopwords = st.session_state["stopwords"]
else:
    stopwords = utils.get_stopwords()

_ngram_rows = [
    ("å‚ç…§ç¬¦å·ä»˜ãè¦ç´ ", r"[ä¸€-é¾¥ã-ã‚“ã‚¡-ãƒ³A-Za-z0-9ï¼\-ï¼‹ãƒ»]+?(?:éƒ¨|å±¤|é¢|ä½“|æ¿|å­”|æº|ç‰‡|éƒ¨æ|è¦ç´ |æ©Ÿæ§‹|è£…ç½®|æ‰‹æ®µ|é›»æ¥µ|ç«¯å­|é ˜åŸŸ|åŸºæ¿|å›è·¯|ææ–™|å·¥ç¨‹)\s*[ï¼ˆ(]\s*[0-9ï¼-ï¼™A-Za-z]+[A-Za-z]?\s*[ï¼‰)]", "regex", 1),
    ("å‚ç…§ç¬¦å·ä»˜ãè¦ç´ ", r"(?:ä¸Šè¨˜|å‰è¨˜)?[ä¸€-é¾¥ã-ã‚“ã‚¡-ãƒ³A-Za-z0-9ï¼\-ï¼‹ãƒ»]+?(?:éƒ¨|å±¤|é¢|ä½“|æ¿|å­”|æº|ç‰‡|éƒ¨æ|è¦ç´ |æ©Ÿæ§‹|è£…ç½®|æ‰‹æ®µ|é›»æ¥µ|ç«¯å­|é ˜åŸŸ|åŸºæ¿|å›è·¯|ææ–™|å·¥ç¨‹)\s*[0-9ï¼-ï¼™A-Za-z]+[A-Za-z]?", "regex", 1),
    ("å‚ç…§ç¬¦å·ä»˜ãè¦ç´ ", r"[A-Z]+[0-9]+", "regex", 1),
    ("è¦‹å‡ºã—ãƒ»ç« å¥","ä¸€å®Ÿæ–½å½¢æ…‹ã«ãŠã„ã¦","literal",1), ("è¦‹å‡ºã—ãƒ»ç« å¥","ä»–ã®å®Ÿæ–½å½¢æ…‹ã«ãŠã„ã¦","literal",1), ("è¦‹å‡ºã—ãƒ»ç« å¥","åˆ¥ã®å®Ÿæ–½å½¢æ…‹ã«ãŠã„ã¦","literal",1),
    ("è¦‹å‡ºã—ãƒ»ç« å¥","æœ¬æ˜ç´°æ›¸ã«ãŠã„ã¦","literal",1), ("è¦‹å‡ºã—ãƒ»ç« å¥","æœ¬æ˜ç´°æ›¸ã§ã¯","literal",1), ("è¦‹å‡ºã—ãƒ»ç« å¥","æœ¬ç™ºæ˜ã®ä¸€å´é¢","literal",1),
    ("è¦‹å‡ºã—ãƒ»ç« å¥","ä¸€å®Ÿæ–½ä¾‹ã«ãŠã„ã¦","literal",1), ("è¦‹å‡ºã—ãƒ»ç« å¥","ä»–ã®å®Ÿæ–½ä¾‹ã«ãŠã„ã¦","literal",1), ("è¦‹å‡ºã—ãƒ»ç« å¥","å¥½ã¾ã—ã„æ…‹æ§˜ã¨ã—ã¦","literal",2),
    ("è¦‹å‡ºã—ãƒ»ç« å¥","å¥½é©ã«ã¯","literal",2), ("è¦‹å‡ºã—ãƒ»ç« å¥","ç”¨èªã®å®šç¾©","literal",2), ("è¦‹å‡ºã—ãƒ»ç« å¥","å›³ç¤ºã—ãªã„","literal",2),
    ("å›³è¡¨å‚ç…§", r"å›³[ ã€€]*[ï¼-ï¼™0-9]+ã«ç¤ºã™", "regex", 1), ("å›³è¡¨å‚ç…§", r"è¡¨[ ã€€]*[ï¼-ï¼™0-9]+ã«ç¤ºã™", "regex", 1),
    ("å›³è¡¨å‚ç…§", r"å¼[ ã€€]*[ï¼-ï¼™0-9]+ã«ç¤ºã™", "regex", 1), ("å›³è¡¨å‚ç…§", r"è«‹æ±‚é …[ ã€€]*[ï¼-ï¼™0-9]+", "regex", 1),
    ("å›³è¡¨å‚ç…§", r"(?:ã€|\[)\s*[ï¼-ï¼™0-9]{4,5}\s*(?:ã€‘|\])", "regex", 1), ("å›³è¡¨å‚ç…§", r"[ï¼ˆ(][ï¼-ï¼™0-9]+[ï¼‰)]", "regex", 2),
    ("å›³è¡¨å‚ç…§", r"ç¬¬\s*[ï¼-ï¼™0-9]+ã®?å®Ÿæ–½å½¢æ…‹", "regex", 2), ("å›³è¡¨å‚ç…§", r"æ®µè½\s*[ï¼-ï¼™0-9]+", "regex", 2),
    ("å›³è¡¨å‚ç…§", r"å›³[ ã€€]*[ï¼-ï¼™0-9]+[A-Za-z]?", "regex", 2), ("å®šç¾©å°å…¥", r"ä»¥ä¸‹ã€[^ã€ã€‚]+ã‚’[^ã€ã€‚]+ã¨ç§°ã™ã‚‹", "regex", 1),
    ("å®šç¾©å°å…¥", r"ä»¥ä¸‹ã€[^ã€ã€‚]+ã‚’[^ã€ã€‚]+ã¨ã„ã†", "regex", 1), ("æ©Ÿèƒ½å¥","ã—ã¦ã‚‚ã‚ˆã„","literal",1), ("æ©Ÿèƒ½å¥","ã§ã‚ã£ã¦ã‚‚ã‚ˆã„","literal",1),
    ("æ©Ÿèƒ½å¥","ã™ã‚‹ã“ã¨ãŒã§ãã‚‹","literal",1), ("æ©Ÿèƒ½å¥","è¡Œã†ã“ã¨ãŒã§ãã‚‹","literal",1), ("æ©Ÿèƒ½å¥","ã«é™å®šã•ã‚Œãªã„","literal",1),
    ("æ©Ÿèƒ½å¥","ã«é™ã‚‰ã‚Œãªã„","literal",1), ("æ©Ÿèƒ½å¥","ä¸€ä¾‹ã¨ã—ã¦","literal",2), ("æ©Ÿèƒ½å¥","ä¾‹ç¤ºçš„ã«ã¯","literal",2),
    ("å‚ç…§å¥","å‰è¿°ã®ã¨ãŠã‚Š","literal",2), ("å‚ç…§å¥","å‰è¿°ã®é€šã‚Š","literal",2), ("å‚ç…§å¥","å¾Œè¿°ã™ã‚‹ã‚ˆã†ã«","literal",2),
    ("å‚ç…§å¥","å¾Œè¿°ã®ã¨ãŠã‚Š","literal",2), ("ç¯„å›²è¡¨ç¾", r"å°‘ãªãã¨ã‚‚(?:ä¸€|ï¼‘)ã¤", "regex", 2), ("ç¯„å›²è¡¨ç¾", "å°‘ãªãã¨ã‚‚ä¸€éƒ¨", "literal", 2),
    ("ç¯„å›²è¡¨ç¾", r"è¤‡æ•°ã®(?:å®Ÿæ–½å½¢æ…‹|æ§‹æˆ|è¦ç´ )", "regex", 3), ("èª²é¡Œå¥", r"(?:ä¸Šè¨˜|å‰è¨˜)ã®?èª²é¡Œ", "regex", 1),
    ("æ¥ç¶šãƒ»è«–ç†","ä¸€æ–¹ã§","literal",3), ("æ¥ç¶šãƒ»è«–ç†","ä»–æ–¹ã§","literal",3), ("æ¥ç¶šãƒ»è«–ç†","ã™ãªã‚ã¡","literal",3),
    ("æ¥ç¶šãƒ»è«–ç†","ã—ãŸãŒã£ã¦","literal",3), ("æ¥ç¶šãƒ»è«–ç†","ã—ã‹ã—ãªãŒã‚‰","literal",3), ("æ¥ç¶šãƒ»è«–ç†","ä¾‹ãˆã°","literal",3),
    ("æ¥ç¶šãƒ»è«–ç†","å…·ä½“çš„ã«ã¯","literal",3), ("è£œåŠ©å¥","ä»¥ä¸‹ã«èª¬æ˜ã™ã‚‹","literal",3), ("è£œåŠ©å¥","å‰è¨˜ã®ã¨ãŠã‚Š","literal",3),
    ("è£œåŠ©å¥","ã“ã‚Œã«ã‚ˆã‚Š","literal",3), ("è£œåŠ©å¥","ã“ã®ã‚ˆã†ã«","literal",3)
]

_ngram_compiled = sorted(_ngram_rows, key=lambda x: (x[3], -len(x[1]) if x[2]=="literal" else -50))
_ngram_compiled = [(cat, re.compile(pat) if ptype == "regex" else pat, ptype, pri) for cat, pat, ptype, pri in _ngram_compiled]

def normalize_text(text):
    if not isinstance(text, str): text = "" if pd.isna(text) else str(text)
    text = unicodedata.normalize("NFKC", text)
    text = text.replace("Âµ", "Î¼")
    text = re.sub(r"\s+", " ", text)
    return text

def apply_ngram_filters(text):
    for cat, pat, ptype, pri in _ngram_compiled:
        if ptype == "literal":
            if pat in text: text = text.replace(pat, "")
        else:
            text = pat.sub("", text)
    return text

@st.cache_data
def extract_compound_nouns(text):
    text = normalize_text(text)
    text = apply_ngram_filters(text) 
    text = re.sub(r'ã€.*?ã€‘', '', text)
    text = re.sub(r'[!\"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~]', ' ', text)

    tokens = t.tokenize(text)
    words, compound_word = [], ''
    for token in tokens:
        pos = token.part_of_speech.split(',')[0]
        if pos == 'åè©':
            compound_word += token.surface
        else:
            if (len(compound_word) > 1 and
                compound_word not in stopwords and
                not re.fullmatch(r'[\dï¼-ï¼™]+', compound_word) and
                not re.fullmatch(r'(å›³|è¡¨|å¼|ç¬¬)[\dï¼-ï¼™]+.*', compound_word) and
                not re.match(r'^(ä¸Šè¨˜|å‰è¨˜|æœ¬é–‹ç¤º|å½“è©²|è©²)', compound_word) and
                not re.search(r'[0-9ï¼-ï¼™]+[)ï¼‰]?$', compound_word) and
                not re.match(r'[0-9ï¼-ï¼™]+[a-zA-Zï½-ï½šï¼¡-ï¼º]', compound_word)):
                words.append(compound_word)
            compound_word = ''
            
    if (len(compound_word) > 1 and
        compound_word not in stopwords and
        not re.fullmatch(r'[\dï¼-ï¼™]+', compound_word) and
        not re.fullmatch(r'(å›³|è¡¨|å¼|ç¬¬)[\dï¼-ï¼™]+.*', compound_word) and
        not re.match(r'^(ä¸Šè¨˜|å‰è¨˜|æœ¬é–‹ç¤º|å½“è©²|è©²)', compound_word) and
        not re.search(r'[0-9ï¼-ï¼™]+[)ï¼‰]?$', compound_word) and
        not re.match(r'[0-9ï¼-ï¼™]+[a-zA-Zï½-ï½šï¼¡-ï¼º]', compound_word)):
        words.append(compound_word)
    return words

def generate_wordcloud_and_list(words, title, top_n=20, font_path=None):
    if not words: return None
    word_freq = Counter(words)
    try:
        wc = WordCloud(
            width=800, height=400, background_color='white',
            font_path=font_path, collocations=False,
            max_words=100
        ).generate_from_frequencies(word_freq)

        fig, ax = plt.subplots(figsize=(12, 6))
        ax.imshow(wc, interpolation='bilinear')
        ax.set_title(title, fontsize=20)
        ax.axis('off')
        st.pyplot(fig)
    except Exception as e:
        st.error(f"ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã®æç”»ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

# ==================================================================
# --- 3. UI & ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ ---
# ==================================================================

utils.render_sidebar()

st.title("ğŸ§­ Explorer")
st.markdown("""
Explorer (æˆ¦ç•¥çš„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¢ç´¢) ã¯ã€ç‰¹è¨±æ–‡æ›¸å†…ã®å°‚é–€ç”¨èªã‚’æŠ½å‡ºã—ã€å¸‚å ´å…¨ä½“ã®ãƒˆãƒ¬ãƒ³ãƒ‰å¤‰é·ã‚„ç«¶åˆä»–ç¤¾ã¨ã®æˆ¦ç•¥çš„å·®ç•°ã‚’å¤šè§’çš„ã«åˆ†æã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§ã™ã€‚
æ€¥ä¸Šæ˜‡ãƒ¯ãƒ¼ãƒ‰ã®ç‰¹å®šã€æ™‚ç³»åˆ—ã§ã®æŠ€è¡“æ¨ç§»ã€ãã—ã¦ä¼æ¥­é–“ã®ãƒã‚¸ã‚·ãƒ§ãƒ‹ãƒ³ã‚°æ¯”è¼ƒã‚’é€šã˜ã¦ã€æ¬¡ã®ä¸€æ‰‹ã¨ãªã‚‹ã‚¤ãƒ³ã‚µã‚¤ãƒˆã‚’ç™ºæ˜ã—ã¾ã™ã€‚
""")

col_theme, _ = st.columns([1, 3])
with col_theme:
    selected_theme = st.selectbox("è¡¨ç¤ºãƒ†ãƒ¼ãƒ:", ["APOLLO Standard", "Modern Presentation"], key="exp_theme")
theme_config = utils.get_theme_config(selected_theme)
st.markdown(f"<style>{theme_config['css']}</style>", unsafe_allow_html=True)

# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
if not st.session_state.get("preprocess_done", False):
    st.error("åˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚Mission Controlã§ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚"); st.stop()

df_main = st.session_state.df_main
col_map = st.session_state.col_map
delimiters = st.session_state.delimiters

# å‡ºé¡˜äººãƒªã‚¹ãƒˆç”Ÿæˆ
app_counts = pd.Series()
if col_map['applicant'] in df_main.columns:
    if 'applicant_main' in df_main.columns:
        app_series = df_main['applicant_main'].explode().dropna()
    else:
        app_series = df_main[col_map['applicant']].fillna('').str.split(delimiters['applicant']).explode().str.strip()
    
    app_counts = app_series[app_series != ''].value_counts()

sorted_applicants = app_counts.index.tolist()
app_count_dict = app_counts.to_dict()

# å‰å‡¦ç†
# å‰å‡¦ç† ("explorer_keywords"ã‚«ãƒ©ãƒ ãŒãªã„å ´åˆã®ã¿å®Ÿè¡Œ)
if 'explorer_keywords' not in df_main.columns:
    with st.spinner("Explorer: ãƒ†ã‚­ã‚¹ãƒˆè§£æã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã‚’å®Ÿè¡Œä¸­..."):
        df_main['explorer_text'] = df_main[col_map['title']].fillna('') + ' ' + df_main[col_map['abstract']].fillna('')
        df_main['explorer_keywords'] = df_main['explorer_text'].apply(extract_compound_nouns)
        st.session_state.df_main = df_main

# ãƒ¢ãƒ¼ãƒ‰é¸æŠ
selected_tab = st.radio(
    "åˆ†æãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠ:",
    ["Global Overview", "Trend Analysis", "Comparative Strategy", "Context Search (KWIC)"],
    horizontal=True
)

st.markdown("---")

# ==================================================================
# --- 4. Global Overview (å…¨ä½“ä¿¯ç°) ---
# ==================================================================
if selected_tab == "Global Overview":
    st.subheader("Global Overview")
    
    top_n_cloud = st.number_input("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰å˜èªæ•°", 10, 100, 50, key="go_cloud_n")
    all_keywords = [w for sublist in df_main['explorer_keywords'] for w in sublist]
    word_counts = Counter(all_keywords)
    
    if not word_counts:
        st.warning("æœ‰åŠ¹ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    else:
        st.markdown("##### 1. å…¨ä½“ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰")
        generate_wordcloud_and_list(all_keywords, "å…¨ä½“ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰", top_n_cloud, FONT_PATH)

        st.markdown("##### 2. å…¨ä½“å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ (æŠ€è¡“ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼)")
        col_net1, col_net2 = st.columns(2)
        with col_net1: global_net_top_n = st.slider("æŠ½å‡ºå˜èªæ•° (Top N)", 30, 100, 60, key="global_net_n")
        with col_net2: global_net_threshold = st.slider("å…±èµ·é–¾å€¤ (Jaccard)", 0.01, 0.3, 0.05, 0.01, key="global_net_th")

        with st.spinner("å…¨ä½“ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨ˆç®—ä¸­..."):
            c_all = Counter(all_keywords)
            top_nodes_global = [w for w, c in c_all.most_common(global_net_top_n)]
            pair_counts_global = Counter()
            for kws in df_main['explorer_keywords']:
                valid_w = [w for w in set(kws) if w in top_nodes_global]
                if len(valid_w) >= 2:
                    for pair in combinations(sorted(valid_w), 2): pair_counts_global[pair] += 1
            
            G_global = nx.Graph()
            for w in top_nodes_global: G_global.add_node(w, size=c_all[w])
            for (u, v), c in pair_counts_global.items():
                weight = c / (c_all[u] + c_all[v] - c)
                if weight >= global_net_threshold: G_global.add_edge(u, v, weight=weight)
            G_global.remove_nodes_from(list(nx.isolates(G_global)))
            
            if G_global.number_of_nodes() > 0:
                communities = community.greedy_modularity_communities(G_global)
                community_map = {}; pos_global = nx.spring_layout(G_global, k=0.8, seed=42)
                for i, comm in enumerate(communities):
                    for node in comm: community_map[node] = i
                
                edge_x, edge_y = [], []
                for edge in G_global.edges():
                    x0, y0 = pos_global[edge[0]]; x1, y1 = pos_global[edge[1]]
                    edge_x.extend([x0, x1, None]); edge_y.extend([y0, y1, None])
                
                edge_trace_g = go.Scatter(x=edge_x, y=edge_y, line=dict(width=0.5, color='#888'), hoverinfo='none', mode='lines')
                node_x, node_y, node_color, node_text = [], [], [], []
                for node in G_global.nodes():
                    node_x.append(pos_global[node][0]); node_y.append(pos_global[node][1])
                    node_color.append(community_map.get(node, 0))
                    node_text.append(f"{node} ({c_all[node]}ä»¶)")
                
                node_trace_g = go.Scatter(
                    x=node_x, y=node_y, mode='markers+text',
                    text=[n for n in G_global.nodes()], textposition="top center",
                    hovertext=node_text, hoverinfo="text",
                    marker=dict(showscale=False, colorscale='Turbo', color=node_color, size=[np.log(c_all[n]+1)*8 for n in G_global.nodes()], line_width=1)
                )
                fig_net_g = go.Figure(data=[edge_trace_g, node_trace_g])
                fig_net_g.update_layout(showlegend=False, xaxis=dict(visible=False), yaxis=dict(visible=False))
                update_fig_layout(fig_net_g, "Global Co-occurrence Network", height=700, theme_config=theme_config)
                st.plotly_chart(fig_net_g, use_container_width=True, config={'editable': False})
                
                # --- Snapshot (Global Network) ---
                # 1. Community Structure
                comm_summary = []
                for i in range(len(communities)):
                    comm_words = list(communities[i])[:5] 
                    comm_summary.append(f"Group {i+1}: {', '.join(comm_words)}")
                
                # 2. Hubs (Degree Centrality)
                deg_centrality = nx.degree_centrality(G_global)
                sorted_hubs = sorted(deg_centrality.items(), key=lambda x: x[1], reverse=True)[:10]
                hubs_str = ", ".join([f"{n}({val:.2f})" for n, val in sorted_hubs])

                # 3. Strongest Edges
                sorted_edges = sorted(G_global.edges(data=True), key=lambda x: x[2]['weight'], reverse=True)[:10]
                edges_str = ", ".join([f"{u}-{v}" for u, v, d in sorted_edges])

                # Combine with Rich Summary
                snap_data = utils.generate_rich_summary(df_main, title_col=col_map['title'], abstract_col=col_map['abstract'])
                snap_data['module'] = 'Explorer'
                snap_data['network_stats'] = {
                    "communities": "; ".join(comm_summary),
                    "hubs": hubs_str,
                    "edges": edges_str
                }
                
                utils.render_snapshot_button(
                    title="å…¨ä½“å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ (æŠ€è¡“ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼)",
                    description="å…¨æœŸé–“ã‚’é€šã˜ãŸæŠ€è¡“ç”¨èªã®å…±èµ·é–¢ä¿‚ã¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ§‹é€ ã€‚",
                    key="exp_global_net_snap",
                    fig=fig_net_g,
                    data_summary=snap_data
                )
            else: st.warning("æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹å…±èµ·é–¢ä¿‚ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

# ==================================================================
# --- 5. Trend Analysis (æ™‚ç³»åˆ—åˆ†æ) ---
# ==================================================================
elif selected_tab == "Trend Analysis":
    st.subheader("Trend Analysis")
    
    target_filter = st.selectbox(
        "åˆ†æå¯¾è±¡:", 
        ["å…¨ä½“ (Market)"] + sorted_applicants, 
        format_func=lambda x: f"{x} (å…¨{len(df_main)}ä»¶)" if x == "å…¨ä½“ (Market)" else f"{x} ({app_count_dict.get(x, 0)}ä»¶)",
        key="trend_target"
    )
    
    if target_filter == "å…¨ä½“ (Market)":
        df_target = df_main
    else:
        if 'applicant_main' in df_main.columns:
            mask = df_main['applicant_main'].apply(lambda x: isinstance(x, list) and target_filter in x)
        else:
            mask = df_main[col_map['applicant']].fillna('').str.contains(re.escape(target_filter))
        df_target = df_main[mask]
        
    st.info(f"åˆ†æå¯¾è±¡: {target_filter} ({len(df_target)}ä»¶)")
    
    if df_target.empty:
        st.warning("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    else:
        current_year = int(df_target['year'].max())
        min_year = int(df_target['year'].min())
        
        interval_years = st.slider("æœŸé–“ã®ç²’åº¦ (å¹´)", 1, 10, 5, key="ta_interval")
        
        periods = []
        c_end = current_year
        while c_end >= min_year:
            c_start = c_end - interval_years + 1
            real_start = max(min_year, c_start)
            periods.append((real_start, c_end))
            c_end -= interval_years
            if c_end < min_year: break
        
        # 1. æ€¥ä¸Šæ˜‡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        st.markdown(f"##### 1. æ€¥ä¸Šæ˜‡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ (Growth Rate)")
        if len(periods) > 1:
            st.caption(f"æ¯”è¼ƒæœŸé–“: [{periods[0][0]}-{periods[0][1]}] vs [{periods[1][0]}-{periods[1][1]}]")
            df_recent = df_target[(df_target['year'] >= periods[0][0]) & (df_target['year'] <= periods[0][1])]
            df_past = df_target[(df_target['year'] >= periods[1][0]) & (df_target['year'] <= periods[1][1])]
            
            c_recent = Counter([w for sublist in df_recent['explorer_keywords'] for w in sublist])
            c_past = Counter([w for sublist in df_past['explorer_keywords'] for w in sublist])
            
            growth_data = []
            min_freq = max(2, len(df_recent) * 0.01)
            for word, count_r in c_recent.items():
                if count_r < min_freq: continue
                count_p = c_past.get(word, 0)
                growth_rate = (count_r - count_p) / (count_p + 1)
                growth_data.append({"Keyword": word, "Growth Rate": growth_rate})
            
            df_growth = pd.DataFrame(growth_data).sort_values("Growth Rate", ascending=False).head(20)
            if not df_growth.empty:
                fig_growth = px.bar(df_growth, x="Growth Rate", y="Keyword", orientation='h', color="Growth Rate", color_continuous_scale="Reds")
                fig_growth.update_layout(yaxis={'categoryorder':'total ascending'})
                update_fig_layout(fig_growth, "Growth Rate Top 20", height=500, theme_config=theme_config)
                st.plotly_chart(fig_growth, use_container_width=True, config={'editable': False})
                
                utils.render_snapshot_button(
                    title="æ€¥ä¸Šæ˜‡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ (Growth Rate)",
                    description=f"ç›´è¿‘æœŸé–“ [{periods[0][0]}-{periods[0][1]}] ã§æ€¥å¢—ã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€‚",
                    key="exp_growth_snap",
                    fig=fig_growth,
                    data_summary=df_growth.to_string(index=False)
                )
        else:
            st.warning("æ¯”è¼ƒå¯¾è±¡ã¨ãªã‚‹éå»ã®ãƒ‡ãƒ¼ã‚¿æœŸé–“ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")

        # 2. æ™‚ç³»åˆ—ãƒãƒ«ãƒãƒ»ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰
        st.markdown(f"##### 2. æ™‚ç³»åˆ—ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ (Time-Lapse)")
        cols = st.columns(3)
        for i, (start, end) in enumerate(periods):
            with cols[i % 3]:
                df_p = df_target[(df_target['year'] >= start) & (df_target['year'] <= end)]
                kws_p = [w for sublist in df_p['explorer_keywords'] for w in sublist]
                st.markdown(f"**{start} - {end}** ({len(df_p)}ä»¶)")
                if kws_p: generate_wordcloud_and_list(kws_p, f"{start}-{end}", 30, FONT_PATH)
            
        # 3. ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        st.markdown(f"##### 3. ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ (èµ¤=æ€¥ä¸Šæ˜‡ / é’=åœæ»)")
        col_net1, col_net2 = st.columns(2)
        with col_net1: ta_net_n = st.slider("æŠ½å‡ºå˜èªæ•°", 30, 100, 60, key="ta_net_n")
        with col_net2: ta_net_th = st.slider("å…±èµ·é–¾å€¤", 0.01, 0.3, 0.05, 0.01, key="ta_net_th")
        
        all_target_kw = [w for sublist in df_target['explorer_keywords'] for w in sublist]
        c_all = Counter(all_target_kw)
        top_nodes = [w for w, c in c_all.most_common(ta_net_n)]
        
        pair_counts = Counter()
        for kws in df_target['explorer_keywords']:
            valid_w = [w for w in set(kws) if w in top_nodes]
            if len(valid_w) >= 2:
                for pair in combinations(sorted(valid_w), 2): pair_counts[pair] += 1
                
        G = nx.Graph()
        for w in top_nodes: G.add_node(w, size=c_all[w])
        for (u, v), c in pair_counts.items():
            weight = c / (c_all[u] + c_all[v] - c)
            if weight >= ta_net_th: G.add_edge(u, v, weight=weight)
        G.remove_nodes_from(list(nx.isolates(G)))
        
        if G.number_of_nodes() > 0:
            pos = nx.spring_layout(G, k=0.8, seed=42)
            node_colors, node_texts = [], []
            
            c_rec_net = Counter([w for sublist in df_target[df_target['year'] >= periods[0][0]]['explorer_keywords'] for w in sublist])
            if len(periods) > 1:
                c_pst_net = Counter([w for sublist in df_target[(df_target['year'] >= periods[1][0]) & (df_target['year'] <= periods[1][1])]['explorer_keywords'] for w in sublist])
            else:
                c_pst_net = Counter()

            for node in G.nodes():
                gr = (c_rec_net.get(node, 0) - c_pst_net.get(node, 0)) / (c_pst_net.get(node, 0) + 1)
                node_colors.append(gr)
                node_texts.append(f"{node}<br>Growth: {gr:.2f}")
            
            edge_x, edge_y = [], []
            for edge in G.edges():
                x0, y0 = pos[edge[0]]; x1, y1 = pos[edge[1]]
                edge_x.extend([x0, x1, None]); edge_y.extend([y0, y1, None])
            
            edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(width=0.5, color='#888'), hoverinfo='none', mode='lines')
            node_trace = go.Scatter(
                x=[pos[n][0] for n in G.nodes()], y=[pos[n][1] for n in G.nodes()],
                mode='markers+text', text=list(G.nodes()), textposition="top center",
                hovertext=node_texts, hoverinfo="text",
                marker=dict(showscale=True, colorscale='RdBu_r', color=node_colors, size=[np.log(G.nodes[n]['size']+1)*8 for n in G.nodes()], line_width=1, colorbar=dict(title="Growth"))
            )
            fig_net = go.Figure(data=[edge_trace, node_trace])
            fig_net.update_layout(showlegend=False, xaxis=dict(visible=False), yaxis=dict(visible=False))
            update_fig_layout(fig_net, "Trend Network", height=700, theme_config=theme_config)
            st.plotly_chart(fig_net, use_container_width=True, config={'editable': False})

            # --- Snapshot (Trend Network) ---
            trend_summary = [f"{n}: Growth {c_rec_net.get(n,0)/(c_pst_net.get(n,0)+1):.2f}" for n in list(G.nodes())[:15]]
            
            # Hubs & Edges
            deg_centrality = nx.degree_centrality(G)
            sorted_hubs = sorted(deg_centrality.items(), key=lambda x: x[1], reverse=True)[:10]
            hubs_str = ", ".join([f"{n}({val:.2f})" for n, val in sorted_hubs])
            
            sorted_edges = sorted(G.edges(data=True), key=lambda x: x[2]['weight'], reverse=True)[:10]
            edges_str = ", ".join([f"{u}-{v}" for u, v, d in sorted_edges])

            snap_data = utils.generate_rich_summary(df_target, title_col=col_map['title'], abstract_col=col_map['abstract'])
            snap_data['module'] = 'Explorer'
            snap_data['network_stats'] = {
                "hubs": hubs_str,
                "edges": edges_str,
                "notes": "Nodes colored by Growth Rate (Red=High, Blue=Low)."
            }

            utils.render_snapshot_button(
                title="ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯",
                description="æŠ€è¡“ç”¨èªã®å…±èµ·é–¢ä¿‚ã«æˆé•·ç‡ï¼ˆèµ¤=æ€¥ä¸Šæ˜‡ï¼‰ã‚’é‡ã­åˆã‚ã›ãŸãƒãƒƒãƒ—ã€‚",
                key="exp_trend_net_snap",
                fig=fig_net,
                data_summary=snap_data
            )

# ==================================================================
# --- 6. Comparative Strategy (ç«¶åˆæ¯”è¼ƒ) ---
# ==================================================================
elif selected_tab == "Comparative Strategy":
    st.subheader("Comparative Strategy")
    
    c1, c2 = st.columns(2)
    with c1: 
        my_comp = st.selectbox(
            "è‡ªç¤¾ (My Company)", 
            ["(é¸æŠã—ã¦ãã ã•ã„)"] + sorted_applicants, 
            format_func=lambda x: x if x == "(é¸æŠã—ã¦ãã ã•ã„)" else f"{x} ({app_count_dict.get(x, 0)}ä»¶)",
            key="comp_my"
        )
    with c2: 
        target_comp = st.selectbox(
            "ç«¶åˆä»–ç¤¾ (Competitor)", 
            ["(é¸æŠã—ã¦ãã ã•ã„)"] + sorted_applicants, 
            format_func=lambda x: x if x == "(é¸æŠã—ã¦ãã ã•ã„)" else f"{x} ({app_count_dict.get(x, 0)}ä»¶)",
            key="comp_target"
        )
    
    if my_comp != "(é¸æŠã—ã¦ãã ã•ã„)" and target_comp != "(é¸æŠã—ã¦ãã ã•ã„)":
        def get_keywords_for_app(app_name):
            if 'applicant_main' in df_main.columns:
                mask = df_main['applicant_main'].apply(lambda x: isinstance(x, list) and app_name in x)
            else:
                mask = df_main[col_map['applicant']].fillna('').str.contains(re.escape(app_name))
            return [w for sublist in df_main[mask]['explorer_keywords'] for w in sublist]

        words_my = get_keywords_for_app(my_comp)
        words_target = get_keywords_for_app(target_comp)
        c_my = Counter(words_my)
        c_tgt = Counter(words_target)
        
        # 1. Tornado Chart
        st.markdown("##### 1. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‡ºç¾é »åº¦æ¯”è¼ƒ (Tornado Chart)")
        all_keys = set(list(c_my.keys()) + list(c_tgt.keys()))
        valid_keys = [k for k in all_keys if (c_my[k] + c_tgt[k]) >= 3]
        
        tornado_data = []
        for k in valid_keys:
            tornado_data.append({
                "Keyword": k, "My Count": -c_my[k], # Left (Negative)
                "Competitor Count": c_tgt[k],       # Right (Positive)
                "My Abs": c_my[k], 
                "Total": c_my[k] + c_tgt[k]
            })
        df_tornado = pd.DataFrame(tornado_data).sort_values("Total", ascending=True).tail(30)
        
        if not df_tornado.empty:
            max_val = max(df_tornado["My Abs"].max(), df_tornado["Competitor Count"].max())
            range_x = [-max_val * 1.1, max_val * 1.1]
            tick_vals = [-max_val, -max_val/2, 0, max_val/2, max_val]
            tick_text = [str(int(abs(v))) for v in tick_vals]

            fig_tornado = go.Figure()
            fig_tornado.add_trace(go.Bar(y=df_tornado["Keyword"], x=df_tornado["My Count"], orientation='h', name=my_comp, marker_color=theme_config["color_sequence"][0]))
            fig_tornado.add_trace(go.Bar(y=df_tornado["Keyword"], x=df_tornado["Competitor Count"], orientation='h', name=target_comp, marker_color=theme_config["color_sequence"][1]))
            
            fig_tornado.update_layout(
                barmode='relative', bargap=0.1, 
                xaxis=dict(
                    title="å‡ºç¾ä»¶æ•° (å·¦: è‡ªç¤¾ / å³: ç«¶åˆ)", 
                    tickmode='array', tickvals=tick_vals, ticktext=tick_text,
                    range=range_x,
                    showline=True, linewidth=1, linecolor='black'
                ),
                yaxis=dict(side='right', showline=True, linewidth=1, linecolor='black'),
                legend=dict(orientation="h", yanchor="top", y=-0.2, xanchor="center", x=0.5),
                margin=dict(r=150, l=20, b=100)
            )
            update_fig_layout(fig_tornado, "Tornado Chart", height=800, theme_config=theme_config)
            st.plotly_chart(fig_tornado, use_container_width=True, config={'editable': False})
            st.info("å·¦å´ (é’ç³») ãŒè‡ªç¤¾ã€å³å´ (èµ¤/ã‚ªãƒ¬ãƒ³ã‚¸ç³») ãŒç«¶åˆã®å‡ºç¾æ•°ã‚’ç¤ºã—ã¾ã™ã€‚")
            
            utils.render_snapshot_button(
                title=f"Tornado Chart ({my_comp} vs {target_comp})",
                description="ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‡ºç¾é »åº¦ã®ç›´æ¥æ¯”è¼ƒã€‚å·¦å³ã¸ã®çªå‡ºãŒå„ç¤¾ã®ç‰¹å¾´ã‚’ç¤ºã™ã€‚",
                key="exp_tornado_snap",
                fig=fig_tornado,
                data_summary=df_tornado[['Keyword', 'My Abs', 'Competitor Count']].tail(20).to_string(index=False)
            )

        # 2. ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰
        st.markdown("##### 2. ä¼æ¥­åˆ¥ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰")
        c_wc1, c_wc2 = st.columns(2)
        with c_wc1:
            st.markdown(f"**{my_comp}**")
            if words_my: generate_wordcloud_and_list(words_my, my_comp, 30, FONT_PATH)
        with c_wc2:
            st.markdown(f"**{target_comp}**")
            if words_target: generate_wordcloud_and_list(words_target, target_comp, 30, FONT_PATH)
            
        # 3. æ”¯é…ç‡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        st.markdown(f"##### 3. æ”¯é…ç‡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ (é’=è‡ªç¤¾å„ªå‹¢ / èµ¤=ç«¶åˆå„ªå‹¢)")
        col_cs1, col_cs2 = st.columns(2)
        with col_cs1: cs_net_n = st.slider("æŠ½å‡ºå˜èªæ•°", 30, 100, 60, key="cs_net_n")
        with col_cs2: cs_net_th = st.slider("å…±èµ·é–¾å€¤", 0.01, 0.3, 0.05, 0.01, key="cs_net_th")
        
        combined_keywords = words_my + words_target
        c_combined = Counter(combined_keywords)
        top_nodes = [w for w, c in c_combined.most_common(cs_net_n)]
        
        if 'applicant_main' in df_main.columns:
            mask_2 = df_main['applicant_main'].apply(lambda x: isinstance(x, list) and (my_comp in x or target_comp in x))
        else:
            mask_2 = df_main[col_map['applicant']].fillna('').str.contains(re.escape(my_comp) + "|" + re.escape(target_comp))
        df_2 = df_main[mask_2]
        
        pair_counts = Counter()
        for kws in df_2['explorer_keywords']:
            valid_w = [w for w in set(kws) if w in top_nodes]
            if len(valid_w) >= 2:
                for pair in combinations(sorted(valid_w), 2): pair_counts[pair] += 1
                
        G = nx.Graph()
        for w in top_nodes: G.add_node(w, size=c_combined[w])
        for (u, v), c in pair_counts.items():
            weight = c / (c_combined[u] + c_combined[v] - c)
            if weight >= cs_net_th: G.add_edge(u, v, weight=weight)
        G.remove_nodes_from(list(nx.isolates(G)))
        
        if G.number_of_nodes() > 0:
            pos = nx.spring_layout(G, k=0.8, seed=42)
            node_colors, node_texts = [], []
            for node in G.nodes():
                m = c_my[node]; t = c_tgt[node]
                if m + t == 0: dom = 0.5
                else: dom = m / (m + t)
                node_colors.append(dom)
                node_texts.append(f"{node}<br>{my_comp}: {m}<br>{target_comp}: {t}")
            
            edge_x, edge_y = [], []
            for edge in G.edges():
                x0, y0 = pos[edge[0]]; x1, y1 = pos[edge[1]]
                edge_x.extend([x0, x1, None]); edge_y.extend([y0, y1, None])
            
            edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(width=0.5, color='#888'), hoverinfo='none', mode='lines')
            node_trace = go.Scatter(
                x=[pos[n][0] for n in G.nodes()], y=[pos[n][1] for n in G.nodes()],
                mode='markers+text', text=list(G.nodes()), textposition="top center",
                hovertext=node_texts, hoverinfo="text",
                marker=dict(showscale=True, colorscale='RdBu', color=node_colors, size=[np.log(G.nodes[n]['size']+1)*8 for n in G.nodes()], line_width=1, colorbar=dict(title="Dominance"))
            )
            fig_net = go.Figure(data=[edge_trace, node_trace])
            fig_net.update_layout(showlegend=False, xaxis=dict(visible=False), yaxis=dict(visible=False))
            update_fig_layout(fig_net, "Dominance Network", height=700, theme_config=theme_config)
            st.plotly_chart(fig_net, use_container_width=True, config={'editable': False})

            # --- Snapshot (Dominance Network) ---
            dom_summary = []
            for n in list(G.nodes())[:20]: # Top nodes
                m = c_my[n]; t = c_tgt[n]
                dom_val = m/(m+t) if (m+t)>0 else 0.5
                dom_summary.append(f"{n}: {my_comp}={m}, {target_comp}={t} (Dom={dom_val:.2f})")

            # Hubs & Edges
            deg_centrality = nx.degree_centrality(G)
            sorted_hubs = sorted(deg_centrality.items(), key=lambda x: x[1], reverse=True)[:10]
            hubs_str = ", ".join([f"{n}({val:.2f})" for n, val in sorted_hubs])
            
            sorted_edges = sorted(G.edges(data=True), key=lambda x: x[2]['weight'], reverse=True)[:10]
            edges_str = ", ".join([f"{u}-{v}" for u, v, d in sorted_edges])

            snap_data = utils.generate_rich_summary(df_2, title_col=col_map['title'], abstract_col=col_map['abstract'])
            snap_data['module'] = 'Explorer'
            snap_data['network_stats'] = {
                "hubs": hubs_str,
                "edges": edges_str,
                "notes": f"Dominance: {my_comp} vs {target_comp}. Blue favorable to {my_comp}, Red favorable to {target_comp}."
            }

            utils.render_snapshot_button(
                title=f"Dominance Network ({my_comp} vs {target_comp})",
                description="å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¸Šã§ã®ä¸¡ç¤¾ã®å„ªåŠ£ï¼ˆæ”¯é…ç‡ï¼‰åˆ†å¸ƒã€‚",
                key="exp_dom_net_snap",
                fig=fig_net,
                data_summary=snap_data
            )

# ==================================================================
# --- 7. Context Search (æ–‡è„ˆæ¤œç´¢) ---
# ==================================================================
elif selected_tab.startswith("Context Search"):
    st.subheader("Context Search (KWIC: KeyWord In Context)")
    search_kw = st.text_input("æ¤œç´¢ã—ãŸã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:", "")
    
    if search_kw:
        mask = df_main['explorer_text'].str.contains(re.escape(search_kw), na=False)
        df_hit = df_main[mask]
        st.write(f"ãƒ’ãƒƒãƒˆä»¶æ•°: {len(df_hit)} ä»¶")
        
        if not df_hit.empty:
            def highlight_text(text, kw):
                if pd.isna(text): return ""
                matches = [m.start() for m in re.finditer(re.escape(kw), text)]
                if not matches: return text[:100] + "..."
                snippets = []
                for idx in matches[:3]: 
                    start = max(0, idx - 40); end = min(len(text), idx + len(kw) + 40)
                    snippet = text[start:end].replace(kw, f"**{kw}**")
                    snippets.append(f"...{snippet}...")
                return " / ".join(snippets)

            for i, row in df_hit.head(20).iterrows():
                with st.expander(f"{row[col_map['title']]} ({row['year']}) - {row.get(col_map['applicant'], '')}"):
                    if col_map['abstract'] and pd.notna(row[col_map['abstract']]):
                        st.markdown(highlight_text(row[col_map['abstract']], search_kw))
                    st.caption(f"å‡ºé¡˜ç•ªå·: {row.get(col_map['app_num'], 'N/A')}")