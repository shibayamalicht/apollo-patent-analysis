import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
import datetime
import warnings
import io
import unicodedata
import re
import platform
import os
import string
from collections import Counter
from itertools import combinations

# æ©Ÿæ¢°å­¦ç¿’ãƒ»è‡ªç„¶è¨€èªå‡¦ç†
from umap import UMAP 
import hdbscan 
from wordcloud import WordCloud
from janome.tokenizer import Tokenizer
import networkx as nx
from scipy.spatial import ConvexHull

# æç”»ç”¨
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import japanize_matplotlib

# è­¦å‘Šã‚’éè¡¨ç¤º
warnings.filterwarnings('ignore')

# ==================================================================
# --- 1. ãƒšãƒ¼ã‚¸è¨­å®š ---
# ==================================================================
st.set_page_config(
    page_title="APOLLO | Saturn V", 
    page_icon="ğŸš€", 
    layout="wide"
)

# ==================================================================
# --- 2. ãƒ•ã‚©ãƒ³ãƒˆè¨­å®š ---
# ==================================================================
def get_japanese_font_path():
    system = platform.system()
    font_paths = []
    
    if system == "Darwin": # Mac
        font_paths = [
            "/System/Library/Fonts/ãƒ’ãƒ©ã‚®ãƒè§’ã‚´ã‚·ãƒƒã‚¯ W3.ttc",
            "/System/Library/Fonts/Hiragino Sans W3.ttc",
            "/System/Library/Fonts/Hiragino Kaku Gothic ProN.ttc",
            "/Library/Fonts/AppleGothic.ttf",
            "/System/Library/Fonts/AppleSDGothicNeo.ttc" 
        ]
    elif system == "Windows": # Windows
        font_paths = [
            "C:/Windows/Fonts/meiryo.ttc",
            "C:/Windows/Fonts/msgothic.ttc",
            "C:/Windows/Fonts/yugothr.ttc",
            "C:/Windows/Fonts/YuGothR.ttc"
        ]
    else: # Linux
        font_paths = [
            "/usr/share/fonts/opentype/ipafont-gothic/ipagp.ttf",
            "/usr/share/fonts/truetype/fonts-japanese-gothic.ttf",
            "/usr/share/fonts/noto/NotoSansCJK-Regular.ttc",
            "/usr/share/fonts/noto/NotoSansCJKjp-Regular.otf"
        ]
        
    for path in font_paths:
        if os.path.exists(path): return path
    return None

FONT_PATH = get_japanese_font_path()
if FONT_PATH:
    try:
        prop = fm.FontProperties(fname=FONT_PATH)
        plt.rcParams['font.family'] = prop.get_name()
    except:
        pass

# ==================================================================
# --- 3. å…±é€šãƒ‡ã‚¶ã‚¤ãƒ³è¨­å®š (CSS) ---
# ==================================================================
st.markdown("""
<style>
    html, body { 
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif; 
    }
    [data-testid="stSidebar"] h1 {
        color: #003366;
        font-weight: 900 !important;
        font-size: 2.5rem !important;
    }
    [data-testid="stSidebarNav"] {
        display: none !important;
    }
    [data-testid="stSidebar"] .block-container {
        padding-top: 2rem;
        padding-bottom: 1rem;
    }
    .block-container { 
        padding-top: 2rem; 
        padding-bottom: 2rem; 
    }
    h3 { border-bottom: 2px solid #f0f0f0; padding-bottom: 5px; }
    .stButton>button {
        font-weight: 600;
    }
    .stTabs [data-baseweb="tab-list"] {
        gap: 8px;
    }
    .stTabs [data-baseweb="tab"] {
        background-color: #f0f2f6;
        border-radius: 8px 8px 0 0;
        padding: 10px 15px;
    }
    .stTabs [aria-selected="true"] {
        background-color: #ffffff;
        border-bottom: 2px solid #003366;
    }
</style>
""", unsafe_allow_html=True)

# ==================================================================
# --- 4. ãƒ‡ã‚¶ã‚¤ãƒ³ãƒ†ãƒ¼ãƒç®¡ç† ---
# ==================================================================

def get_theme_config(theme_name):
    themes = {
        "APOLLO Standard": {
            "bg_color": "#ffffff",
            "text_color": "#333333",
            "sidebar_bg": "#f8f9fa",
            "plotly_template": "plotly_white",
            "color_sequence": px.colors.qualitative.G10,
            "density_scale": "Blues",
            "accent_color": "#003366",
            "css": """[data-testid="stHeader"] { background-color: #ffffff; } h1, h2, h3 { color: #003366; }"""
        },
        "Modern Presentation": {
            "bg_color": "#fdfdfd",
            "text_color": "#2c3e50",
            "sidebar_bg": "#eaeaea",
            "plotly_template": "plotly_white",
            "color_sequence": ["#264653", "#2a9d8f", "#e9c46a", "#f4a261", "#e76f51", "#8ab17d"],
            "density_scale": "Teal",
            "accent_color": "#264653",
            "css": """[data-testid="stSidebar"] { background-color: #eaeaea; } [data-testid="stHeader"] { background-color: #fdfdfd; } h1, h2, h3 { color: #264653; font-family: "Georgia", serif; } .stButton>button { background-color: #264653; color: white; border-radius: 0px; }"""
        }
    }
    return themes.get(theme_name, themes["APOLLO Standard"])

def update_fig_layout(fig, title, height=1000, width=800, theme_config=None, show_axes=False):
    if theme_config is None:
        return fig
    
    layout_params = dict(
        template=theme_config["plotly_template"],
        title=dict(text=title, font=dict(size=18, color=theme_config["text_color"])),
        paper_bgcolor=theme_config["bg_color"],
        plot_bgcolor=theme_config["bg_color"],
        font=dict(color=theme_config["text_color"], family="Helvetica Neue"),
        height=height,
        width=width,
        margin=dict(l=20, r=20, t=60, b=20),
        legend=dict(
            orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1,
            bgcolor="rgba(255,255,255,0.8)", bordercolor="#eee", borderwidth=1
        )
    )

    if not show_axes:
        layout_params['xaxis'] = dict(visible=False, showgrid=False, zeroline=False, showticklabels=False)
        layout_params['yaxis'] = dict(
            visible=False, showgrid=False, zeroline=False, showticklabels=False,
            scaleanchor="x", scaleratio=1
        )
    else:
        if "width" in layout_params:
            del layout_params["width"]

        layout_params['xaxis'] = dict(
            visible=True, showgrid=False, zeroline=False, showline=False, showticklabels=True
        )
        layout_params['yaxis'] = dict(
            visible=True, showgrid=True, gridcolor='#eee', zeroline=False, showline=False, showticklabels=True
        )

    fig.update_layout(**layout_params)
    return fig

# ==================================================================
# --- 5. ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†é–¢æ•° ---
# ==================================================================

@st.cache_resource
def load_tokenizer_saturn():
    return Tokenizer()

t = load_tokenizer_saturn()

# ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰å®šç¾©
_stopwords_original_list = [
    "ã™ã‚‹","ã‚ã‚‹","ãªã‚‹","ãŸã‚","ã“ã¨","ã‚ˆã†","ã‚‚ã®","ã“ã‚Œ","ãã‚Œ","ã‚ã‚Œ","ã“ã“","ãã“","ã©ã‚Œ","ã©ã®",
    "ã“ã®","ãã®","å½“è©²","è©²","ãŠã‚ˆã³","åŠã³","ã¾ãŸã¯","ã¾ãŸ","ä¾‹ãˆã°","ä¾‹ãˆã°ã¯","ã«ãŠã„ã¦","ã«ã‚ˆã‚Š",
    "ã«å¯¾ã—ã¦","ã«é–¢ã—ã¦","ã«ã¤ã„ã¦","ã¨ã—ã¦","ã¨ã—ã¦ã¯","å ´åˆ","ä¸€æ–¹","ä»–æ–¹","ã•ã‚‰ã«","ãã—ã¦","ãŸã ã—",
    "ãªãŠ","ç­‰","ãªã©","ç­‰ã€…","ã„ã‚ã‚†ã‚‹","æ‰€è¬‚","åŒæ§˜","åŒæ™‚","å‰è¨˜","æœ¬","åŒ","å„","å„ç¨®","æ‰€å®š","æ‰€æœ›",
    "ä¸€ä¾‹","ä»–","ä¸€éƒ¨","ä¸€ã¤","è¤‡æ•°","å°‘ãªãã¨ã‚‚","å°‘ãªãã¨ã‚‚ä¸€ã¤","ä¸Šè¨˜","ä¸‹è¨˜","å‰è¿°","å¾Œè¿°","æ—¢è¿°",
    "é–¢ã™ã‚‹","åŸºã¥ã","ç”¨ã„ã‚‹","ä½¿ç”¨","åˆ©ç”¨","æœ‰ã™ã‚‹","å«ã‚€","å‚™ãˆã‚‹","è¨­ã‘ã‚‹","ã™ãªã‚ã¡","å¾“ã£ã¦",
    "ã—ã‹ã—ãªãŒã‚‰","æ¬¡ã«","ç‰¹ã«","å…·ä½“çš„ã«","è©³ç´°ã«","ã„ãšã‚Œ","ã†ã¡","ãã‚Œãã‚Œ","ã¨ã",
    "ã‹ã‹ã‚‹","ã‹ã‚ˆã†ãª","ã‹ã‹ã‚‹å ´åˆ","æœ¬ä»¶","æœ¬é¡˜","æœ¬å‡ºé¡˜","æœ¬æ˜ç´°æ›¸","ã“ã‚Œã‚‰","ãã‚Œã‚‰","å„ã€…","éšæ™‚","é©å®œ",
    "ä»»æ„","å¿…ãšã—ã‚‚","é€šå¸¸","ä¸€èˆ¬ã«","å…¸å‹çš„","ä»£è¡¨çš„",
    "æœ¬ç™ºæ˜","ç™ºæ˜","å®Ÿæ–½ä¾‹","å®Ÿæ–½å½¢æ…‹","å¤‰å½¢ä¾‹","è«‹æ±‚","è«‹æ±‚é …","å›³","å›³é¢","ç¬¦å·","ç¬¦å·ã®èª¬æ˜",
    "å›³é¢ã®ç°¡å˜ãªèª¬æ˜","ç™ºæ˜ã®è©³ç´°ãªèª¬æ˜","æŠ€è¡“åˆ†é‡","èƒŒæ™¯æŠ€è¡“","å¾“æ¥æŠ€è¡“","ç™ºæ˜ãŒè§£æ±ºã—ã‚ˆã†ã¨ã™ã‚‹èª²é¡Œ","èª²é¡Œ",
    "è§£æ±ºæ‰‹æ®µ","åŠ¹æœ","è¦ç´„","ç™ºæ˜ã®åŠ¹æœ","ç›®çš„","æ‰‹æ®µ","æ§‹æˆ","æ§‹é€ ","å·¥ç¨‹","å‡¦ç†","æ–¹æ³•","æ‰‹æ³•","æ–¹å¼",
    "ã‚·ã‚¹ãƒ†ãƒ ","ãƒ—ãƒ­ã‚°ãƒ©ãƒ ","è¨˜æ†¶åª’ä½“","ç‰¹å¾´","ç‰¹å¾´ã¨ã™ã‚‹","ç‰¹å¾´éƒ¨","ã‚¹ãƒ†ãƒƒãƒ—","ãƒ•ãƒ­ãƒ¼","ã‚·ãƒ¼ã‚±ãƒ³ã‚¹","å®šç¾©",
    "é–¢ä¿‚","å¯¾å¿œ","æ•´åˆ","å®Ÿæ–½ã®å½¢æ…‹","å®Ÿæ–½ã®æ…‹æ§˜","æ…‹æ§˜","å¤‰å½¢","ä¿®æ­£ä¾‹","å›³ç¤º","å›³ç¤ºä¾‹","å›³ç¤ºã—ãªã„",
    "å‚ç…§","å‚ç…§ç¬¦å·","æ®µè½","è©³ç´°èª¬æ˜","è¦æ—¨","ä¸€å®Ÿæ–½å½¢æ…‹","ä»–ã®å®Ÿæ–½å½¢æ…‹","ä¸€å®Ÿæ–½ä¾‹","åˆ¥ã®å´é¢","ä»˜è¨˜",
    "é©ç”¨ä¾‹","ç”¨èªã®å®šç¾©","é–‹ç¤º","æœ¬é–‹ç¤º","é–‹ç¤ºå†…å®¹","ä¸Šéƒ¨","ä¸‹éƒ¨","å†…éƒ¨","å¤–éƒ¨","å†…å´","å¤–å´","è¡¨é¢",
    "è£é¢","å´é¢","ä¸Šé¢","ä¸‹é¢","ç«¯é¢","å…ˆç«¯","åŸºç«¯","å¾Œç«¯","ä¸€ç«¯","ä»–ç«¯","ä¸­å¿ƒ","ä¸­å¤®","å‘¨ç¸","å‘¨è¾º",
    "è¿‘å‚","æ–¹å‘","ä½ç½®","ç©ºé–“","é ˜åŸŸ","ç¯„å›²","é–“éš”","è·é›¢","å½¢çŠ¶","å½¢æ…‹","çŠ¶æ…‹","ç¨®é¡","å±¤","è†œ","éƒ¨",
    "éƒ¨æ","éƒ¨ä½","éƒ¨å“","æ©Ÿæ§‹","è£…ç½®","å®¹å™¨","çµ„æˆ","ææ–™","ç”¨é€”","é©ç”¨","é©ç”¨ä¾‹","ç‰‡å´","ä¸¡å´","å·¦å´",
    "å³å´","å‰æ–¹","å¾Œæ–¹","ä¸Šæµ","ä¸‹æµ","éš£æ¥","è¿‘æ¥","é›¢é–“","é–“ç½®","ä»‹åœ¨","é‡ç•³","æ¦‚ã­","ç•¥","ç•¥ä¸­å¤®",
    "å›ºå®šå´","å¯å‹•å´","ä¼¸é•·","åç¸®","ä¿‚åˆ","åµŒåˆ","å–ä»˜","é€£çµéƒ¨","æ”¯æŒä½“","æ”¯æŒéƒ¨","ã‚¬ã‚¤ãƒ‰éƒ¨",
    "ãƒ‡ãƒ¼ã‚¿","æƒ…å ±","ä¿¡å·","å‡ºåŠ›","å…¥åŠ›","åˆ¶å¾¡","æ¼”ç®—","å–å¾—","é€ä¿¡","å—ä¿¡","è¡¨ç¤º","é€šçŸ¥","è¨­å®š","å¤‰æ›´",
    "æ›´æ–°","ä¿å­˜","å‰Šé™¤","è¿½åŠ ","å®Ÿè¡Œ","é–‹å§‹","çµ‚äº†","ç¶™ç¶š","åœæ­¢","åˆ¤å®š","åˆ¤æ–­","æ±ºå®š","é¸æŠ","ç‰¹å®š",
    "æŠ½å‡º","æ¤œå‡º","æ¤œçŸ¥","æ¸¬å®š","è¨ˆæ¸¬","ç§»å‹•","å›è»¢","å¤‰ä½","å¤‰å½¢","å›ºå®š","é…ç½®","ç”Ÿæˆ","ä»˜ä¸","ä¾›çµ¦",
    "é©ç”¨","ç…§åˆ","æ¯”è¼ƒ","ç®—å‡º","è§£æ","åŒå®š","åˆæœŸåŒ–","èª­å‡º","æ›¸è¾¼","ç™»éŒ²","è¨˜éŒ²","é…ä¿¡","é€£æº","åˆ‡æ›¿",
    "èµ·å‹•","å¾©å¸°","ç›£è¦–","é€šçŸ¥å‡¦ç†","å–å¾—å‡¦ç†","æ¼”ç®—å‡¦ç†","è‰¯å¥½","å®¹æ˜“","ç°¡ä¾¿","é©åˆ‡","æœ‰åˆ©","æœ‰ç”¨","æœ‰åŠ¹",
    "åŠ¹æœçš„","é«˜ã„","ä½ã„","å¤§ãã„","å°ã•ã„","æ–°è¦","æ”¹è‰¯","æ”¹å–„","æŠ‘åˆ¶","å‘ä¸Š","ä½æ¸›","å‰Šæ¸›","å¢—åŠ ",
    "æ¸›å°‘","å¯èƒ½","å¥½é©","å¥½ã¾ã—ã„","æœ›ã¾ã—ã„","å„ªã‚Œã‚‹","å„ªã‚ŒãŸ","é«˜æ€§èƒ½","é«˜åŠ¹ç‡","ä½ã‚³ã‚¹ãƒˆ","ã‚³ã‚¹ãƒˆ",
    "ç°¡æ˜“","å®‰å®š","å®‰å®šæ€§","è€ä¹…","è€ä¹…æ€§","ä¿¡é ¼æ€§","ç°¡ç´ ","ç°¡ç•¥","å˜ç´”","æœ€é©","æœ€é©åŒ–","æ±ç”¨","æ±ç”¨æ€§",
    "å®Ÿç¾","é”æˆ","ç¢ºä¿","ç¶­æŒ","é˜²æ­¢","å›é¿","ä¿ƒé€²","ä¸è¦","å¿…è¦","é«˜ç²¾åº¦","çœé›»åŠ›","çœè³‡æº","é«˜ä¿¡é ¼",
    "ä½è² è·","é«˜ç´”åº¦","é«˜å¯†åº¦","é«˜æ„Ÿåº¦","è¿…é€Ÿ","å††æ»‘","ç°¡ç•¥åŒ–","ä½ä¾¡æ ¼","å®ŸåŠ¹çš„","å¯èƒ½åŒ–","æœ‰åŠ¹åŒ–",
    "éå¿…é ˆ","é©åˆ","äº’æ›","å‡ºé¡˜","å‡ºé¡˜äºº","å‡ºé¡˜ç•ªå·","å‡ºé¡˜æ—¥","å‡ºé¡˜æ›¸","å‡ºé¡˜å…¬é–‹","å…¬é–‹","å…¬é–‹ç•ªå·",
    "å…¬é–‹å…¬å ±","å…¬å ±","å…¬å ±ç•ªå·","ç‰¹è¨±","ç‰¹è¨±ç•ªå·","ç‰¹è¨±æ–‡çŒ®","éç‰¹è¨±æ–‡çŒ®","å¼•ç”¨","å¼•ç”¨æ–‡çŒ®","å…ˆè¡ŒæŠ€è¡“",
    "å¯©æŸ»","å¯©æŸ»å®˜","æ‹’çµ¶","æ„è¦‹æ›¸","è£œæ­£æ›¸","å„ªå…ˆ","å„ªå…ˆæ—¥","åˆ†å‰²å‡ºé¡˜","ç¶™ç¶šå‡ºé¡˜","å›½å†…ç§»è¡Œ","å›½éš›å‡ºé¡˜",
    "å›½éš›å…¬é–‹","PCT","ç™»éŒ²","å…¬é–‹æ—¥","å¯©æŸ»è«‹æ±‚","æ‹’çµ¶ç†ç”±","è£œæ­£","è¨‚æ­£","ç„¡åŠ¹å¯©åˆ¤","ç•°è­°","å–æ¶ˆ","å–ä¸‹ã’",
    "äº‹ä»¶ç•ªå·","ä»£ç†äºº","å¼ç†å£«","ä¿‚å±","çµŒé",
    "ç¬¬","ç¬¬ä¸€","ç¬¬äºŒ","ç¬¬ä¸‰","ç¬¬1","ç¬¬ï¼’","ç¬¬ï¼“","ç¬¬ï¼‘","ç¬¬ï¼’","ç¬¬ï¼“","ä¸€","äºŒ","ä¸‰","å››","äº”","å…­","ä¸ƒ","å…«","ä¹","é›¶","æ•°","è¤‡åˆ","å¤šæ•°","å°‘æ•°","å›³1","å›³2","å›³3","å›³4","å›³5","å›³6","å›³7","å›³8","å›³9","è¡¨1","è¡¨2","è¡¨3","å¼1","å¼2","å¼3","%","ï¼…","wt%","vol%","è³ªé‡%","é‡é‡%","å®¹é‡%","mol","mol%","mol/L","M","mm","cm","m","nm","Î¼m","Î¼","rpm","Pa","kPa","MPa","GPa","N","W","V","A","mA","Hz","kHz","MHz","GHz","â„ƒ","Â°C","K","mL","L","g","kg","mg","wt","vol","h","hr","hrs","min","s","sec","ppm","ppb","bar","Î©","ohm","J","kJ","Wh","kWh",
    "æ ªå¼ä¼šç¤¾","æœ‰é™ä¼šç¤¾","åˆè³‡ä¼šç¤¾","åˆåä¼šç¤¾","åˆåŒä¼šç¤¾","Inc","Inc.","Ltd","Ltd.","Co","Co.","Corp","Corp.","LLC",
    "GmbH","AG","BV","B.V.","S.A.","S.p.A.","ï¼ˆæ ªï¼‰","ãˆ±","ï¼ˆæœ‰ï¼‰",
    "æº¶æ¶²","æº¶åª’","è§¦åª’","åå¿œ","ç”Ÿæˆç‰©","åŸæ–™","æˆåˆ†","å«æœ‰","å«æœ‰é‡","é…åˆ","æ··åˆ","æ··åˆç‰©","æ¿ƒåº¦","æ¸©åº¦","æ™‚é–“",
    "å‰²åˆ","æ¯”ç‡","åŸº","å®˜èƒ½åŸº","åŒ–åˆç‰©","çµ„æˆç‰©","æ¨¹è„‚","ãƒãƒªãƒãƒ¼","ãƒ¢ãƒãƒãƒ¼","åŸºæ¿","åŸºæ","ãƒ•ã‚£ãƒ«ãƒ ","ã‚·ãƒ¼ãƒˆ",
    "ç²’å­","ç²‰æœ«","æ¯”è¼ƒä¾‹","å‚è€ƒä¾‹","è©¦é¨“","è©¦æ–™","è©•ä¾¡","æ¡ä»¶","å®Ÿé¨“","å®Ÿé¨“ä¾‹","åå¿œæ¡ä»¶","åå¿œæ™‚é–“","åå¿œæ¸©åº¦",
    "å‡¦ç†è£…ç½®","ç«¯æœ«","ãƒ¦ãƒ‹ãƒƒãƒˆ","ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«","å›è·¯","ç´ å­","é›»æº","é›»åœ§","é›»æµ","ä¿¡å·ç·š","é…ç·š","ç«¯å­","ç«¯éƒ¨","æ¥ç¶š",
    "æ¥ç¶šéƒ¨","æ¼”ç®—éƒ¨","è¨˜æ†¶éƒ¨","è¨˜æ†¶è£…ç½®","è¨˜éŒ²åª’ä½“","ãƒ¦ãƒ¼ã‚¶","åˆ©ç”¨è€…","ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ","ã‚µãƒ¼ãƒ","ç”»é¢","UI","GUI",
    "ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹","ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹","DB","ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯","é€šä¿¡","è¦æ±‚","å¿œç­”","ãƒªã‚¯ã‚¨ã‚¹ãƒˆ","ãƒ¬ã‚¹ãƒãƒ³ã‚¹","ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿",
    "å¼•æ•°","å±æ€§","ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£","ãƒ•ãƒ©ã‚°","ID","ãƒ•ã‚¡ã‚¤ãƒ«","ãƒ‡ãƒ¼ã‚¿æ§‹é€ ","ãƒ†ãƒ¼ãƒ–ãƒ«","ãƒ¬ã‚³ãƒ¼ãƒ‰",
    "è»¸","ã‚·ãƒ£ãƒ•ãƒˆ","ã‚®ã‚¢","ãƒ¢ãƒ¼ã‚¿","ã‚¨ãƒ³ã‚¸ãƒ³","ã‚¢ã‚¯ãƒãƒ¥ã‚¨ãƒ¼ã‚¿","ã‚»ãƒ³ã‚µ","ãƒãƒ«ãƒ–","ãƒãƒ³ãƒ—","ç­ä½“","ãƒã‚¦ã‚¸ãƒ³ã‚°","ãƒ•ãƒ¬ãƒ¼ãƒ ",
    "ã‚·ãƒ£ãƒ¼ã‚·","é§†å‹•","ä¼é”","æ”¯æŒ","é€£çµ","è§£æ±º", "æº–å‚™", "æä¾›", "ç™ºç”Ÿ", "ä»¥ä¸Š", "ååˆ†",
    "ã§ãã‚‹", "ã„ã‚‹", "æ˜ç´°æ›¸", "è¨˜è¼‰", "è¨˜è¿°", "æ²è¼‰", "è¨€åŠ", "å†…å®¹", "è©³ç´°", "èª¬æ˜", "è¡¨è¨˜", "è¡¨ç¾", "ç®‡æ¡æ›¸ã", "ä»¥ä¸‹ã®", "ä»¥ä¸Šã®", "å…¨ã¦ã®", "ä»»æ„ã®", "ç‰¹å®šã®"
]

@st.cache_data
def expand_stopwords_to_full_width(words):
    expanded = set(words)
    hankaku = string.ascii_letters + string.digits
    zenkaku = "ï½ï½‚ï½ƒï½„ï½…ï½†ï½‡ï½ˆï½‰ï½Šï½‹ï½Œï½ï½ï½ï½ï½‘ï½’ï½“ï½”ï½•ï½–ï½—ï½˜ï½™ï½šï¼¡ï¼¢ï¼£ï¼¤ï¼¥ï¼¦ï¼§ï¼¨ï¼©ï¼ªï¼«ï¼¬ï¼­ï¼®ï¼¯ï¼°ï¼±ï¼²ï¼³ï¼´ï¼µï¼¶ï¼·ï¼¸ï¼¹ï¼ºï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™"
    trans = str.maketrans(hankaku, zenkaku)
    for w in words:
        if any(c in hankaku for c in w): expanded.add(w.translate(trans))
    return sorted(list(expanded))

stopwords = set(expand_stopwords_to_full_width(_stopwords_original_list))

_ngram_rows = [
    ("å‚ç…§ç¬¦å·ä»˜ãè¦ç´ ", r"[ä¸€-é¾¥ã-ã‚“ã‚¡-ãƒ³A-Za-z0-9ï¼\-ï¼‹ãƒ»]+?(?:éƒ¨|å±¤|é¢|ä½“|æ¿|å­”|æº|ç‰‡|éƒ¨æ|è¦ç´ |æ©Ÿæ§‹|è£…ç½®|æ‰‹æ®µ|é›»æ¥µ|ç«¯å­|é ˜åŸŸ|åŸºæ¿|å›è·¯|ææ–™|å·¥ç¨‹)\s*[ï¼ˆ(]\s*[0-9ï¼-ï¼™A-Za-z]+[A-Za-z]?\s*[ï¼‰)]", "regex", 1),
    ("å‚ç…§ç¬¦å·ä»˜ãè¦ç´ ", r"(?:ä¸Šè¨˜|å‰è¨˜)?[ä¸€-é¾¥ã-ã‚“ã‚¡-ãƒ³A-Za-z0-9ï¼\-ï¼‹ãƒ»]+?(?:éƒ¨|å±¤|é¢|ä½“|æ¿|å­”|æº|ç‰‡|éƒ¨æ|è¦ç´ |æ©Ÿæ§‹|è£…ç½®|æ‰‹æ®µ|é›»æ¥µ|ç«¯å­|é ˜åŸŸ|åŸºæ¿|å›è·¯|ææ–™|å·¥ç¨‹)\s*[0-9ï¼-ï¼™A-Za-z]+[A-Za-z]?", "regex", 1),
    ("å‚ç…§ç¬¦å·ä»˜ãè¦ç´ ", r"[A-Z]+[0-9]+", "regex", 1),
    ("è¦‹å‡ºã—ãƒ»ç« å¥","ä¸€å®Ÿæ–½å½¢æ…‹ã«ãŠã„ã¦","literal",1), ("è¦‹å‡ºã—ãƒ»ç« å¥","ä»–ã®å®Ÿæ–½å½¢æ…‹ã«ãŠã„ã¦","literal",1), ("è¦‹å‡ºã—ãƒ»ç« å¥","åˆ¥ã®å®Ÿæ–½å½¢æ…‹ã«ãŠã„ã¦","literal",1),
    ("è¦‹å‡ºã—ãƒ»ç« å¥","æœ¬æ˜ç´°æ›¸ã«ãŠã„ã¦","literal",1), ("è¦‹å‡ºã—ãƒ»ç« å¥","æœ¬æ˜ç´°æ›¸ã§ã¯","literal",1), ("è¦‹å‡ºã—ãƒ»ç« å¥","æœ¬ç™ºæ˜ã®ä¸€å´é¢","literal",1),
    ("è¦‹å‡ºã—ãƒ»ç« å¥","ä¸€å®Ÿæ–½ä¾‹ã«ãŠã„ã¦","literal",1), ("è¦‹å‡ºã—ãƒ»ç« å¥","ä»–ã®å®Ÿæ–½ä¾‹ã«ãŠã„ã¦","literal",1), ("è¦‹å‡ºã—ãƒ»ç« å¥","å¥½ã¾ã—ã„æ…‹æ§˜ã¨ã—ã¦","literal",2),
    ("è¦‹å‡ºã—ãƒ»ç« å¥","å¥½é©ã«ã¯","literal",2), ("è¦‹å‡ºã—ãƒ»ç« å¥","ç”¨èªã®å®šç¾©","literal",2), ("è¦‹å‡ºã—ãƒ»ç« å¥","å›³ç¤ºã—ãªã„","literal",2),
    ("å›³è¡¨å‚ç…§", r"å›³[ ã€€]*[ï¼-ï¼™0-9]+ã«ç¤ºã™", "regex", 1), ("å›³è¡¨å‚ç…§", r"è¡¨[ ã€€]*[ï¼-ï¼™0-9]+ã«ç¤ºã™", "regex", 1),
    ("å›³è¡¨å‚ç…§", r"å¼[ ã€€]*[ï¼-ï¼™0-9]+ã«ç¤ºã™", "regex", 1), ("å›³è¡¨å‚ç…§", r"è«‹æ±‚é …[ ã€€]*[ï¼-ï¼™0-9]+", "regex", 1),
    ("å›³è¡¨å‚ç…§", r"(?:ã€|\[)\s*[ï¼-ï¼™0-9]{4,5}\s*(?:ã€‘|\])", "regex", 1), ("å›³è¡¨å‚ç…§", r"[ï¼ˆ(][ï¼-ï¼™0-9]+[ï¼‰)]", "regex", 2),
    ("å›³è¡¨å‚ç…§", r"ç¬¬\s*[ï¼-ï¼™0-9]+ã®?å®Ÿæ–½å½¢æ…‹", "regex", 2), ("å›³è¡¨å‚ç…§", r"æ®µè½\s*[ï¼-ï¼™0-9]+", "regex", 2),
    ("å›³è¡¨å‚ç…§", r"å›³[ ã€€]*[ï¼-ï¼™0-9]+[A-Za-z]?", "regex", 2), ("å®šç¾©å°å…¥", r"ä»¥ä¸‹ã€[^ã€ã€‚]+ã‚’[^ã€ã€‚]+ã¨ç§°ã™ã‚‹", "regex", 1),
    ("å®šç¾©å°å…¥", r"ä»¥ä¸‹ã€[^ã€ã€‚]+ã‚’[^ã€ã€‚]+ã¨ã„ã†", "regex", 1), ("æ©Ÿèƒ½å¥","ã—ã¦ã‚‚ã‚ˆã„","literal",1), ("æ©Ÿèƒ½å¥","ã§ã‚ã£ã¦ã‚‚ã‚ˆã„","literal",1),
    ("æ©Ÿèƒ½å¥","ã™ã‚‹ã“ã¨ãŒã§ãã‚‹","literal",1), ("æ©Ÿèƒ½å¥","è¡Œã†ã“ã¨ãŒã§ãã‚‹","literal",1), ("æ©Ÿèƒ½å¥","ã«é™å®šã•ã‚Œãªã„","literal",1),
    ("æ©Ÿèƒ½å¥","ã«é™ã‚‰ã‚Œãªã„","literal",1), ("æ©Ÿèƒ½å¥","ä¸€ä¾‹ã¨ã—ã¦","literal",2), ("æ©Ÿèƒ½å¥","ä¾‹ç¤ºçš„ã«ã¯","literal",2),
    ("å‚ç…§å¥","å‰è¿°ã®ã¨ãŠã‚Š","literal",2), ("å‚ç…§å¥","å‰è¿°ã®é€šã‚Š","literal",2), ("å‚ç…§å¥","å¾Œè¿°ã™ã‚‹ã‚ˆã†ã«","literal",2),
    ("å‚ç…§å¥","å¾Œè¿°ã®ã¨ãŠã‚Š","literal",2), ("ç¯„å›²è¡¨ç¾", r"å°‘ãªãã¨ã‚‚(?:ä¸€|ï¼‘)ã¤", "regex", 2), ("ç¯„å›²è¡¨ç¾", "å°‘ãªãã¨ã‚‚ä¸€éƒ¨", "literal", 2),
    ("ç¯„å›²è¡¨ç¾", r"è¤‡æ•°ã®(?:å®Ÿæ–½å½¢æ…‹|æ§‹æˆ|è¦ç´ )", "regex", 3), ("èª²é¡Œå¥", r"(?:ä¸Šè¨˜|å‰è¨˜)ã®?èª²é¡Œ", "regex", 1),
    ("æ¥ç¶šãƒ»è«–ç†","ä¸€æ–¹ã§","literal",3), ("æ¥ç¶šãƒ»è«–ç†","ä»–æ–¹ã§","literal",3), ("æ¥ç¶šãƒ»è«–ç†","ã™ãªã‚ã¡","literal",3),
    ("æ¥ç¶šãƒ»è«–ç†","ã—ãŸãŒã£ã¦","literal",3), ("æ¥ç¶šãƒ»è«–ç†","ã—ã‹ã—ãªãŒã‚‰","literal",3), ("æ¥ç¶šãƒ»è«–ç†","ä¾‹ãˆã°","literal",3),
    ("æ¥ç¶šãƒ»è«–ç†","å…·ä½“çš„ã«ã¯","literal",3), ("è£œåŠ©å¥","ä»¥ä¸‹ã«èª¬æ˜ã™ã‚‹","literal",3), ("è£œåŠ©å¥","å‰è¨˜ã®ã¨ãŠã‚Š","literal",3),
    ("è£œåŠ©å¥","ã“ã‚Œã«ã‚ˆã‚Š","literal",3), ("è£œåŠ©å¥","ã“ã®ã‚ˆã†ã«","literal",3)
]

_ngram_compiled = sorted(_ngram_rows, key=lambda x: (x[3], -len(x[1]) if x[2]=="literal" else -50))
_ngram_compiled = [(cat, re.compile(pat) if ptype == "regex" else pat, ptype, pri) for cat, pat, ptype, pri in _ngram_compiled]

def normalize_text(text):
    if not isinstance(text, str): text = "" if pd.isna(text) else str(text)
    text = unicodedata.normalize("NFKC", text)
    text = text.replace("Âµ", "Î¼")
    text = re.sub(r"\s+", " ", text)
    return text

def apply_ngram_filters(text):
    for cat, pat, ptype, pri in _ngram_compiled:
        if ptype == "literal":
            if pat in text: text = text.replace(pat, "")
        else:
            text = pat.sub("", text)
    return text

@st.cache_data
def extract_compound_nouns(text):
    text = normalize_text(text)
    text = apply_ngram_filters(text) 
    text = re.sub(r'ã€.*?ã€‘', '', text)
    text = re.sub(r'[!\"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~]', ' ', text)

    tokens = t.tokenize(text)
    words, compound_word = [], ''
    for token in tokens:
        pos = token.part_of_speech.split(',')[0]
        if pos == 'åè©':
            compound_word += token.surface
        else:
            if (len(compound_word) > 1 and
                compound_word not in stopwords and
                not re.fullmatch(r'[\dï¼-ï¼™]+', compound_word) and
                not re.fullmatch(r'(å›³|è¡¨|å¼|ç¬¬)[\dï¼-ï¼™]+.*', compound_word) and
                not re.match(r'^(ä¸Šè¨˜|å‰è¨˜|æœ¬é–‹ç¤º|å½“è©²|è©²)', compound_word) and
                not re.search(r'[0-9ï¼-ï¼™]+[)ï¼‰]?$', compound_word) and
                not re.match(r'[0-9ï¼-ï¼™]+[a-zA-Zï½-ï½šï¼¡-ï¼º]', compound_word)):
                words.append(compound_word)
            compound_word = ''
            
    if (len(compound_word) > 1 and
        compound_word not in stopwords and
        not re.fullmatch(r'[\dï¼-ï¼™]+', compound_word) and
        not re.fullmatch(r'(å›³|è¡¨|å¼|ç¬¬)[\dï¼-ï¼™]+.*', compound_word) and
        not re.match(r'^(ä¸Šè¨˜|å‰è¨˜|æœ¬é–‹ç¤º|å½“è©²|è©²)', compound_word) and
        not re.search(r'[0-9ï¼-ï¼™]+[)ï¼‰]?$', compound_word) and
        not re.match(r'[0-9ï¼-ï¼™]+[a-zA-Zï½-ï½šï¼¡-ï¼º]', compound_word)):
        words.append(compound_word)
    return words

def generate_wordcloud_and_list(words, title, top_n=20, font_path=None):
    if not words: return None
    word_freq = Counter(words)
    try:
        wc = WordCloud(
            width=800, height=400, background_color='white',
            font_path=font_path, collocations=False,
            max_words=100
        ).generate_from_frequencies(word_freq)

        fig, ax = plt.subplots(figsize=(12, 6))
        ax.imshow(wc, interpolation='bilinear')
        ax.set_title(title, fontsize=20)
        ax.axis('off')
        st.pyplot(fig)
    except Exception as e:
        st.error(f"ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã®æç”»ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

def get_top_tfidf_words(row_vector, feature_names, top_n=5):
    scores = row_vector.toarray().flatten() 
    indices = np.argsort(scores)[::-1]
    non_zero_indices = [i for i in indices if scores[i] > 0]
    top_indices = non_zero_indices[:top_n]
    top_words = [feature_names[i] for i in top_indices]
    return ", ".join(top_words)

def update_hover_text(df, col_map):
    hover_texts = []
    for index, row in df.iterrows():
        text = ""
        if col_map['title'] and pd.notna(row[col_map['title']]): text += f"<b>åç§°:</b> {str(row[col_map['title']])[:50]}...<br>"
        if col_map['app_num'] and pd.notna(row[col_map['app_num']]): text += f"<b>ç•ªå·:</b> {row[col_map['app_num']]}<br>"
        if col_map['applicant'] and pd.notna(row[col_map['applicant']]): text += f"<b>å‡ºé¡˜äºº:</b> {str(row[col_map['applicant']])[:50]}...<br>"
        if 'characteristic_words' in row: text += f"<b>ç‰¹å¾´èª:</b> {row['characteristic_words']}<br>"
        if 'cluster_label' in row: text += f"<b>ã‚¯ãƒ©ã‚¹ã‚¿:</b> {row['cluster_label']}"
        hover_texts.append(text)
    df['hover_text'] = hover_texts
    return df

def update_drill_hover_text(df_subset):
    df_subset['drill_hover_text'] = df_subset.apply(
        lambda row: f"{row['hover_text']}<br><b>ã‚µãƒ–ã‚¯ãƒ©ã‚¹ã‚¿:</b> {row['drill_cluster_label']}", axis=1
    )
    return df_subset

def _create_label_editor_ui(original_map, current_map, key_prefix):
    widgets_dict = {}
    sorted_ids = sorted([cid for cid in original_map.keys() if cid != -1])
    for cluster_id in sorted_ids:
        orig_label = original_map.get(cluster_id, "")
        curr_label = current_map.get(cluster_id, orig_label)
        if orig_label == "(è©²å½“ãªã—)": continue
        col1, col2 = st.columns([2, 3])
        with col1: st.markdown(f":green[{orig_label}]")
        with col2:
            new_label = st.text_input(f"Edit {cluster_id}", value=curr_label, label_visibility="collapsed", key=f"{key_prefix}_{cluster_id}")
            widgets_dict[cluster_id] = new_label
    if -1 in original_map:
        orig_noise = original_map[-1]
        curr_noise = current_map.get(-1, orig_noise)
        col1, col2 = st.columns([2, 3])
        with col1: st.markdown(f":green[{orig_noise}]")
        with col2:
            st.text_input(f"noise_label", value=curr_noise, disabled=True, key=f"{key_prefix}_noise")
            widgets_dict[-1] = curr_noise
    return widgets_dict

def get_date_bin_options(df_filtered, interval_years, year_column='year'):
    if df_filtered is None or df_filtered.empty: return [f"(ãƒ‡ãƒ¼ã‚¿ãªã—)"]
    if year_column not in df_filtered.columns: return [f"(å…¨æœŸé–“) ({len(df_filtered)}ä»¶)"]
    
    df_filtered = df_filtered.copy()
    df_filtered[year_column] = pd.to_numeric(df_filtered[year_column], errors='coerce')
    if df_filtered[year_column].isnull().all(): return [f"(å…¨æœŸé–“) ({len(df_filtered)}ä»¶)"]

    try:
        min_year = int(df_filtered[year_column].min())
        max_year = int(df_filtered[year_column].max())
        if min_year == max_year: return [f"{min_year} ({len(df_filtered)}ä»¶)"]
        
        bins = list(range(min_year, max_year + interval_years, interval_years))
        if not bins: bins = [min_year]
        if bins[-1] <= max_year: bins.append(bins[-1] + interval_years)

        labels = [f"{bins[i]}-{bins[i+1] - 1}" for i in range(len(bins)-1)]
        df_filtered['temp_date_bin'] = pd.cut(df_filtered[year_column], bins=bins, labels=labels, right=False, include_lowest=True)
        date_bin_counts = df_filtered['temp_date_bin'].value_counts()
        
        options = [f"(å…¨æœŸé–“) ({len(df_filtered)}ä»¶)"] + [f"{label} ({date_bin_counts.get(label, 0)}ä»¶)" for label in labels if date_bin_counts.get(label, 0) > 0]
        return options
    except Exception as e:
        return [f"Error: {str(e)}"]

# ==================================================================
# --- 6. UIæ§‹æˆ ---
# ==================================================================

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ ---
with st.sidebar:
    st.title("APOLLO") 
    st.markdown("Advanced Patent & Overall Landscape-analytics Logic Orbiter")
    st.markdown("**v.3**")
    st.markdown("---")
    st.subheader("Home")
    st.page_link("Home.py", label="Mission Control", icon="ğŸ›°ï¸")
    st.subheader("Modules")
    st.page_link("pages/1_ğŸŒ_ATLAS.py", label="ATLAS", icon="ğŸŒ")
    st.page_link("pages/2_ğŸ’¡_CORE.py", label="CORE", icon="ğŸ’¡")
    st.page_link("pages/3_ğŸš€_Saturn_V.py", label="Saturn V", icon="ğŸš€")
    st.page_link("pages/4_ğŸ“ˆ_MEGA.py", label="MEGA", icon="ğŸ“ˆ")
    st.page_link("pages/5_ğŸ§­_Explorer.py", label="Explorer", icon="ğŸ§­")
    st.page_link("pages/6_ğŸ”—_CREW.py", label="CREW", icon="ğŸ”—")
    st.markdown("---")
    st.caption("ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³:\n1. Mission Control ã§ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€å‰å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\n2. ä¸Šã®ãƒªã‚¹ãƒˆã‹ã‚‰åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é¸æŠã—ã¾ã™ã€‚")
    st.markdown("---")
    st.caption("Â© 2025 ã—ã°ã‚„ã¾")

st.title("ğŸš€ Saturn V")
st.markdown("SBERTï¼ˆæ–‡è„ˆãƒ»æ„å‘³ï¼‰ã«åŸºã¥ãã€ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªæŠ€è¡“ãƒãƒƒãƒ—åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§ã™ã€‚")

col_theme, _ = st.columns([1, 3])
with col_theme:
    selected_theme = st.selectbox("è¡¨ç¤ºãƒ†ãƒ¼ãƒ:", ["APOLLO Standard", "Modern Presentation"], key="saturn_theme_selector")
theme_config = get_theme_config(selected_theme)
st.markdown(f"<style>{theme_config['css']}</style>", unsafe_allow_html=True)

# ==================================================================
# --- 7. ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ & åˆæœŸåŒ– ---
# ==================================================================
if not st.session_state.get("preprocess_done", False):
    st.error("åˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    st.warning("å…ˆã«ã€ŒMission Controlã€ï¼ˆãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ï¼‰ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€ã€Œåˆ†æã‚¨ãƒ³ã‚¸ãƒ³èµ·å‹•ã€ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    st.stop()
else:
    df_main = st.session_state.df_main
    col_map = st.session_state.col_map
    delimiters = st.session_state.delimiters
    sbert_embeddings = st.session_state.sbert_embeddings
    tfidf_matrix = st.session_state.tfidf_matrix
    feature_names = st.session_state.feature_names
    
if "saturnv_sbert_umap_done" not in st.session_state: st.session_state.saturnv_sbert_umap_done = False
if "saturnv_cluster_done" not in st.session_state: st.session_state.saturnv_cluster_done = False
if "saturnv_labels_map" not in st.session_state: st.session_state.saturnv_labels_map = {}
if "main_cluster_running" not in st.session_state: st.session_state.main_cluster_running = False
if "saturnv_global_zmax" not in st.session_state: st.session_state.saturnv_global_zmax = None

# ==================================================================
# --- 8. Saturn V ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ ---
# ==================================================================

# --- åˆå›UMAPè¨ˆç®— ---
if not st.session_state.saturnv_sbert_umap_done:
    with st.spinner("Saturn V ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åˆå›èµ·å‹•ä¸­: UMAPã«ã‚ˆã‚‹æ¬¡å…ƒå‰Šæ¸› (SBERTãƒ™ãƒ¼ã‚¹) ã‚’å®Ÿè¡Œã—ã¦ã„ã¾ã™..."):
        try:
            reducer = UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)
            embedding = reducer.fit_transform(sbert_embeddings) 
            st.session_state.df_main['umap_x'] = embedding[:, 0]
            st.session_state.df_main['umap_y'] = embedding[:, 1]
            st.session_state.df_main['characteristic_words'] = [get_top_tfidf_words(tfidf_matrix[i], feature_names) for i in range(tfidf_matrix.shape[0])]
            
            try:
                H, _, _ = np.histogram2d(embedding[:, 0], embedding[:, 1], bins=50)
                st.session_state.saturnv_global_zmax = H.max()
            except:
                st.session_state.saturnv_global_zmax = None
            
            st.session_state.saturnv_sbert_umap_done = True
            st.success("UMAPã®åˆæœŸè¨ˆç®—ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
            st.rerun()
        except Exception as e:
            st.error(f"UMAPã®åˆæœŸè¨ˆç®—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            st.stop()

# --- ãƒ¡ã‚¤ãƒ³UI ---
tab_main, tab_drill, tab_stats, tab_export = st.tabs([
    "Landscape Map (TELESCOPE)", 
    "Drilldown (PROBE)", 
    "ç‰¹è¨±ãƒãƒƒãƒ— (çµ±è¨ˆåˆ†æ)", 
    "Data Export"
])

# --- TELESCOPE ---
with tab_main:
    st.subheader("ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ")
    col1, col2, col3 = st.columns([2, 2, 1])
    with col1: min_cluster_size_w = st.number_input("æœ€å°ã‚¯ãƒ©ã‚¹ã‚¿ã‚µã‚¤ã‚º (æ¨å¥¨: 10-50):", min_value=2, value=15, key="main_min_cluster_size")
    with col2: min_samples_w = st.number_input("æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•° (æ¨å¥¨: 5-20):", min_value=1, value=10, key="main_min_samples")
    with col3: label_top_n_w = st.number_input("ã‚¯ãƒ©ã‚¹ã‚¿ãƒ©ãƒ™ãƒ«å˜èªæ•°:", min_value=1, value=3, key="main_label_top_n")
    
    if st.button("æç”» (å†è¨ˆç®—)", type="primary", key="main_run_cluster", disabled=st.session_state.main_cluster_running):
        st.session_state.main_cluster_running = True
        with st.spinner("HDBSCANã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œä¸­..."):
            try:
                embedding = st.session_state.df_main[['umap_x', 'umap_y']].values
                clusterer = hdbscan.HDBSCAN(min_cluster_size=int(min_cluster_size_w), min_samples=int(min_samples_w), metric='euclidean', cluster_selection_method='eom')
                clustering = clusterer.fit(embedding)
                st.session_state.df_main['cluster'] = clustering.labels_
                
                labels_map = {}
                label_top_n = int(label_top_n_w)
                unique_clusters = sorted(st.session_state.df_main['cluster'].unique())
                
                for cluster_id in unique_clusters:
                    if cluster_id == -1:
                        labels_map[cluster_id] = "ãƒã‚¤ã‚º / å°ã‚¯ãƒ©ã‚¹ã‚¿"
                        continue
                    indices = st.session_state.df_main[st.session_state.df_main['cluster'] == cluster_id].index
                    if len(indices) == 0:
                        labels_map[cluster_id] = "(è©²å½“ãªã—)"
                        continue
                    cluster_vectors = tfidf_matrix[indices]
                    mean_vector = np.array(cluster_vectors.mean(axis=0)).flatten()
                    top_indices = np.argsort(mean_vector)[::-1][:label_top_n]
                    label = ", ".join([feature_names[i] for i in top_indices])
                    labels_map[cluster_id] = f"[{cluster_id}] {label}"
                
                st.session_state.df_main['cluster_label'] = st.session_state.df_main['cluster'].map(labels_map)
                st.session_state.saturnv_labels_map = labels_map.copy()
                st.session_state.saturnv_labels_map_original = labels_map.copy()
                st.session_state.df_main = update_hover_text(st.session_state.df_main, col_map)
                st.session_state.saturnv_cluster_done = True
                st.success("ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Œäº†")
                st.rerun()
            except Exception as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
            finally:
                st.session_state.main_cluster_running = False

    st.markdown("---")
    
    if st.session_state.saturnv_cluster_done:
        st.subheader("ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è¨­å®š (ãƒ¡ã‚¤ãƒ³ç”¨)")
        def on_main_interval_change():
            if "main_date_filter" in st.session_state: del st.session_state.main_date_filter

        col1, col2 = st.columns(2)
        with col1:
            if 'year' in df_main.columns and df_main['year'].notna().any():
                bin_interval_w_val = st.selectbox("æœŸé–“ã®ç²’åº¦:", [5, 3, 2, 1], index=0, key="main_bin_interval", on_change=on_main_interval_change)
                date_bin_options = get_date_bin_options(df_main, int(bin_interval_w_val), 'year')
                date_bin_filter_w = st.selectbox("è¡¨ç¤ºæœŸé–“:", date_bin_options, key="main_date_filter")
            else:
                date_bin_filter_w = "(å…¨æœŸé–“)"
        
        with col2:
            if 'applicant_main' in st.session_state.df_main.columns:
                applicants = st.session_state.df_main['applicant_main'].explode().dropna()
            elif col_map['applicant'] and col_map['applicant'] in st.session_state.df_main.columns:
                applicants = st.session_state.df_main[col_map['applicant']].fillna('').str.split(delimiters['applicant']).explode().str.strip()
            else:
                applicants = pd.Series([])

            if not applicants.empty:
                applicant_counts = applicants.value_counts()
                unique_applicants = applicant_counts.index.tolist()
                applicant_options = [(f"(å…¨å‡ºé¡˜äºº) ({len(st.session_state.df_main)}ä»¶)", "ALL")] + \
                                    [(f"{app} ({applicant_counts[app]}ä»¶)", app) for app in unique_applicants]
                
                applicant_filter_w = st.multiselect(
                    "å‡ºé¡˜äºº:", 
                    applicant_options, 
                    default=[applicant_options[0]], 
                    format_func=lambda x: x[0], 
                    key="main_applicant_filter"
                )
            else:
                applicant_filter_w = [(f"(å…¨å‡ºé¡˜äºº) ({len(st.session_state.df_main)}ä»¶)", "ALL")]

        cluster_counts = st.session_state.df_main['cluster_label'].value_counts()
        cluster_options = [(f"(å…¨ã‚¯ãƒ©ã‚¹ã‚¿) ({len(st.session_state.df_main)}ä»¶)", "ALL")] + [
            (f"{st.session_state.saturnv_labels_map.get(cid)} ({cluster_counts.get(st.session_state.saturnv_labels_map.get(cid), 0)}ä»¶)", cid)
            for cid in sorted(st.session_state.df_main['cluster'].unique())
        ]
        cluster_filter_w = st.multiselect("ãƒãƒƒãƒ—è¡¨ç¤ºã‚¯ãƒ©ã‚¹ã‚¿:", cluster_options, default=[cluster_options[0]], format_func=lambda x: x[0], key="main_cluster_filter")

        st.subheader("åˆ†æçµæœ (TELESCOPE ãƒ¡ã‚¤ãƒ³ãƒãƒƒãƒ—)")
        
        # --- UIãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ ---
        map_mode = st.radio("è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰:", ["æ•£å¸ƒå›³ (Scatter)", "å¯†åº¦ãƒãƒƒãƒ— (Density)", "ã‚¯ãƒ©ã‚¹ã‚¿é ˜åŸŸ (Clusters)"], horizontal=True)
        c1, c2, c3 = st.columns(3)
        with c1:
            st.markdown("**å¯†åº¦ãƒãƒƒãƒ—è¨­å®š**")
            main_mesh_size = st.number_input("ãƒ¡ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚º (Grid)", value=30, min_value=10, max_value=200, step=5, key="main_mesh_size")
            use_abs_scale = False
            if map_mode == "å¯†åº¦ãƒãƒƒãƒ— (Density)":
                use_abs_scale = st.checkbox("å¯†åº¦ã‚¹ã‚±ãƒ¼ãƒ«ã‚’å›ºå®š (çµ¶å¯¾è©•ä¾¡)", value=False, key="main_abs_scale")
        with c2:
            st.markdown("**ãƒ•ã‚£ãƒ«ã‚¿**")
            remove_noise_chk = st.checkbox("ãƒã‚¤ã‚ºã‚’é™¤ã (Exclude Noise)", value=False, key="main_remove_noise")
        with c3:
            st.markdown("**è¡¨ç¤ºã‚ªãƒ—ã‚·ãƒ§ãƒ³**")
            show_labels_chk = st.checkbox("ãƒãƒƒãƒ—ã«ãƒ©ãƒ™ãƒ«ã‚’è¡¨ç¤ºã™ã‚‹", value=True, key="main_show_labels")
        
        # --- ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ»ãƒ¬ã‚¤ãƒ¤ãƒ¼æ§‹é€  ---
        
        # 1. Universe (ç¬¬1å±¤: èƒŒæ™¯/Ghostç”¨)
        df_universe = st.session_state.df_main.copy()
        if remove_noise_chk:
            df_universe = df_universe[df_universe['cluster'] != -1]

        # 2. Trend (ç¬¬2å±¤: åœ°å½¢ç”¨)
        df_trend = df_universe.copy()
        if not date_bin_filter_w.startswith("(å…¨æœŸé–“)"):
            try:
                date_bin_label = date_bin_filter_w.split(' (')[0].strip()
                start_year, end_year = map(int, date_bin_label.split('-'))
                df_trend = df_trend[(df_trend['year'] >= start_year) & (df_trend['year'] <= end_year)]
            except: pass

        # 3. Focus (ç¬¬3å±¤: æ³¨ç›®ç”¨)
        df_focus = df_trend.copy()
        
        # å‡ºé¡˜äººãƒ•ã‚£ãƒ«ã‚¿
        applicant_values = [val[1] for val in applicant_filter_w]
        if "ALL" not in applicant_values:
             mask_list = [df_focus[col_map['applicant']].fillna('').str.contains(re.escape(app)) for app in applicant_values]
             if mask_list:
                 df_focus = df_focus[pd.concat(mask_list, axis=1).any(axis=1)]
             else:
                 df_focus = df_focus.iloc[0:0]

        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ•ã‚£ãƒ«ã‚¿
        cluster_values = [val[1] for val in cluster_filter_w]
        if "ALL" not in cluster_values:
            df_focus = df_focus[df_focus['cluster'].isin(cluster_values)]

        # 4. Ghost (Universe - Focus)
        try:
            df_ghost = df_universe.drop(df_focus.index, errors='ignore')
        except:
            df_ghost = pd.DataFrame()

        # --- æç”»ãƒ­ã‚¸ãƒƒã‚¯ ---
        fig_main = go.Figure()
        
        # å¯†åº¦ãƒãƒƒãƒ—
        if not df_trend.empty and map_mode == "å¯†åº¦ãƒãƒƒãƒ— (Density)":
            custom_density_colorscale = [
                [0.0, "rgba(255, 255, 255, 0)"], 
                [0.1, "rgba(225, 245, 254, 0.3)"],
                [0.4, "rgba(129, 212, 250, 0.6)"],
                [1.0, "rgba(2, 119, 189, 0.9)"]
            ]
            
            contour_params = dict(
                x=df_trend['umap_x'], y=df_trend['umap_y'], 
                colorscale=custom_density_colorscale, 
                reversescale=False, xaxis='x', yaxis='y', 
                showscale=False, name="å¯†åº¦", 
                nbinsx=main_mesh_size, nbinsy=main_mesh_size,
                contours=dict(coloring='fill', showlines=True),
                line=dict(width=0.5, color='rgba(0, 0, 0, 0.2)')
            )
            if use_abs_scale and st.session_state.saturnv_global_zmax:
                contour_params.update(dict(zauto=False, zmin=0, zmax=st.session_state.saturnv_global_zmax))
            else: 
                contour_params.update(dict(zauto=True))
            
            fig_main.add_trace(go.Histogram2dContour(**contour_params))

        # ã‚¯ãƒ©ã‚¹ã‚¿é ˜åŸŸ
        if map_mode == "ã‚¯ãƒ©ã‚¹ã‚¿é ˜åŸŸ (Clusters)" and not df_universe.empty:
            unique_clusters = sorted(df_universe['cluster'].unique())
            color_sequence = theme_config["color_sequence"]
            for i, cid in enumerate(unique_clusters):
                if cid == -1: continue
                points = df_universe[df_universe['cluster'] == cid][['umap_x', 'umap_y']].values
                if len(points) >= 3:
                    try:
                        hull = ConvexHull(points)
                        hull_points = points[hull.vertices]
                        hull_points = np.append(hull_points, [hull_points[0]], axis=0)
                        cluster_color = color_sequence[i % len(color_sequence)]
                        fig_main.add_trace(go.Scatter(
                            x=hull_points[:, 0], y=hull_points[:, 1], mode='lines', fill='toself',
                            fillcolor=cluster_color, opacity=0.1, line=dict(color=cluster_color, width=2),
                            hoverinfo='skip', showlegend=False
                        ))
                    except: pass

        # Ghost (UniverseèƒŒæ™¯)
        if not df_ghost.empty:
            ghost_color = '#dddddd'
            ghost_opacity = 0.4
            fig_main.add_trace(go.Scattergl(
                x=df_ghost['umap_x'], y=df_ghost['umap_y'], mode='markers', 
                marker=dict(color=ghost_color, size=3, opacity=ghost_opacity, line=dict(width=0)), 
                hoverinfo='skip', name='ãã®ä»– (Ghost)'
            ))

        # Focus (æ³¨ç›®)
        if not df_focus.empty:
            marker_line = dict(width=1, color='white') if map_mode == "å¯†åº¦ãƒãƒƒãƒ— (Density)" else dict(width=0)
            is_applicant_filtered = "ALL" not in applicant_values
            
            if is_applicant_filtered:
                palette = px.colors.qualitative.Bold
                for i, app_name in enumerate(applicant_values):
                    mask = df_focus[col_map['applicant']].fillna('').str.contains(re.escape(app_name))
                    df_app = df_focus[mask]
                    if not df_app.empty:
                        fig_main.add_trace(go.Scattergl(
                            x=df_app['umap_x'], y=df_app['umap_y'], mode='markers',
                            marker=dict(color=palette[i % len(palette)], size=6, opacity=0.9, line=marker_line),
                            hoverinfo='text', hovertext=df_app['hover_text'], name=app_name
                        ))
            else:
                fig_main.add_trace(go.Scattergl(
                    x=df_focus['umap_x'], y=df_focus['umap_y'], mode='markers', 
                    marker=dict(color=df_focus['cluster'], colorscale=theme_config["color_sequence"], showscale=False, size=5, opacity=0.8, line=marker_line), 
                    hoverinfo='text', hovertext=df_focus['hover_text'], name='ç‰¹è¨±'
                ))

        # ãƒ©ãƒ™ãƒ«è¿½åŠ 
        if show_labels_chk:
            label_data_source = df_universe
            target_cids = cluster_values if "ALL" not in cluster_values else label_data_source['cluster'].unique()
            color_sequence = theme_config["color_sequence"]
            sorted_unique_cids = sorted(df_universe['cluster'].unique()) 

            for cid, grp in label_data_source[label_data_source['cluster'].isin(target_cids)].groupby('cluster'):
                if cid == -1: continue
                mean_pos = grp[['umap_x', 'umap_y']].mean()
                label_txt = grp['cluster_label'].iloc[0]
                try:
                    color_idx = sorted_unique_cids.index(cid)
                    border_color = color_sequence[color_idx % len(color_sequence)]
                except: border_color = "#333333"

                fig_main.add_annotation(
                    x=mean_pos['umap_x'], y=mean_pos['umap_y'], 
                    text=label_txt, showarrow=False, 
                    font=dict(size=11, color='black', family="Helvetica"), 
                    bgcolor='rgba(255,255,255,0.8)', bordercolor=border_color, borderwidth=2, borderpad=4
                )

        norm_msg = " (çµ¶å¯¾è©•ä¾¡)" if use_abs_scale and map_mode == "å¯†åº¦ãƒãƒƒãƒ— (Density)" else ""
        update_fig_layout(fig_main, f"Saturn V - ãƒ¡ã‚¤ãƒ³ãƒãƒƒãƒ—{norm_msg}", height=1000, theme_config=theme_config)
        st.plotly_chart(fig_main, use_container_width=True)

        st.subheader("ãƒ©ãƒ™ãƒ«ç·¨é›†")
        if "saturnv_labels_map_original" not in st.session_state: st.session_state.saturnv_labels_map_original = st.session_state.saturnv_labels_map.copy()
        st.session_state.saturnv_labels_map_custom = _create_label_editor_ui(st.session_state.saturnv_labels_map_original, st.session_state.saturnv_labels_map, "main_label")
        if st.button("ãƒ©ãƒ™ãƒ«ã‚’æ›´æ–°", key="main_update_labels"):
            st.session_state.df_main['cluster_label'] = st.session_state.df_main['cluster'].map(st.session_state.saturnv_labels_map_custom)
            st.session_state.df_main = update_hover_text(st.session_state.df_main, col_map)
            st.session_state.saturnv_labels_map = st.session_state.saturnv_labels_map_custom
            st.rerun()

    # --- PROBE (ãƒ‰ãƒªãƒ«ãƒ€ã‚¦ãƒ³) ---
    with tab_drill:
        st.subheader("åˆ†æå¯¾è±¡ã‚¯ãƒ©ã‚¹ã‚¿ã®é¸æŠ")
        drilldown_options = [('(é¸æŠã—ã¦ãã ã•ã„)', 'NONE')]
        if "saturnv_labels_map" in st.session_state:
            drilldown_options += [(f"{label} ({count}ä»¶)", cid) for cid, label in st.session_state.saturnv_labels_map.items() if cid != -1 for count in [st.session_state.df_main['cluster'].value_counts().get(cid, 0)]]
        
        selected_drilldown_target_drill = st.selectbox("åˆ†æå¯¾è±¡ã‚¯ãƒ©ã‚¹ã‚¿:", options=drilldown_options, format_func=lambda x: x[0], key="drill_target_select")
        drilldown_target_id = selected_drilldown_target_drill[1] 

        st.subheader("ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è¨­å®š (ãƒ‰ãƒªãƒ«ãƒ€ã‚¦ãƒ³ç”¨)")
        if drilldown_target_id == "NONE":
            df_subset_filter = pd.DataFrame(columns=df_main.columns)
            st.info("ğŸ‘† ä¸Šã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã§ã€Œåˆ†æå¯¾è±¡ã‚¯ãƒ©ã‚¹ã‚¿ã€ã‚’é¸æŠã™ã‚‹ã¨ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¡ãƒ‹ãƒ¥ãƒ¼ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")
        else:
            df_subset_filter = df_main[df_main['cluster'] == drilldown_target_id].copy()
            
        def on_drill_interval_change():
            if "drill_date_filter_w" in st.session_state: del st.session_state.drill_date_filter_w
            
        if drilldown_target_id != "NONE":
            col1, col2 = st.columns(2)
            with col1:
                if 'year' in df_subset_filter.columns and df_subset_filter['year'].notna().any():
                    drill_bin_interval_w_val = st.selectbox("æœŸé–“ã®ç²’åº¦:", [5, 3, 2, 1], index=0, key="drill_interval_w", on_change=on_drill_interval_change)
                    drill_date_bin_options = get_date_bin_options(df_subset_filter, int(drill_bin_interval_w_val), 'year')
                    drill_date_bin_filter_w = st.selectbox("è¡¨ç¤ºæœŸé–“:", drill_date_bin_options, key="drill_date_filter_w")
                else:
                    drill_date_bin_filter_w = "(å…¨æœŸé–“)"
            with col2:
                if 'applicant_main' in df_subset_filter.columns:
                    applicants_drill = df_subset_filter['applicant_main'].explode().dropna()
                elif col_map['applicant'] and col_map['applicant'] in df_subset_filter.columns:
                    applicants_drill = df_subset_filter[col_map['applicant']].fillna('').str.split(delimiters['applicant']).explode().str.strip()
                else:
                    applicants_drill = pd.Series([])

                if not applicants_drill.empty:
                    app_counts_drill = applicants_drill.value_counts()
                    unique_applicants_drill = app_counts_drill.index.tolist()
                    drill_applicant_options = [(f"(å…¨å‡ºé¡˜äºº) ({len(df_subset_filter)}ä»¶)", "ALL")] + \
                                              [(f"{app} ({app_counts_drill[app]}ä»¶)", app) for app in unique_applicants_drill]
                    
                    drill_applicant_filter_w = st.multiselect(
                        "å‡ºé¡˜äºº:", 
                        drill_applicant_options, 
                        default=[drill_applicant_options[0]], 
                        format_func=lambda x: x[0], 
                        key="drill_applicant_filter_w"
                    )
                else:
                    drill_applicant_filter_w = [(f"(å…¨å‡ºé¡˜äºº) ({len(df_subset_filter)}ä»¶)", "ALL")]

        st.subheader("ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°è¨­å®š (ãƒ‰ãƒªãƒ«ãƒ€ã‚¦ãƒ³ç”¨)")
        col1, col2, col3 = st.columns(3)
        with col1: drill_min_cluster_size_w = st.number_input('æœ€å°ã‚¯ãƒ©ã‚¹ã‚¿ã‚µã‚¤ã‚º:', min_value=2, value=5, key="drill_min_cluster_size_w")
        with col2: drill_min_samples_w = st.number_input('æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°:', min_value=1, value=5, key="drill_min_samples_w")
        with col3: drill_label_top_n_w = st.number_input('ãƒ©ãƒ™ãƒ«å˜èªæ•°:', min_value=1, value=3, key="drill_label_top_n_w")
        drill_show_labels_chk = st.checkbox('ãƒãƒƒãƒ—ã«ãƒ©ãƒ™ãƒ«ã‚’è¡¨ç¤ºã™ã‚‹', value=True, key="drill_show_labels_chk")

        if st.button("é¸æŠã‚¯ãƒ©ã‚¹ã‚¿ã§å†ãƒãƒƒãƒ—", type="primary", key="drill_run_button"):
            if drilldown_target_id == "NONE":
                st.error("ã‚¨ãƒ©ãƒ¼: åˆ†æå¯¾è±¡ã‚¯ãƒ©ã‚¹ã‚¿ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
            else:
                with st.spinner(f"ã‚¯ãƒ©ã‚¹ã‚¿ {drilldown_target_id} ã®ãƒ‰ãƒªãƒ«ãƒ€ã‚¦ãƒ³ã‚’å®Ÿè¡Œä¸­..."):
                    try:
                        df_subset = df_main[df_main['cluster'] == drilldown_target_id].copy()
                        base_label = df_subset['cluster_label'].iloc[0]
                        
                        if not drill_date_bin_filter_w.startswith("(å…¨æœŸé–“)"):
                            try:
                                date_bin_label = drill_date_bin_filter_w.split(' (')[0].strip() 
                                start_year, end_year = map(int, date_bin_label.split('-'))
                                df_subset = df_subset[(df_subset['year'] >= start_year) & (df_subset['year'] <= end_year)]
                            except: pass 

                        # ãƒ‰ãƒªãƒ«ãƒ€ã‚¦ãƒ³ã§ã¯çµã‚Šè¾¼ã¿å†è¨ˆç®—ãŒä¸»ç›®çš„ãªã®ã§ã€Applicantãƒ•ã‚£ãƒ«ã‚¿ã¯ãƒ‡ãƒ¼ã‚¿å‰Šæ¸›ã¨ã—ã¦æ‰±ã†
                        drill_app_values = [val[1] for val in drill_applicant_filter_w]
                        if "ALL" not in drill_app_values:
                            mask_list_drill = [df_subset[col_map['applicant']].fillna('').str.contains(re.escape(app)) for app in drill_app_values]
                            df_subset = df_subset[pd.concat(mask_list_drill, axis=1).any(axis=1)]
                        
                        if len(df_subset) < 10:
                            st.warning(f"ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã™ãã¾ã™ ({len(df_subset)}ä»¶)ã€‚")
                        else:
                            subset_indices = df_subset.index
                            subset_tfidf = tfidf_matrix[subset_indices]
                            subset_sbert = sbert_embeddings[subset_indices]
                            subset_indices_pd = pd.Index(subset_indices)

                            n_neighbors = min(10, len(df_subset) - 1)
                            if n_neighbors < 2: n_neighbors = 2
                            
                            reducer_drill = UMAP(n_neighbors=n_neighbors, min_dist=0.1, n_components=2, random_state=42)
                            embedding_drill = reducer_drill.fit_transform(subset_sbert) 
                            df_subset['drill_x'] = embedding_drill[:, 0]
                            df_subset['drill_y'] = embedding_drill[:, 1]
                            
                            clusterer_drill = hdbscan.HDBSCAN(min_cluster_size=int(drill_min_cluster_size_w), min_samples=int(drill_min_samples_w), metric='euclidean', cluster_selection_method='eom')
                            df_subset['drill_cluster'] = clusterer_drill.fit_predict(embedding_drill)
                            
                            drill_labels_map = {}
                            for cid in sorted(df_subset['drill_cluster'].unique()):
                                if cid == -1:
                                    drill_labels_map[cid] = "ãƒã‚¤ã‚º"
                                    continue
                                idxs = df_subset[df_subset['drill_cluster'] == cid].index
                                tfidf_pos = [subset_indices_pd.get_loc(i) for i in idxs if i in subset_indices_pd]
                                if tfidf_pos:
                                    mean_vec = np.array(subset_tfidf[tfidf_pos].mean(axis=0)).flatten()
                                    top_idx = np.argsort(mean_vec)[::-1][:int(drill_label_top_n_w)]
                                    label = ", ".join([feature_names[i] for i in top_idx])
                                    drill_labels_map[cid] = f"[{cid}] {label}"
                            
                            df_subset['drill_cluster_label'] = df_subset['drill_cluster'].map(drill_labels_map)
                            df_subset = update_drill_hover_text(df_subset)
                            st.session_state.df_drilldown_result = df_subset.copy()
                            st.session_state.drill_labels_map = drill_labels_map.copy()
                            st.session_state.drill_labels_map_original = drill_labels_map.copy()
                            st.session_state.drill_base_label = base_label
                            st.success("å®Œäº†ã—ã¾ã—ãŸã€‚")
                            st.rerun()

                    except Exception as e:
                        st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

        if "df_drilldown_result" in st.session_state:
            df_drill = st.session_state.df_drilldown_result.copy()
            drill_labels_map = st.session_state.drill_labels_map
            
            st.subheader("ã‚µãƒ–ã‚¯ãƒ©ã‚¹ã‚¿ãƒ»ãƒ©ãƒ™ãƒ«ç·¨é›†")
            if "drill_labels_map_original" not in st.session_state:
                 st.session_state.drill_labels_map_original = drill_labels_map.copy()
            drill_label_widgets = _create_label_editor_ui(st.session_state.drill_labels_map_original, st.session_state.drill_labels_map, "drill_label")
            if st.button("ã‚µãƒ–ã‚¯ãƒ©ã‚¹ã‚¿ãƒ»ãƒ©ãƒ™ãƒ«ã‚’æ›´æ–°", key="drill_update_labels"):
                for cid, val in drill_label_widgets.items(): drill_labels_map[cid] = val
                df_drill['drill_cluster_label'] = df_drill['drill_cluster'].map(drill_labels_map)
                st.session_state.df_drilldown_result = update_drill_hover_text(df_drill)
                st.session_state.drill_labels_map = drill_labels_map
                st.rerun()

            st.subheader("ãƒ‰ãƒªãƒ«ãƒ€ã‚¦ãƒ³ãƒãƒƒãƒ—")
            
            # --- UIãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ ---
            drill_map_mode = st.radio("è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰:", ["æ•£å¸ƒå›³ (Scatter)", "å¯†åº¦ãƒãƒƒãƒ— (Density)", "ã‚¯ãƒ©ã‚¹ã‚¿é ˜åŸŸ (Clusters)"], horizontal=True, key="drill_map_mode_radio")
            
            d_c1, d_c2, d_c3 = st.columns(3)
            with d_c1:
                st.markdown("**å¯†åº¦ãƒãƒƒãƒ—è¨­å®š**")
                drill_mesh_size = st.number_input("ãƒ¡ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚º (Grid)", value=40, min_value=10, max_value=200, step=5, key="drill_mesh_size")
            with d_c2:
                st.markdown("**ãƒ•ã‚£ãƒ«ã‚¿**")
                drill_remove_noise_chk = st.checkbox("ãƒã‚¤ã‚ºã‚’é™¤ã (Exclude Noise)", value=False, key="drill_remove_noise")
            with d_c3:
                st.empty()

            if drill_remove_noise_chk:
                df_drill_plot = df_drill[df_drill['drill_cluster'] != -1]
            else:
                df_drill_plot = df_drill

            fig_drill = go.Figure()
            
            custom_density_colorscale_d = [
                [0.0, "rgba(255, 255, 255, 0)"], 
                [0.1, "rgba(225, 245, 254, 0.3)"],
                [0.4, "rgba(129, 212, 250, 0.6)"],
                [1.0, "rgba(2, 119, 189, 0.9)"]
            ]

            if drill_map_mode == "å¯†åº¦ãƒãƒƒãƒ— (Density)":
                contour_d = dict(
                    x=df_drill_plot['drill_x'], y=df_drill_plot['drill_y'], 
                    colorscale=custom_density_colorscale_d, 
                    reversescale=False, xaxis='x', yaxis='y', showscale=False, name="å¯†åº¦", 
                    nbinsx=drill_mesh_size, nbinsy=drill_mesh_size, 
                    contours=dict(coloring='fill', showlines=True),
                    line=dict(width=0.5, color='rgba(0, 0, 0, 0.2)')
                )
                fig_drill.add_trace(go.Histogram2dContour(**contour_d))
                
            if drill_map_mode == "ã‚¯ãƒ©ã‚¹ã‚¿é ˜åŸŸ (Clusters)":
                color_sequence = theme_config["color_sequence"]
                unique_clusters_d = sorted(df_drill_plot['drill_cluster'].unique())
                for i, cid in enumerate(unique_clusters_d):
                    if cid == -1: continue
                    points = df_drill_plot[df_drill_plot['drill_cluster'] == cid][['drill_x', 'drill_y']].values
                    if len(points) >= 3:
                        try:
                            hull = ConvexHull(points)
                            hull_points = points[hull.vertices]
                            hull_points = np.append(hull_points, [hull_points[0]], axis=0)
                            c_color = color_sequence[i % len(color_sequence)]
                            fig_drill.add_trace(go.Scatter(
                                x=hull_points[:, 0], y=hull_points[:, 1], mode='lines', fill='toself',
                                fillcolor=c_color, opacity=0.1, line=dict(color=c_color, width=2),
                                hoverinfo='skip', showlegend=False
                            ))
                        except: pass

            marker_line_d = dict(width=1, color='white') if drill_map_mode == "å¯†åº¦ãƒãƒƒãƒ— (Density)" else dict(width=0)
            fig_drill.add_trace(go.Scattergl(
                x=df_drill_plot['drill_x'], y=df_drill_plot['drill_y'], mode='markers',
                marker=dict(color=df_drill_plot['drill_cluster'], colorscale=theme_config["color_sequence"] if isinstance(theme_config["color_sequence"], str) else 'turbo', showscale=False, size=5, opacity=0.8, line=marker_line_d),
                hoverinfo='text', hovertext=df_drill_plot['drill_hover_text'], name='è¡¨ç¤ºå¯¾è±¡'
            ))
            
            annotations_drill = []
            if drill_show_labels_chk:
                color_sequence = theme_config["color_sequence"]
                sorted_unique_cids_d = sorted(df_drill_plot['drill_cluster'].unique())
                
                for cid, grp in df_drill_plot[df_drill_plot['drill_cluster'] != -1].groupby('drill_cluster'):
                    mean_pos = grp[['drill_x', 'drill_y']].mean()
                    
                    try:
                        color_idx = sorted_unique_cids_d.index(cid)
                        border_color = color_sequence[color_idx % len(color_sequence)]
                    except:
                        border_color = "#333333"

                    annotations_drill.append(go.layout.Annotation(
                        x=mean_pos['drill_x'], y=mean_pos['drill_y'], text=drill_labels_map.get(cid, ""), showarrow=False, 
                        font=dict(size=10, color='black', family="Helvetica"), 
                        bgcolor='rgba(255,255,255,0.8)',
                        bordercolor=border_color,
                        borderwidth=2,
                        borderpad=4
                    ))
            fig_drill.update_layout(annotations=annotations_drill)
            update_fig_layout(fig_drill, f'Saturn V ãƒ‰ãƒªãƒ«ãƒ€ã‚¦ãƒ³: {st.session_state.drill_base_label}', height=1000, theme_config=theme_config)
            st.plotly_chart(fig_drill, use_container_width=True)
            
            # --- ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚° ---
            st.markdown("---")
            st.subheader("ã‚¯ãƒ©ã‚¹ã‚¿ãƒ»ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ (Text Mining)")
            col_tm1, col_tm2 = st.columns(2)
            with col_tm1:
                cooc_top_n = st.slider("å…±èµ·: ä¸Šä½å˜èªæ•°", 30, 100, 50, key="cooc_top_n")
                cooc_threshold = st.slider("å…±èµ·: Jaccardä¿‚æ•° é–¾å€¤", 0.01, 0.3, 0.05, 0.01, key="cooc_threshold")
            
            if st.button("ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã‚’å®Ÿè¡Œ", key="run_text_mining"):
                with st.spinner("åˆ†æä¸­..."):
                    all_text = ""
                    for _, row in df_drill.iterrows():
                        if col_map['title'] and pd.notna(row[col_map['title']]): all_text += row[col_map['title']] + " "
                        if col_map['abstract'] and pd.notna(row[col_map['abstract']]): all_text += row[col_map['abstract']] + " "
                    words = extract_compound_nouns(all_text)
                    
                    if not words: st.warning("æœ‰åŠ¹ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãªã—")
                    else:
                        st.markdown("##### 1. ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰")
                        generate_wordcloud_and_list(words, f"ã‚¯ãƒ©ã‚¹ã‚¿: {st.session_state.drill_base_label}", 30, FONT_PATH)
                        
                        st.markdown("##### 2. å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯")
                        word_freq = Counter(words)
                        top_words = [w for w, c in word_freq.most_common(cooc_top_n)]
                        pair_counts = Counter()
                        for _, row in df_drill.iterrows():
                            dt = ""
                            if col_map['title']: dt += str(row[col_map['title']]) + " "
                            if col_map['abstract']: dt += str(row[col_map['abstract']]) + " "
                            dw = set(extract_compound_nouns(dt))
                            dw = {w for w in dw if w in top_words}
                            if len(dw) >= 2:
                                for pair in combinations(sorted(list(dw)), 2): pair_counts[pair] += 1
                        
                        G = nx.Graph()
                        for w in top_words: G.add_node(w, count=word_freq[w])
                        for (w1, w2), c in pair_counts.items():
                            jac = c / (word_freq[w1] + word_freq[w2] - c)
                            if jac >= cooc_threshold: G.add_edge(w1, w2, weight=jac)
                        
                        G.remove_nodes_from(list(nx.isolates(G)))
                        if G.number_of_nodes() == 0: st.warning("å…±èµ·ãƒšã‚¢ãªã—")
                        else:
                            pos = nx.spring_layout(G, k=0.5, seed=42)
                            edge_x, edge_y = [], []
                            for edge in G.edges():
                                x0, y0 = pos[edge[0]]; x1, y1 = pos[edge[1]]
                                edge_x.extend([x0, x1, None]); edge_y.extend([y0, y1, None])
                            edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(width=0.5, color='#888'), hoverinfo='none', mode='lines')
                            
                            node_x, node_y, node_text, node_size = [], [], [], []
                            for node in G.nodes():
                                x, y = pos[node]; node_x.append(x); node_y.append(y)
                                c = G.nodes[node]['count']
                                node_text.append(f"{node} ({c})")
                                node_size.append(np.log(c+1)*10)
                            
                            node_trace = go.Scatter(
                                x=node_x, y=node_y, mode='markers+text', hoverinfo='text', text=list(G.nodes()), textposition="top center",
                                marker=dict(showscale=True, colorscale='YlGnBu', size=node_size, color=node_size, line_width=2)
                            )
                            fig_net = go.Figure(data=[edge_trace, node_trace], layout=go.Layout(title='å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯', showlegend=False, hovermode='closest', margin=dict(b=20,l=5,r=5,t=40), xaxis=dict(showgrid=False, zeroline=False, showticklabels=False), yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))
                            update_fig_layout(fig_net, 'å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯', theme_config=theme_config, show_axes=False)
                            fig_net.update_xaxes(visible=False)
                            fig_net.update_yaxes(visible=False)
                            st.plotly_chart(fig_net, use_container_width=True)

    # --- C. ç‰¹è¨±ãƒãƒƒãƒ— (çµ±è¨ˆåˆ†æ) ---
    with tab_stats:
        st.subheader("ç‰¹è¨±ãƒãƒƒãƒ—ï¼ˆçµ±è¨ˆåˆ†æï¼‰")
        if st.session_state.saturnv_cluster_done:
            cluster_counts_stats = st.session_state.df_main['cluster_label'].value_counts()
            cluster_options_stats = [(f"(å…¨ã‚¯ãƒ©ã‚¹ã‚¿) ({len(st.session_state.df_main)}ä»¶)", "ALL")] + [
                (f"{st.session_state.saturnv_labels_map.get(cid)} ({cluster_counts_stats.get(st.session_state.saturnv_labels_map.get(cid), 0)}ä»¶)", cid)
                for cid in sorted(st.session_state.df_main['cluster'].unique())
            ]
            stats_cluster_filter_w = st.multiselect("é›†è¨ˆå¯¾è±¡ã‚¯ãƒ©ã‚¹ã‚¿:", cluster_options_stats, default=[cluster_options_stats[0]], format_func=lambda x: x[0], key="stats_cluster_filter")
            
            c1, c2 = st.columns(2)
            with c1:
                auto_min_year = 2000
                auto_max_year = datetime.datetime.now().year
                if 'year' in st.session_state.df_main.columns:
                    try:
                        valid_years = st.session_state.df_main['year'].dropna()
                        if not valid_years.empty:
                            auto_min_year = int(valid_years.min())
                            auto_max_year = int(valid_years.max())
                    except:
                        pass

                if 'stats_start_year' not in st.session_state: st.session_state.stats_start_year = auto_min_year
                if 'stats_end_year' not in st.session_state: st.session_state.stats_end_year = auto_max_year
                
                s_year = st.number_input('é–‹å§‹å¹´:', min_value=1900, max_value=2100, key="stats_start_year", step=1)
                e_year = st.number_input('çµ‚äº†å¹´:', min_value=1900, max_value=2100, key="stats_end_year", step=1)
            with c2:
                n_apps = st.number_input('è¡¨ç¤ºäººæ•°:', min_value=1, value=15, key="stats_num_assignees")
            
            if st.button("ç‰¹è¨±ãƒãƒƒãƒ—ã‚’æç”»", key="stats_run_button"):
                df_s = st.session_state.df_main.copy()
                vals = [v[1] for v in stats_cluster_filter_w]
                if "ALL" not in vals: df_s = df_s[df_s['cluster'].isin(vals)]
                df_s = df_s[(df_s['year'] >= s_year) & (df_s['year'] <= e_year)]
                
                if df_s.empty: st.warning("ãƒ‡ãƒ¼ã‚¿ãªã—")
                else:
                    # 1. æ™‚ç³»åˆ—
                    yc = df_s['year'].value_counts().sort_index().reindex(range(s_year, e_year+1), fill_value=0)
                    fig1 = px.bar(x=yc.index, y=yc.values, labels={'x':'å¹´', 'y':'ä»¶æ•°'}, color_discrete_sequence=[theme_config["color_sequence"][0]])
                    update_fig_layout(fig1, 'å‡ºé¡˜æ¨ç§»', theme_config=theme_config, show_axes=True)
                    st.plotly_chart(fig1, use_container_width=True)
                    
                    # 2. ãƒ©ãƒ³ã‚­ãƒ³ã‚°
                    ac = df_s['applicant_main'].explode().value_counts().head(n_apps).sort_values(ascending=True)
                    fig2 = px.bar(x=ac.values, y=ac.index, orientation='h', labels={'x':'ä»¶æ•°', 'y':'å‡ºé¡˜äºº'}, color_discrete_sequence=[theme_config["color_sequence"][1]])
                    update_fig_layout(fig2, 'å‡ºé¡˜äººãƒ©ãƒ³ã‚­ãƒ³ã‚°', height=max(600, len(ac)*30), theme_config=theme_config, show_axes=True)
                    st.plotly_chart(fig2, use_container_width=True)
                    
                    # 3. ãƒãƒ–ãƒ«
                    ae = df_s.explode('applicant_main')
                    ae['ap'] = ae['applicant_main'].astype(str).str.strip()
                    top_a = ae['ap'].value_counts().head(n_apps).index.tolist()
                    pd_plot = ae[ae['ap'].isin(top_a)].groupby(['year', 'ap']).size().reset_index(name='count')
                    
                    if not pd_plot.empty:
                        fig3 = px.scatter(pd_plot, x='year', y='ap', size='count', color='ap', labels={'year':'å‡ºé¡˜å¹´', 'ap':'å‡ºé¡˜äºº', 'count':'ä»¶æ•°'}, category_orders={'ap': top_a})
                        update_fig_layout(fig3, 'å‡ºé¡˜å¹´åˆ¥å‹•å‘', height=700, theme_config=theme_config, show_axes=True)
                        fig3.update_layout(
                            legend=dict(
                                orientation="v", 
                                yanchor="top", y=1, 
                                xanchor="left", x=1.02, 
                                borderwidth=0
                            )
                        )
                        st.plotly_chart(fig3, use_container_width=True)

    # --- D. ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ ---
    with tab_export:
        st.subheader("ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
        if st.session_state.saturnv_cluster_done:
            cols_drop = ['hover_text', 'parsed_date', 'drill_cluster', 'drill_cluster_label', 'drill_hover_text', 'drill_x', 'drill_y', 'temp_date_bin']
            csv = df_main.drop(columns=cols_drop, errors='ignore').to_csv(encoding='utf-8-sig', index=False).encode('utf-8-sig')
            st.download_button("ãƒ¡ã‚¤ãƒ³ãƒãƒƒãƒ—å…¨ãƒ‡ãƒ¼ã‚¿ (CSV)", csv, "APOLLO_SaturnV_Main.csv", "text/csv")
        
        if "df_drilldown_result" in st.session_state:
            cols_drop_d = ['hover_text', 'parsed_date', 'date_bin', 'drill_hover_text', 'drill_date_bin', 'temp_date_bin']
            csv_d = st.session_state.df_drilldown_result.drop(columns=cols_drop_d, errors='ignore').to_csv(encoding='utf-8-sig', index=False).encode('utf-8-sig')
            st.download_button("ãƒ‰ãƒªãƒ«ãƒ€ã‚¦ãƒ³çµæœ (CSV)", csv_d, "APOLLO_SaturnV_Drill.csv", "text/csv")