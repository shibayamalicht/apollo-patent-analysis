import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
import datetime
import warnings
import io
import unicodedata
import re
import platform
import os
import string
from collections import Counter
from itertools import combinations

# æ©Ÿæ¢°å­¦ç¿’ãƒ»è‡ªç„¶è¨€èªå‡¦ç†
from umap import UMAP 
import hdbscan 
from wordcloud import WordCloud
from janome.tokenizer import Tokenizer
import networkx as nx

# æç”»è¨­å®š
import matplotlib.pyplot as plt
import matplotlib.font_manager
import japanize_matplotlib

# è­¦å‘Šã‚’éè¡¨ç¤º
warnings.filterwarnings('ignore')

# ==================================================================
# --- 1. ãƒ‡ã‚¶ã‚¤ãƒ³ãƒ†ãƒ¼ãƒç®¡ç† ---
# ==================================================================

def get_theme_config(theme_name):
    """é¸æŠã•ã‚ŒãŸãƒ†ãƒ¼ãƒã«å¿œã˜ãŸCSSã¨Plotlyãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆè¨­å®šã‚’è¿”ã™"""
    themes = {
        "APOLLO Standard": {
            "bg_color": "#ffffff",
            "text_color": "#333333",
            "sidebar_bg": "#f8f9fa",
            "plotly_template": "plotly_white",
            "color_sequence": px.colors.qualitative.G10,
            "density_scale": "Blues",
            "accent_color": "#003366",
            "css": """
                html, body { background-color: #ffffff; color: #333333; }
                [data-testid="stSidebar"] { background-color: #f8f9fa; }
                [data-testid="stHeader"] { background-color: #ffffff; }
                h1, h2, h3 { color: #003366; }
            """
        },
        "Modern Presentation": {
            "bg_color": "#fdfdfd",
            "text_color": "#2c3e50",
            "sidebar_bg": "#eaeaea",
            "plotly_template": "plotly_white",
            "color_sequence": ["#264653", "#2a9d8f", "#e9c46a", "#f4a261", "#e76f51", "#8ab17d"],
            "density_scale": "Teal",
            "accent_color": "#264653",
            "css": """
                html, body { background-color: #fdfdfd; color: #2c3e50; font-family: "Helvetica Neue", Arial, sans-serif; }
                [data-testid="stSidebar"] { background-color: #eaeaea; }
                [data-testid="stHeader"] { background-color: #fdfdfd; }
                h1, h2, h3 { color: #264653; font-family: "Georgia", serif; }
                .stButton>button { background-color: #264653; color: white; border-radius: 0px; }
            """
        }
    }
    return themes.get(theme_name, themes["APOLLO Standard"])

def update_fig_layout(fig, title, height=800, theme_config=None):
    """Plotlyã‚°ãƒ©ãƒ•ã®å…±é€šãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¨­å®š"""
    if theme_config is None:
        return fig
    fig.update_layout(
        template=theme_config["plotly_template"],
        title=title,
        paper_bgcolor=theme_config["bg_color"],
        plot_bgcolor=theme_config["bg_color"],
        font_color=theme_config["text_color"],
        height=height
    )
    return fig

# ==================================================================
# --- 2. ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†é–¢æ•° ---
# ==================================================================

@st.cache_resource
def load_tokenizer_saturn():
    return Tokenizer()

t = load_tokenizer_saturn()

# ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰å®šç¾©
_stopwords_original_list = [
    "ã™ã‚‹","ã‚ã‚‹","ãªã‚‹","ãŸã‚","ã“ã¨","ã‚ˆã†","ã‚‚ã®","ã“ã‚Œ","ãã‚Œ","ã‚ã‚Œ","ã“ã“","ãã“","ã©ã‚Œ","ã©ã®",
    "ã“ã®","ãã®","å½“è©²","è©²","ãŠã‚ˆã³","åŠã³","ã¾ãŸã¯","ã¾ãŸ","ä¾‹ãˆã°","ä¾‹ãˆã°ã¯","ã«ãŠã„ã¦","ã«ã‚ˆã‚Š",
    "ã«å¯¾ã—ã¦","ã«é–¢ã—ã¦","ã«ã¤ã„ã¦","ã¨ã—ã¦","ã¨ã—ã¦ã¯","å ´åˆ","ä¸€æ–¹","ä»–æ–¹","ã•ã‚‰ã«","ãã—ã¦","ãŸã ã—",
    "ãªãŠ","ç­‰","ãªã©","ç­‰ã€…","ã„ã‚ã‚†ã‚‹","æ‰€è¬‚","åŒæ§˜","åŒæ™‚","å‰è¨˜","æœ¬","åŒ","å„","å„ç¨®","æ‰€å®š","æ‰€æœ›",
    "ä¸€ä¾‹","ä»–","ä¸€éƒ¨","ä¸€ã¤","è¤‡æ•°","å°‘ãªãã¨ã‚‚","å°‘ãªãã¨ã‚‚ä¸€ã¤","ä¸Šè¨˜","ä¸‹è¨˜","å‰è¿°","å¾Œè¿°","æ—¢è¿°",
    "é–¢ã™ã‚‹","åŸºã¥ã","ç”¨ã„ã‚‹","ä½¿ç”¨","åˆ©ç”¨","æœ‰ã™ã‚‹","å«ã‚€","å‚™ãˆã‚‹","è¨­ã‘ã‚‹","ã™ãªã‚ã¡","å¾“ã£ã¦",
    "ã—ã‹ã—ãªãŒã‚‰","æ¬¡ã«","ç‰¹ã«","å…·ä½“çš„ã«","è©³ç´°ã«","ã„ãšã‚Œ","ã†ã¡","ãã‚Œãã‚Œ","ã¨ã",
    "ã‹ã‹ã‚‹","ã‹ã‚ˆã†ãª","ã‹ã‹ã‚‹å ´åˆ","æœ¬ä»¶","æœ¬é¡˜","æœ¬å‡ºé¡˜","æœ¬æ˜ç´°æ›¸","ã“ã‚Œã‚‰","ãã‚Œã‚‰","å„ã€…","éšæ™‚","é©å®œ",
    "ä»»æ„","å¿…ãšã—ã‚‚","é€šå¸¸","ä¸€èˆ¬ã«","å…¸å‹çš„","ä»£è¡¨çš„",
    "æœ¬ç™ºæ˜","ç™ºæ˜","å®Ÿæ–½ä¾‹","å®Ÿæ–½å½¢æ…‹","å¤‰å½¢ä¾‹","è«‹æ±‚","è«‹æ±‚é …","å›³","å›³é¢","ç¬¦å·","ç¬¦å·ã®èª¬æ˜",
    "å›³é¢ã®ç°¡å˜ãªèª¬æ˜","ç™ºæ˜ã®è©³ç´°ãªèª¬æ˜","æŠ€è¡“åˆ†é‡","èƒŒæ™¯æŠ€è¡“","å¾“æ¥æŠ€è¡“","ç™ºæ˜ãŒè§£æ±ºã—ã‚ˆã†ã¨ã™ã‚‹èª²é¡Œ","èª²é¡Œ",
    "è§£æ±ºæ‰‹æ®µ","åŠ¹æœ","è¦ç´„","ç™ºæ˜ã®åŠ¹æœ","ç›®çš„","æ‰‹æ®µ","æ§‹æˆ","æ§‹é€ ","å·¥ç¨‹","å‡¦ç†","æ–¹æ³•","æ‰‹æ³•","æ–¹å¼",
    "ã‚·ã‚¹ãƒ†ãƒ ","ãƒ—ãƒ­ã‚°ãƒ©ãƒ ","è¨˜æ†¶åª’ä½“","ç‰¹å¾´","ç‰¹å¾´ã¨ã™ã‚‹","ç‰¹å¾´éƒ¨","ã‚¹ãƒ†ãƒƒãƒ—","ãƒ•ãƒ­ãƒ¼","ã‚·ãƒ¼ã‚±ãƒ³ã‚¹","å®šç¾©",
    "é–¢ä¿‚","å¯¾å¿œ","æ•´åˆ","å®Ÿæ–½ã®å½¢æ…‹","å®Ÿæ–½ã®æ…‹æ§˜","æ…‹æ§˜","å¤‰å½¢","ä¿®æ­£ä¾‹","å›³ç¤º","å›³ç¤ºä¾‹","å›³ç¤ºã—ãªã„",
    "å‚ç…§","å‚ç…§ç¬¦å·","æ®µè½","è©³ç´°èª¬æ˜","è¦æ—¨","ä¸€å®Ÿæ–½å½¢æ…‹","ä»–ã®å®Ÿæ–½å½¢æ…‹","ä¸€å®Ÿæ–½ä¾‹","åˆ¥ã®å´é¢","ä»˜è¨˜",
    "é©ç”¨ä¾‹","ç”¨èªã®å®šç¾©","é–‹ç¤º","æœ¬é–‹ç¤º","é–‹ç¤ºå†…å®¹","ä¸Šéƒ¨","ä¸‹éƒ¨","å†…éƒ¨","å¤–éƒ¨","å†…å´","å¤–å´","è¡¨é¢",
    "è£é¢","å´é¢","ä¸Šé¢","ä¸‹é¢","ç«¯é¢","å…ˆç«¯","åŸºç«¯","å¾Œç«¯","ä¸€ç«¯","ä»–ç«¯","ä¸­å¿ƒ","ä¸­å¤®","å‘¨ç¸","å‘¨è¾º",
    "è¿‘å‚","æ–¹å‘","ä½ç½®","ç©ºé–“","é ˜åŸŸ","ç¯„å›²","é–“éš”","è·é›¢","å½¢çŠ¶","å½¢æ…‹","çŠ¶æ…‹","ç¨®é¡","å±¤","è†œ","éƒ¨",
    "éƒ¨æ","éƒ¨ä½","éƒ¨å“","æ©Ÿæ§‹","è£…ç½®","å®¹å™¨","çµ„æˆ","ææ–™","ç”¨é€”","é©ç”¨","é©ç”¨ä¾‹","ç‰‡å´","ä¸¡å´","å·¦å´",
    "å³å´","å‰æ–¹","å¾Œæ–¹","ä¸Šæµ","ä¸‹æµ","éš£æ¥","è¿‘æ¥","é›¢é–“","é–“ç½®","ä»‹åœ¨","é‡ç•³","æ¦‚ã­","ç•¥","ç•¥ä¸­å¤®",
    "å›ºå®šå´","å¯å‹•å´","ä¼¸é•·","åç¸®","ä¿‚åˆ","åµŒåˆ","å–ä»˜","é€£çµéƒ¨","æ”¯æŒä½“","æ”¯æŒéƒ¨","ã‚¬ã‚¤ãƒ‰éƒ¨",
    "ãƒ‡ãƒ¼ã‚¿","æƒ…å ±","ä¿¡å·","å‡ºåŠ›","å…¥åŠ›","åˆ¶å¾¡","æ¼”ç®—","å–å¾—","é€ä¿¡","å—ä¿¡","è¡¨ç¤º","é€šçŸ¥","è¨­å®š","å¤‰æ›´",
    "æ›´æ–°","ä¿å­˜","å‰Šé™¤","è¿½åŠ ","å®Ÿè¡Œ","é–‹å§‹","çµ‚äº†","ç¶™ç¶š","åœæ­¢","åˆ¤å®š","åˆ¤æ–­","æ±ºå®š","é¸æŠ","ç‰¹å®š",
    "æŠ½å‡º","æ¤œå‡º","æ¤œçŸ¥","æ¸¬å®š","è¨ˆæ¸¬","ç§»å‹•","å›è»¢","å¤‰ä½","å¤‰å½¢","å›ºå®š","é…ç½®","ç”Ÿæˆ","ä»˜ä¸","ä¾›çµ¦",
    "é©ç”¨","ç…§åˆ","æ¯”è¼ƒ","ç®—å‡º","è§£æ","åŒå®š","åˆæœŸåŒ–","èª­å‡º","æ›¸è¾¼","ç™»éŒ²","è¨˜éŒ²","é…ä¿¡","é€£æº","åˆ‡æ›¿",
    "èµ·å‹•","å¾©å¸°","ç›£è¦–","é€šçŸ¥å‡¦ç†","å–å¾—å‡¦ç†","æ¼”ç®—å‡¦ç†","è‰¯å¥½","å®¹æ˜“","ç°¡ä¾¿","é©åˆ‡","æœ‰åˆ©","æœ‰ç”¨","æœ‰åŠ¹",
    "åŠ¹æœçš„","é«˜ã„","ä½ã„","å¤§ãã„","å°ã•ã„","æ–°è¦","æ”¹è‰¯","æ”¹å–„","æŠ‘åˆ¶","å‘ä¸Š","ä½æ¸›","å‰Šæ¸›","å¢—åŠ ",
    "æ¸›å°‘","å¯èƒ½","å¥½é©","å¥½ã¾ã—ã„","æœ›ã¾ã—ã„","å„ªã‚Œã‚‹","å„ªã‚ŒãŸ","é«˜æ€§èƒ½","é«˜åŠ¹ç‡","ä½ã‚³ã‚¹ãƒˆ","ã‚³ã‚¹ãƒˆ",
    "ç°¡æ˜“","å®‰å®š","å®‰å®šæ€§","è€ä¹…","è€ä¹…æ€§","ä¿¡é ¼æ€§","ç°¡ç´ ","ç°¡ç•¥","å˜ç´”","æœ€é©","æœ€é©åŒ–","æ±ç”¨","æ±ç”¨æ€§",
    "å®Ÿç¾","é”æˆ","ç¢ºä¿","ç¶­æŒ","é˜²æ­¢","å›é¿","ä¿ƒé€²","ä¸è¦","å¿…è¦","é«˜ç²¾åº¦","çœé›»åŠ›","çœè³‡æº","é«˜ä¿¡é ¼",
    "ä½è² è·","é«˜ç´”åº¦","é«˜å¯†åº¦","é«˜æ„Ÿåº¦","è¿…é€Ÿ","å††æ»‘","ç°¡ç•¥åŒ–","ä½ä¾¡æ ¼","å®ŸåŠ¹çš„","å¯èƒ½åŒ–","æœ‰åŠ¹åŒ–",
    "éå¿…é ˆ","é©åˆ","äº’æ›","å‡ºé¡˜","å‡ºé¡˜äºº","å‡ºé¡˜ç•ªå·","å‡ºé¡˜æ—¥","å‡ºé¡˜æ›¸","å‡ºé¡˜å…¬é–‹","å…¬é–‹","å…¬é–‹ç•ªå·",
    "å…¬é–‹å…¬å ±","å…¬å ±","å…¬å ±ç•ªå·","ç‰¹è¨±","ç‰¹è¨±ç•ªå·","ç‰¹è¨±æ–‡çŒ®","éç‰¹è¨±æ–‡çŒ®","å¼•ç”¨","å¼•ç”¨æ–‡çŒ®","å…ˆè¡ŒæŠ€è¡“",
    "å¯©æŸ»","å¯©æŸ»å®˜","æ‹’çµ¶","æ„è¦‹æ›¸","è£œæ­£æ›¸","å„ªå…ˆ","å„ªå…ˆæ—¥","åˆ†å‰²å‡ºé¡˜","ç¶™ç¶šå‡ºé¡˜","å›½å†…ç§»è¡Œ","å›½éš›å‡ºé¡˜",
    "å›½éš›å…¬é–‹","PCT","ç™»éŒ²","å…¬é–‹æ—¥","å¯©æŸ»è«‹æ±‚","æ‹’çµ¶ç†ç”±","è£œæ­£","è¨‚æ­£","ç„¡åŠ¹å¯©åˆ¤","ç•°è­°","å–æ¶ˆ","å–ä¸‹ã’",
    "äº‹ä»¶ç•ªå·","ä»£ç†äºº","å¼ç†å£«","ä¿‚å±","çµŒé",
    "ç¬¬","ç¬¬ä¸€","ç¬¬äºŒ","ç¬¬ä¸‰","ç¬¬1","ç¬¬ï¼’","ç¬¬ï¼“","ç¬¬ï¼‘","ç¬¬ï¼’","ç¬¬ï¼“","ï¼‘","ï¼’","ï¼“","ï¼”","ï¼•","ï¼–","ï¼—","ï¼˜","ï¼™","ï¼",
    "ä¸€","äºŒ","ä¸‰","å››","äº”","å…­","ä¸ƒ","å…«","ä¹","é›¶","æ•°","è¤‡åˆ","å¤šæ•°","å°‘æ•°","å›³1","å›³2","å›³3","å›³4","å›³5","å›³6","å›³7","å›³8","å›³9",
    "è¡¨1","è¡¨2","è¡¨3","å¼1","å¼2","å¼3","ï¼","ï¼‘","ï¼’","ï¼“","ï¼”","ï¼•","ï¼–","ï¼—","ï¼˜","ï¼™","%","ï¼…","wt%","vol%","è³ªé‡%","é‡é‡%","å®¹é‡%","mol","mol%","mol/L","M","mm","cm","m","nm","Î¼m","Î¼","rpm",
    "Pa","kPa","MPa","GPa","N","W","V","A","mA","Hz","kHz","MHz","GHz","â„ƒ","Â°C","K","mL","L","g","kg","mg","wt","vol",
    "h","hr","hrs","min","s","sec","ppm","ppb","bar","Î©","ohm","J","kJ","Wh","kWh",
    "æ ªå¼ä¼šç¤¾","æœ‰é™ä¼šç¤¾","åˆè³‡ä¼šç¤¾","åˆåä¼šç¤¾","åˆåŒä¼šç¤¾","Inc","Inc.","Ltd","Ltd.","Co","Co.","Corp","Corp.","LLC",
    "GmbH","AG","BV","B.V.","S.A.","S.p.A.","ï¼ˆæ ªï¼‰","ãˆ±","ï¼ˆæœ‰ï¼‰",
    "æº¶æ¶²","æº¶åª’","è§¦åª’","åå¿œ","ç”Ÿæˆç‰©","åŸæ–™","æˆåˆ†","å«æœ‰","å«æœ‰é‡","é…åˆ","æ··åˆ","æ··åˆç‰©","æ¿ƒåº¦","æ¸©åº¦","æ™‚é–“",
    "å‰²åˆ","æ¯”ç‡","åŸº","å®˜èƒ½åŸº","åŒ–åˆç‰©","çµ„æˆç‰©","æ¨¹è„‚","ãƒãƒªãƒãƒ¼","ãƒ¢ãƒãƒãƒ¼","åŸºæ¿","åŸºæ","ãƒ•ã‚£ãƒ«ãƒ ","ã‚·ãƒ¼ãƒˆ",
    "ç²’å­","ç²‰æœ«","æ¯”è¼ƒä¾‹","å‚è€ƒä¾‹","è©¦é¨“","è©¦æ–™","è©•ä¾¡","æ¡ä»¶","å®Ÿé¨“","å®Ÿé¨“ä¾‹","åå¿œæ¡ä»¶","åå¿œæ™‚é–“","åå¿œæ¸©åº¦",
    "å‡¦ç†è£…ç½®","ç«¯æœ«","ãƒ¦ãƒ‹ãƒƒãƒˆ","ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«","å›è·¯","ç´ å­","é›»æº","é›»åœ§","é›»æµ","ä¿¡å·ç·š","é…ç·š","ç«¯å­","ç«¯éƒ¨","æ¥ç¶š",
    "æ¥ç¶šéƒ¨","æ¼”ç®—éƒ¨","è¨˜æ†¶éƒ¨","è¨˜æ†¶è£…ç½®","è¨˜éŒ²åª’ä½“","ãƒ¦ãƒ¼ã‚¶","åˆ©ç”¨è€…","ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ","ã‚µãƒ¼ãƒ","ç”»é¢","UI","GUI",
    "ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹","ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹","DB","ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯","é€šä¿¡","è¦æ±‚","å¿œç­”","ãƒªã‚¯ã‚¨ã‚¹ãƒˆ","ãƒ¬ã‚¹ãƒãƒ³ã‚¹","ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿",
    "å¼•æ•°","å±æ€§","ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£","ãƒ•ãƒ©ã‚°","ID","ãƒ•ã‚¡ã‚¤ãƒ«","ãƒ‡ãƒ¼ã‚¿æ§‹é€ ","ãƒ†ãƒ¼ãƒ–ãƒ«","ãƒ¬ã‚³ãƒ¼ãƒ‰",
    "è»¸","ã‚·ãƒ£ãƒ•ãƒˆ","ã‚®ã‚¢","ãƒ¢ãƒ¼ã‚¿","ã‚¨ãƒ³ã‚¸ãƒ³","ã‚¢ã‚¯ãƒãƒ¥ã‚¨ãƒ¼ã‚¿","ã‚»ãƒ³ã‚µ","ãƒãƒ«ãƒ–","ãƒãƒ³ãƒ—","ç­ä½“","ãƒã‚¦ã‚¸ãƒ³ã‚°","ãƒ•ãƒ¬ãƒ¼ãƒ ",
    "ã‚·ãƒ£ãƒ¼ã‚·","é§†å‹•","ä¼é”","æ”¯æŒ","é€£çµ","è§£æ±º", "æº–å‚™", "æä¾›", "ç™ºç”Ÿ", "ä»¥ä¸Š", "ååˆ†"
]

@st.cache_data
def expand_stopwords_to_full_width(words):
    """åŠè§’æ–‡å­—ã‚’å…¨è§’æ–‡å­—ã«å¤‰æ›ã—ãŸã‚»ãƒƒãƒˆã‚’è¿½åŠ ã™ã‚‹"""
    expanded = set(words)
    hankaku_chars = string.ascii_letters + string.digits
    zenkaku_chars = "ï½ï½‚ï½ƒï½„ï½…ï½†ï½‡ï½ˆï½‰ï½Šï½‹ï½Œï½ï½ï½ï½ï½‘ï½’ï½“ï½”ï½•ï½–ï½—ï½˜ï½™ï½šï¼¡ï¼¢ï¼£ï¼¤ï¼¥ï¼¦ï¼§ï¼¨ï¼©ï¼ªï¼«ï¼¬ï¼­ï¼®ï¼¯ï¼°ï¼±ï¼²ï¼³ï¼´ï¼µï¼¶ï¼·ï¼¸ï¼¹ï¼ºï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™"
    trans_table = str.maketrans(hankaku_chars, zenkaku_chars)
    for word in words:
        if any(c in hankaku_chars for c in word):
            expanded.add(word.translate(trans_table))
    return sorted(list(expanded))

stopwords = set(expand_stopwords_to_full_width(_stopwords_original_list))

# n-gramãƒ•ã‚£ãƒ«ã‚¿å®šç¾©
_ngram_rows = [
    ("å‚ç…§ç¬¦å·ä»˜ãè¦ç´ ", r"[ä¸€-é¾¥ã-ã‚“ã‚¡-ãƒ³A-Za-z0-9ï¼\-ï¼‹ãƒ»]+?(?:éƒ¨|å±¤|é¢|ä½“|æ¿|å­”|æº|ç‰‡|éƒ¨æ|è¦ç´ |æ©Ÿæ§‹|è£…ç½®|æ‰‹æ®µ|é›»æ¥µ|ç«¯å­|é ˜åŸŸ|åŸºæ¿|å›è·¯|ææ–™|å·¥ç¨‹)\s*[ï¼ˆ(]\s*[0-9ï¼-ï¼™A-Za-z]+[A-Za-z]?\s*[ï¼‰)]", "regex", 1),
    ("å‚ç…§ç¬¦å·ä»˜ãè¦ç´ ", r"(?:ä¸Šè¨˜|å‰è¨˜)?[ä¸€-é¾¥ã-ã‚“ã‚¡-ãƒ³A-Za-z0-9ï¼\-ï¼‹ãƒ»]+?(?:éƒ¨|å±¤|é¢|ä½“|æ¿|å­”|æº|ç‰‡|éƒ¨æ|è¦ç´ |æ©Ÿæ§‹|è£…ç½®|æ‰‹æ®µ|é›»æ¥µ|ç«¯å­|é ˜åŸŸ|åŸºæ¿|å›è·¯|ææ–™|å·¥ç¨‹)\s*[0-9ï¼-ï¼™A-Za-z]+[A-Za-z]?", "regex", 1),
    ("å‚ç…§ç¬¦å·ä»˜ãè¦ç´ ", r"[A-Z]+[0-9]+", "regex", 1),
    ("è¦‹å‡ºã—ãƒ»ç« å¥","ä¸€å®Ÿæ–½å½¢æ…‹ã«ãŠã„ã¦","literal",1), ("è¦‹å‡ºã—ãƒ»ç« å¥","ä»–ã®å®Ÿæ–½å½¢æ…‹ã«ãŠã„ã¦","literal",1), ("è¦‹å‡ºã—ãƒ»ç« å¥","åˆ¥ã®å®Ÿæ–½å½¢æ…‹ã«ãŠã„ã¦","literal",1),
    ("è¦‹å‡ºã—ãƒ»ç« å¥","æœ¬æ˜ç´°æ›¸ã«ãŠã„ã¦","literal",1), ("è¦‹å‡ºã—ãƒ»ç« å¥","æœ¬æ˜ç´°æ›¸ã§ã¯","literal",1), ("è¦‹å‡ºã—ãƒ»ç« å¥","æœ¬ç™ºæ˜ã®ä¸€å´é¢","literal",1),
    ("è¦‹å‡ºã—ãƒ»ç« å¥","ä¸€å®Ÿæ–½ä¾‹ã«ãŠã„ã¦","literal",1), ("è¦‹å‡ºã—ãƒ»ç« å¥","ä»–ã®å®Ÿæ–½ä¾‹ã«ãŠã„ã¦","literal",1), ("è¦‹å‡ºã—ãƒ»ç« å¥","å¥½ã¾ã—ã„æ…‹æ§˜ã¨ã—ã¦","literal",2),
    ("è¦‹å‡ºã—ãƒ»ç« å¥","å¥½é©ã«ã¯","literal",2), ("è¦‹å‡ºã—ãƒ»ç« å¥","ç”¨èªã®å®šç¾©","literal",2), ("è¦‹å‡ºã—ãƒ»ç« å¥","å›³ç¤ºã—ãªã„","literal",2),
    ("å›³è¡¨å‚ç…§", r"å›³[ ã€€]*[ï¼-ï¼™0-9]+ã«ç¤ºã™", "regex", 1), ("å›³è¡¨å‚ç…§", r"è¡¨[ ã€€]*[ï¼-ï¼™0-9]+ã«ç¤ºã™", "regex", 1),
    ("å›³è¡¨å‚ç…§", r"å¼[ ã€€]*[ï¼-ï¼™0-9]+ã«ç¤ºã™", "regex", 1), ("å›³è¡¨å‚ç…§", r"è«‹æ±‚é …[ ã€€]*[ï¼-ï¼™0-9]+", "regex", 1),
    ("å›³è¡¨å‚ç…§", r"(?:ã€|\[)\s*[ï¼-ï¼™0-9]{4,5}\s*(?:ã€‘|\])", "regex", 1), ("å›³è¡¨å‚ç…§", r"[ï¼ˆ(][ï¼-ï¼™0-9]+[ï¼‰)]", "regex", 2),
    ("å›³è¡¨å‚ç…§", r"ç¬¬\s*[ï¼-ï¼™0-9]+ã®?å®Ÿæ–½å½¢æ…‹", "regex", 2), ("å›³è¡¨å‚ç…§", r"æ®µè½\s*[ï¼-ï¼™0-9]+", "regex", 2),
    ("å›³è¡¨å‚ç…§", r"å›³[ ã€€]*[ï¼-ï¼™0-9]+[A-Za-z]?", "regex", 2), ("å®šç¾©å°å…¥", r"ä»¥ä¸‹ã€[^ã€ã€‚]+ã‚’[^ã€ã€‚]+ã¨ç§°ã™ã‚‹", "regex", 1),
    ("å®šç¾©å°å…¥", r"ä»¥ä¸‹ã€[^ã€ã€‚]+ã‚’[^ã€ã€‚]+ã¨ã„ã†", "regex", 1), ("æ©Ÿèƒ½å¥","ã—ã¦ã‚‚ã‚ˆã„","literal",1), ("æ©Ÿèƒ½å¥","ã§ã‚ã£ã¦ã‚‚ã‚ˆã„","literal",1),
    ("æ©Ÿèƒ½å¥","ã™ã‚‹ã“ã¨ãŒã§ãã‚‹","literal",1), ("æ©Ÿèƒ½å¥","è¡Œã†ã“ã¨ãŒã§ãã‚‹","literal",1), ("æ©Ÿèƒ½å¥","ã«é™å®šã•ã‚Œãªã„","literal",1),
    ("æ©Ÿèƒ½å¥","ã«é™ã‚‰ã‚Œãªã„","literal",1), ("æ©Ÿèƒ½å¥","ä¸€ä¾‹ã¨ã—ã¦","literal",2), ("æ©Ÿèƒ½å¥","ä¾‹ç¤ºçš„ã«ã¯","literal",2),
    ("å‚ç…§å¥","å‰è¿°ã®ã¨ãŠã‚Š","literal",2), ("å‚ç…§å¥","å‰è¿°ã®é€šã‚Š","literal",2), ("å‚ç…§å¥","å¾Œè¿°ã™ã‚‹ã‚ˆã†ã«","literal",2),
    ("å‚ç…§å¥","å¾Œè¿°ã®ã¨ãŠã‚Š","literal",2), ("ç¯„å›²è¡¨ç¾", r"å°‘ãªãã¨ã‚‚(?:ä¸€|ï¼‘)ã¤", "regex", 2), ("ç¯„å›²è¡¨ç¾", "å°‘ãªãã¨ã‚‚ä¸€éƒ¨", "literal", 2),
    ("ç¯„å›²è¡¨ç¾", r"è¤‡æ•°ã®(?:å®Ÿæ–½å½¢æ…‹|æ§‹æˆ|è¦ç´ )", "regex", 3), ("èª²é¡Œå¥", r"(?:ä¸Šè¨˜|å‰è¨˜)ã®?èª²é¡Œ", "regex", 1),
    ("æ¥ç¶šãƒ»è«–ç†","ä¸€æ–¹ã§","literal",3), ("æ¥ç¶šãƒ»è«–ç†","ä»–æ–¹ã§","literal",3), ("æ¥ç¶šãƒ»è«–ç†","ã™ãªã‚ã¡","literal",3),
    ("æ¥ç¶šãƒ»è«–ç†","ã—ãŸãŒã£ã¦","literal",3), ("æ¥ç¶šãƒ»è«–ç†","ã—ã‹ã—ãªãŒã‚‰","literal",3), ("æ¥ç¶šãƒ»è«–ç†","ä¾‹ãˆã°","literal",3),
    ("æ¥ç¶šãƒ»è«–ç†","å…·ä½“çš„ã«ã¯","literal",3), ("è£œåŠ©å¥","ä»¥ä¸‹ã«èª¬æ˜ã™ã‚‹","literal",3), ("è£œåŠ©å¥","å‰è¨˜ã®ã¨ãŠã‚Š","literal",3),
    ("è£œåŠ©å¥","ã“ã‚Œã«ã‚ˆã‚Š","literal",3), ("è£œåŠ©å¥","ã“ã®ã‚ˆã†ã«","literal",3)
]

_ngram_compiled = sorted(_ngram_rows, key=lambda x: (x[3], -len(x[1]) if x[2]=="literal" else -50))
_ngram_compiled = [(cat, re.compile(pat) if ptype == "regex" else pat, ptype, pri) for cat, pat, ptype, pri in _ngram_compiled]

def normalize_text(text):
    if not isinstance(text, str): text = "" if pd.isna(text) else str(text)
    text = unicodedata.normalize("NFKC", text)
    text = text.replace("Âµ", "Î¼")
    text = re.sub(r"\s+", " ", text)
    return text

def apply_ngram_filters(text):
    for cat, pat, ptype, pri in _ngram_compiled:
        if ptype == "literal":
            if pat in text: text = text.replace(pat, "")
        else:
            text = pat.sub("", text)
    return text

@st.cache_data
def extract_compound_nouns(text):
    text = normalize_text(text)
    text = apply_ngram_filters(text)
    text = re.sub(r'ã€.*?ã€‘', '', text)
    text = re.sub(r'[!\"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~]', ' ', text)

    tokens = t.tokenize(text)
    words, compound_word = [], ''
    for token in tokens:
        pos = token.part_of_speech.split(',')[0]
        if pos == 'åè©':
            compound_word += token.surface
        else:
            if (len(compound_word) > 1 and
                compound_word not in stopwords and
                not re.fullmatch(r'[\dï¼-ï¼™]+', compound_word) and
                not re.fullmatch(r'(å›³|è¡¨|å¼|ç¬¬)[\dï¼-ï¼™]+.*', compound_word) and
                not re.match(r'^(ä¸Šè¨˜|å‰è¨˜|æœ¬é–‹ç¤º|å½“è©²|è©²)', compound_word) and
                not re.search(r'[0-9ï¼-ï¼™]+[)ï¼‰]?$', compound_word) and
                not re.match(r'[0-9ï¼-ï¼™]+[a-zA-Zï½-ï½šï¼¡-ï¼º]', compound_word)):
                words.append(compound_word)
            compound_word = ''
            
    if (len(compound_word) > 1 and
        compound_word not in stopwords and
        not re.fullmatch(r'[\dï¼-ï¼™]+', compound_word) and
        not re.fullmatch(r'(å›³|è¡¨|å¼|ç¬¬)[\dï¼-ï¼™]+.*', compound_word) and
        not re.match(r'^(ä¸Šè¨˜|å‰è¨˜|æœ¬é–‹ç¤º|å½“è©²|è©²)', compound_word) and
        not re.search(r'[0-9ï¼-ï¼™]+[)ï¼‰]?$', compound_word) and
        not re.match(r'[0-9ï¼-ï¼™]+[a-zA-Zï½-ï½šï¼¡-ï¼º]', compound_word)):
        words.append(compound_word)
    return words

def get_font_path_for_wordcloud():
    system_name = platform.system()
    candidates = []
    if system_name == 'Linux':
        candidates = ['/usr/share/fonts/opentype/ipafont-gothic/ipagp.ttf', '/usr/share/fonts/truetype/fonts-japanese-gothic.ttf']
    elif system_name == 'Darwin': 
        candidates = ['/System/Library/Fonts/Hiragino Sans W3.ttc', '/System/Library/Fonts/ãƒ’ãƒ©ã‚®ãƒè§’ã‚´ã‚·ãƒƒã‚¯ W3.ttc']
    elif system_name == 'Windows':
        candidates = ['C:\\Windows\\Fonts\\meiryo.ttc', 'C:\\Windows\\Fonts\\msgothic.ttc']
    
    for path in candidates:
        if os.path.exists(path): return path
    return None

font_path = get_font_path_for_wordcloud()

def generate_wordcloud_and_list(words, title, top_n=20, font_path=None):
    if not words:
        st.subheader(title)
        st.warning("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚ã€è¡¨ç¤ºã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")
        return None

    word_freq = Counter(words)

    try:
        wc = WordCloud(
            width=800, height=400, background_color='white',
            font_path=font_path, collocations=False,
            max_words=100
        ).generate_from_frequencies(word_freq)

        fig, ax = plt.subplots(figsize=(12, 6))
        ax.imshow(wc, interpolation='bilinear')
        ax.set_title(title, fontsize=20)
        ax.axis('off')
        st.pyplot(fig)
        
        st.markdown(f"**ä¸Šä½ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ (Top {top_n})**")
        list_data = { "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": [], "å‡ºç¾é »åº¦": [] }
        for word, freq in word_freq.most_common(top_n):
            list_data["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"].append(word)
            list_data["å‡ºç¾é »åº¦"].append(freq)
        st.dataframe(pd.DataFrame(list_data), height=200)
        
    except Exception as e:
        st.error(f"ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã®æç”»ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        if font_path is None:
            st.warning("æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

@st.cache_data
def get_top_tfidf_words(_row_vector, feature_names, top_n=5):
    scores = _row_vector.toarray().flatten() 
    indices = np.argsort(scores)[::-1]
    non_zero_indices = [i for i in indices if scores[i] > 0]
    top_indices = non_zero_indices[:top_n]
    top_words = [feature_names[i] for i in top_indices]
    return ", ".join(top_words)

def update_hover_text(df, col_map):
    hover_texts = []
    for index, row in df.iterrows():
        text = ""
        if col_map['title'] and pd.notna(row[col_map['title']]): text += f"<b>åç§°:</b> {str(row[col_map['title']])[:50]}...<br>"
        if col_map['app_num'] and pd.notna(row[col_map['app_num']]): text += f"<b>ç•ªå·:</b> {row[col_map['app_num']]}<br>"
        if col_map['applicant'] and pd.notna(row[col_map['applicant']]): text += f"<b>å‡ºé¡˜äºº:</b> {str(row[col_map['applicant']])[:50]}...<br>"
        if 'characteristic_words' in row: text += f"<b>ç‰¹å¾´èª:</b> {row['characteristic_words']}<br>"
        if 'cluster_label' in row: text += f"<b>ã‚¯ãƒ©ã‚¹ã‚¿:</b> {row['cluster_label']}"
        hover_texts.append(text)
    df['hover_text'] = hover_texts
    return df

def update_drill_hover_text(df_subset):
    df_subset['drill_hover_text'] = df_subset.apply(
        lambda row: f"{row['hover_text']}<br><b>ã‚µãƒ–ã‚¯ãƒ©ã‚¹ã‚¿:</b> {row['drill_cluster_label']}", axis=1
    )
    return df_subset

def _create_label_editor_ui(original_map, current_map, key_prefix):
    widgets_dict = {}
    sorted_ids = sorted([cid for cid in original_map.keys() if cid != -1])
    
    for cluster_id in sorted_ids:
        orig_label = original_map.get(cluster_id, "")
        curr_label = current_map.get(cluster_id, orig_label)
        if orig_label == "(è©²å½“ãªã—)": continue
        col1, col2 = st.columns([2, 3])
        with col1: st.markdown(f":green[{orig_label}]")
        with col2:
            new_label = st.text_input(f"Edit {cluster_id}", value=curr_label, label_visibility="collapsed", key=f"{key_prefix}_{cluster_id}")
            widgets_dict[cluster_id] = new_label
            
    if -1 in original_map:
        orig_noise = original_map[-1]
        curr_noise = current_map.get(-1, orig_noise)
        col1, col2 = st.columns([2, 3])
        with col1: st.markdown(f":green[{orig_noise}]")
        with col2:
            st.text_input(f"noise_label", value=curr_noise, disabled=True, key=f"{key_prefix}_noise")
            widgets_dict[-1] = curr_noise
    return widgets_dict

def get_date_bin_options(df_filtered, interval_years, year_column='year'):
    if df_filtered is None or df_filtered.empty: return [f"(ãƒ‡ãƒ¼ã‚¿ãªã—)"]
    if year_column not in df_filtered.columns: return [f"(å…¨æœŸé–“) ({len(df_filtered)}ä»¶)"]
    
    df_filtered = df_filtered.copy()
    df_filtered[year_column] = pd.to_numeric(df_filtered[year_column], errors='coerce')
    if df_filtered[year_column].isnull().all(): return [f"(å…¨æœŸé–“) ({len(df_filtered)}ä»¶)"]

    try:
        min_year = int(df_filtered[year_column].min())
        max_year = int(df_filtered[year_column].max())
        if min_year == max_year: return [f"{min_year} ({len(df_filtered)}ä»¶)"]
        
        bins = list(range(min_year, max_year + interval_years, interval_years))
        if not bins: bins = [min_year]
        if bins[-1] <= max_year: bins.append(bins[-1] + interval_years)

        labels = [f"{bins[i]}-{bins[i+1] - 1}" for i in range(len(bins)-1)]
        df_filtered['temp_date_bin'] = pd.cut(df_filtered[year_column], bins=bins, labels=labels, right=False, include_lowest=True)
        date_bin_counts = df_filtered['temp_date_bin'].value_counts()
        
        options = [f"(å…¨æœŸé–“) ({len(df_filtered)}ä»¶)"] + [f"{label} ({date_bin_counts.get(label, 0)}ä»¶)" for label in labels if date_bin_counts.get(label, 0) > 0]
        return options
    except Exception as e:
        return [f"Error: {str(e)}"]

# ==================================================================
# --- 3. Streamlit UIæ§‹æˆ ---
# ==================================================================

st.set_page_config(
    page_title="APOLLO | Saturn V", 
    page_icon="ğŸš€", 
    layout="wide"
)

# CSSæ³¨å…¥
st.markdown("""
<style>
    html, body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif; }
    [data-testid="stSidebar"] h1 { color: #003366; font-weight: 900 !important; font-size: 2.5rem !important; }
    [data-testid="stSidebarNav"] { display: none !important; }
    [data-testid="stSidebar"] .block-container { padding-top: 2rem; padding-bottom: 1rem; }
    .block-container { padding-top: 2rem; padding-bottom: 2rem; }
    .stButton>button { font-weight: 600; }
    .stTabs [data-baseweb="tab-list"] { gap: 8px; }
    .stTabs [data-baseweb="tab"] { background-color: #f0f2f6; border-radius: 8px 8px 0 0; padding: 10px 15px; }
    .stTabs [aria-selected="true"] { background-color: #ffffff; border-bottom: 2px solid #003366; }
</style>
""", unsafe_allow_html=True)

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ ---
with st.sidebar:
    st.title("APOLLO") 
    st.markdown("Advanced Patent & Overall Landscape-analytics Logic Orbiter")
    st.markdown("---")
    
    st.subheader("Home")
    st.page_link("Home.py", label="Mission Control", icon="ğŸ›°ï¸")
    
    st.subheader("Modules")
    st.page_link("pages/1_ğŸŒ_ATLAS.py", label="ATLAS", icon="ğŸŒ")
    st.page_link("pages/2_ğŸ’¡_CORE.py", label="CORE", icon="ğŸ’¡")
    st.page_link("pages/3_ğŸš€_Saturn_V.py", label="Saturn V", icon="ğŸš€")
    st.page_link("pages/4_ğŸ“ˆ_MEGA.py", label="MEGA", icon="ğŸ“ˆ")
    st.page_link("pages/5_ğŸ§­_Explorer.py", label="Explorer", icon="ğŸ§­")
    
    st.markdown("---")
    
    st.caption("ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³:")
    st.caption("1. Mission Control ã§ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€å‰å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
    st.caption("2. ä¸Šã®ãƒªã‚¹ãƒˆã‹ã‚‰åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é¸æŠã—ã¾ã™ã€‚")
    
    st.markdown("---")
    st.caption("Â© 2025 ã—ã°ã‚„ã¾")

# --- ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ ---
st.title("ğŸš€ Saturn V")
st.markdown("SBERTï¼ˆæ–‡è„ˆãƒ»æ„å‘³ï¼‰ã«åŸºã¥ãã€ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªæŠ€è¡“ãƒãƒƒãƒ—åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§ã™ã€‚")

# ãƒ†ãƒ¼ãƒé¸æŠ
col_theme, _ = st.columns([1, 3])
with col_theme:
    selected_theme = st.selectbox("è¡¨ç¤ºãƒ†ãƒ¼ãƒ:", ["APOLLO Standard", "Modern Presentation"], key="saturn_theme_selector")
theme_config = get_theme_config(selected_theme)
st.markdown(f"<style>{theme_config['css']}</style>", unsafe_allow_html=True)

# ==================================================================
# --- 4. ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ & åˆæœŸåŒ– ---
# ==================================================================
if not st.session_state.get("preprocess_done", False):
    st.error("åˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    st.warning("å…ˆã«ã€ŒMission Controlã€ï¼ˆãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ï¼‰ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€ã€Œåˆ†æã‚¨ãƒ³ã‚¸ãƒ³èµ·å‹•ã€ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    st.stop()
else:
    df_main = st.session_state.df_main
    col_map = st.session_state.col_map
    delimiters = st.session_state.delimiters
    sbert_embeddings = st.session_state.sbert_embeddings
    tfidf_matrix = st.session_state.tfidf_matrix
    feature_names = st.session_state.feature_names
    
if "saturnv_sbert_umap_done" not in st.session_state: st.session_state.saturnv_sbert_umap_done = False
if "saturnv_cluster_done" not in st.session_state: st.session_state.saturnv_cluster_done = False
if "saturnv_labels_map" not in st.session_state: st.session_state.saturnv_labels_map = {}
if "main_cluster_running" not in st.session_state: st.session_state.main_cluster_running = False
if "saturnv_global_zmax" not in st.session_state: st.session_state.saturnv_global_zmax = None

# ==================================================================
# --- 5. Saturn V ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ ---
# ==================================================================

# --- åˆå›UMAPè¨ˆç®— ---
if not st.session_state.saturnv_sbert_umap_done:
    with st.spinner("Saturn V ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åˆå›èµ·å‹•ä¸­: UMAPã«ã‚ˆã‚‹æ¬¡å…ƒå‰Šæ¸› (SBERTãƒ™ãƒ¼ã‚¹) ã‚’å®Ÿè¡Œã—ã¦ã„ã¾ã™..."):
        try:
            reducer = UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)
            embedding = reducer.fit_transform(sbert_embeddings) 
            st.session_state.df_main['umap_x'] = embedding[:, 0]
            st.session_state.df_main['umap_y'] = embedding[:, 1]
            st.session_state.df_main['characteristic_words'] = [get_top_tfidf_words(tfidf_matrix[i], feature_names) for i in range(tfidf_matrix.shape[0])]
            
            try:
                H, _, _ = np.histogram2d(embedding[:, 0], embedding[:, 1], bins=50)
                st.session_state.saturnv_global_zmax = H.max()
            except:
                st.session_state.saturnv_global_zmax = None
            
            st.session_state.saturnv_sbert_umap_done = True
            st.success("UMAPã®åˆæœŸè¨ˆç®—ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
            st.rerun()
        except Exception as e:
            st.error(f"UMAPã®åˆæœŸè¨ˆç®—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            st.stop()

# --- ãƒ¡ã‚¤ãƒ³UI ---
tab_main, tab_drill, tab_stats, tab_export = st.tabs([
    "Landscape Map (TELESCOPE)", 
    "Drilldown (PROBE)", 
    "ç‰¹è¨±ãƒãƒƒãƒ— (çµ±è¨ˆåˆ†æ)", 
    "Data Export"
])

# --- TELESCOPE ---
with tab_main:
    st.subheader("ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ")
    col1, col2, col3 = st.columns([2, 2, 1])
    with col1: min_cluster_size_w = st.number_input("æœ€å°ã‚¯ãƒ©ã‚¹ã‚¿ã‚µã‚¤ã‚º:", min_value=2, value=15, key="main_min_cluster_size")
    with col2: min_samples_w = st.number_input("æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°:", min_value=1, value=10, key="main_min_samples")
    with col3: label_top_n_w = st.number_input("ãƒ©ãƒ™ãƒ«å˜èªæ•°:", min_value=1, value=3, key="main_label_top_n")
    
    if st.button("æç”» (å†è¨ˆç®—)", type="primary", key="main_run_cluster", disabled=st.session_state.main_cluster_running):
        st.session_state.main_cluster_running = True
        with st.spinner("HDBSCANã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œä¸­..."):
            try:
                embedding = st.session_state.df_main[['umap_x', 'umap_y']].values
                clusterer = hdbscan.HDBSCAN(min_cluster_size=int(min_cluster_size_w), min_samples=int(min_samples_w), metric='euclidean', cluster_selection_method='eom')
                clustering = clusterer.fit(embedding)
                st.session_state.df_main['cluster'] = clustering.labels_
                
                labels_map = {}
                label_top_n = int(label_top_n_w)
                unique_clusters = sorted(st.session_state.df_main['cluster'].unique())
                
                for cluster_id in unique_clusters:
                    if cluster_id == -1:
                        labels_map[cluster_id] = "ãƒã‚¤ã‚º / å°ã‚¯ãƒ©ã‚¹ã‚¿"
                        continue
                    indices = st.session_state.df_main[st.session_state.df_main['cluster'] == cluster_id].index
                    if len(indices) == 0:
                        labels_map[cluster_id] = "(è©²å½“ãªã—)"
                        continue
                    cluster_vectors = tfidf_matrix[indices]
                    mean_vector = np.array(cluster_vectors.mean(axis=0)).flatten()
                    top_indices = np.argsort(mean_vector)[::-1][:label_top_n]
                    label = ", ".join([feature_names[i] for i in top_indices])
                    labels_map[cluster_id] = f"[{cluster_id}] {label}"
                
                st.session_state.df_main['cluster_label'] = st.session_state.df_main['cluster'].map(labels_map)
                st.session_state.saturnv_labels_map = labels_map.copy()
                st.session_state.saturnv_labels_map_original = labels_map.copy()
                st.session_state.df_main = update_hover_text(st.session_state.df_main, col_map)
                st.session_state.saturnv_cluster_done = True
                st.success("ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Œäº†")
                st.rerun()
            except Exception as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
            finally:
                st.session_state.main_cluster_running = False

    st.markdown("---")
    
    if st.session_state.saturnv_cluster_done:
        st.subheader("ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è¨­å®š (ãƒ¡ã‚¤ãƒ³ç”¨)")
        def on_main_interval_change():
            if "main_date_filter" in st.session_state: del st.session_state.main_date_filter

        col1, col2 = st.columns(2)
        with col1:
            if 'year' in df_main.columns and df_main['year'].notna().any():
                bin_interval_w_val = st.selectbox("æœŸé–“ã®ç²’åº¦:", [5, 3, 2, 1], index=0, key="main_bin_interval", on_change=on_main_interval_change)
                date_bin_options = get_date_bin_options(df_main, int(bin_interval_w_val), 'year')
                date_bin_filter_w = st.selectbox("è¡¨ç¤ºæœŸé–“:", date_bin_options, key="main_date_filter")
            else:
                date_bin_filter_w = "(å…¨æœŸé–“)"
        
        with col2:
            if col_map['applicant'] and col_map['applicant'] in st.session_state.df_main.columns:
                applicants = st.session_state.df_main[col_map['applicant']].fillna('').str.split(delimiters['applicant']).explode().str.strip()
                unique_applicants = sorted([app for app in applicants.unique() if app])
                applicant_options = [(f"(å…¨å‡ºé¡˜äºº) ({len(st.session_state.df_main)}ä»¶)", "ALL")] + [(f"{app}", app) for app in unique_applicants]
                applicant_filter_w = st.multiselect("å‡ºé¡˜äºº:", applicant_options, default=[applicant_options[0]], format_func=lambda x: x[0], key="main_applicant_filter")
            else:
                applicant_filter_w = [(f"(å…¨å‡ºé¡˜äºº) ({len(st.session_state.df_main)}ä»¶)", "ALL")]

        cluster_counts = st.session_state.df_main['cluster_label'].value_counts()
        cluster_options = [(f"(å…¨ã‚¯ãƒ©ã‚¹ã‚¿) ({len(st.session_state.df_main)}ä»¶)", "ALL")] + [
            (f"{st.session_state.saturnv_labels_map.get(cid)} ({cluster_counts.get(st.session_state.saturnv_labels_map.get(cid), 0)}ä»¶)", cid)
            for cid in sorted(st.session_state.df_main['cluster'].unique())
        ]
        cluster_filter_w = st.multiselect("ãƒãƒƒãƒ—è¡¨ç¤ºã‚¯ãƒ©ã‚¹ã‚¿:", cluster_options, default=[cluster_options[0]], format_func=lambda x: x[0], key="main_cluster_filter")

        st.subheader("åˆ†æçµæœ (TELESCOPE ãƒ¡ã‚¤ãƒ³ãƒãƒƒãƒ—)")
        col_mode, col_norm = st.columns([1, 1])
        with col_mode: map_mode = st.radio("è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰:", ["æ•£å¸ƒå›³ (Scatter)", "å¯†åº¦ãƒãƒƒãƒ— (Density)"], horizontal=True)
        use_abs_scale = False
        if map_mode == "å¯†åº¦ãƒãƒƒãƒ— (Density)":
            with col_norm: use_abs_scale = st.checkbox("å¯†åº¦ã‚¹ã‚±ãƒ¼ãƒ«ã‚’å›ºå®š (çµ¶å¯¾è©•ä¾¡)", value=False)
        
        show_labels_chk = st.checkbox("ãƒãƒƒãƒ—ã«ãƒ©ãƒ™ãƒ«ã‚’è¡¨ç¤ºã™ã‚‹", value=True, key="main_show_labels")
        
        df_visible = st.session_state.df_main.copy()
        # ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨
        if not date_bin_filter_w.startswith("(å…¨æœŸé–“)"):
            try:
                date_bin_label = date_bin_filter_w.split(' (')[0].strip()
                start_year, end_year = map(int, date_bin_label.split('-'))
                df_visible = df_visible[(df_visible['year'] >= start_year) & (df_visible['year'] <= end_year)]
            except: pass

        applicant_values = [val[1] for val in applicant_filter_w]
        is_applicant_color_mode = True
        if "ALL" in applicant_values:
             is_applicant_color_mode = False
        else:
             mask_list = [df_visible[col_map['applicant']].fillna('').str.contains(re.escape(app)) for app in applicant_values]
             if mask_list: df_visible = df_visible[pd.concat(mask_list, axis=1).any(axis=1)]
        
        cluster_values = [val[1] for val in cluster_filter_w]
        if "ALL" not in cluster_values:
            df_visible = df_visible[df_visible['cluster'].isin(cluster_values)]

        # æç”»
        fig_main = go.Figure()
        if map_mode == "å¯†åº¦ãƒãƒƒãƒ— (Density)":
             contour_params = dict(x=df_visible['umap_x'], y=df_visible['umap_y'], colorscale=theme_config["density_scale"], reversescale=False, xaxis='x', yaxis='y', showscale=True, name="å¯†åº¦", nbinsx=50, nbinsy=50)
             if use_abs_scale and st.session_state.saturnv_global_zmax:
                 contour_params.update(dict(zauto=False, zmin=0, zmax=st.session_state.saturnv_global_zmax))
             else: contour_params.update(dict(zauto=True))
             fig_main.add_trace(go.Histogram2dContour(**contour_params))
             fig_main.add_trace(go.Scatter(x=df_visible['umap_x'], y=df_visible['umap_y'], mode='markers', marker=dict(color='white', size=3, opacity=0.3), hoverinfo='text', hovertext=df_visible['hover_text'], name='ç‰¹è¨±'))
        else:
            # æ•£å¸ƒå›³
            fig_main.add_trace(go.Scatter(x=df_visible['umap_x'], y=df_visible['umap_y'], mode='markers', marker=dict(color=df_visible['cluster'], colorscale=theme_config["color_sequence"], showscale=False, size=4, opacity=0.6), hoverinfo='text', hovertext=df_visible['hover_text'], name='è¡¨ç¤ºå¯¾è±¡'))

        # ãƒ©ãƒ™ãƒ«è¿½åŠ 
        if show_labels_chk:
            for cid, grp in df_visible[df_visible['cluster'].isin(df_visible['cluster'].unique())].groupby('cluster'):
                if cid == -1: continue
                mean_pos = grp[['umap_x', 'umap_y']].mean()
                if not grp.empty:
                    label_txt = grp['cluster_label'].iloc[0]
                    fig_main.add_annotation(x=mean_pos['umap_x'], y=mean_pos['umap_y'], text=label_txt, showarrow=False, font=dict(size=11, color='black' if map_mode=="æ•£å¸ƒå›³ (Scatter)" else 'white'), bgcolor='rgba(255,255,255,0.7)' if map_mode=="æ•£å¸ƒå›³ (Scatter)" else 'rgba(0,0,0,0.5)')

        norm_msg = " (çµ¶å¯¾è©•ä¾¡)" if use_abs_scale and map_mode == "å¯†åº¦ãƒãƒƒãƒ— (Density)" else ""
        update_fig_layout(fig_main, f"Saturn V - ãƒ¡ã‚¤ãƒ³ãƒãƒƒãƒ— (SBERT UMAP){norm_msg}", height=800, theme_config=theme_config)
        fig_main.update_xaxes(showgrid=False, zeroline=False, showticklabels=False)
        fig_main.update_yaxes(showgrid=False, zeroline=False, showticklabels=False)
        st.plotly_chart(fig_main, use_container_width=True)

        st.subheader("ãƒ©ãƒ™ãƒ«ç·¨é›†")
        if "saturnv_labels_map_original" not in st.session_state: st.session_state.saturnv_labels_map_original = st.session_state.saturnv_labels_map.copy()
        st.session_state.saturnv_labels_map_custom = _create_label_editor_ui(st.session_state.saturnv_labels_map_original, st.session_state.saturnv_labels_map, "main_label")
        if st.button("ãƒ©ãƒ™ãƒ«ã‚’æ›´æ–°", key="main_update_labels"):
            st.session_state.df_main['cluster_label'] = st.session_state.df_main['cluster'].map(st.session_state.saturnv_labels_map_custom)
            st.session_state.df_main = update_hover_text(st.session_state.df_main, col_map)
            st.session_state.saturnv_labels_map = st.session_state.saturnv_labels_map_custom
            st.rerun()

    # --- PROBE (ãƒ‰ãƒªãƒ«ãƒ€ã‚¦ãƒ³) ---
    with tab_drill:
        st.subheader("åˆ†æå¯¾è±¡ã‚¯ãƒ©ã‚¹ã‚¿ã®é¸æŠ")
        drilldown_options = [('(é¸æŠã—ã¦ãã ã•ã„)', 'NONE')]
        if "saturnv_labels_map" in st.session_state:
            drilldown_options += [(f"{label} ({count}ä»¶)", cid) for cid, label in st.session_state.saturnv_labels_map.items() if cid != -1 for count in [st.session_state.df_main['cluster'].value_counts().get(cid, 0)]]
        
        selected_drilldown_target_drill = st.selectbox("åˆ†æå¯¾è±¡ã‚¯ãƒ©ã‚¹ã‚¿:", options=drilldown_options, format_func=lambda x: x[0], key="drill_target_select")
        drilldown_target_id = selected_drilldown_target_drill[1] 

        st.subheader("ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è¨­å®š (ãƒ‰ãƒªãƒ«ãƒ€ã‚¦ãƒ³ç”¨)")
        if drilldown_target_id == "NONE":
            df_subset_filter = pd.DataFrame(columns=df_main.columns)
            st.info("ğŸ‘† ä¸Šã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã§ã€Œåˆ†æå¯¾è±¡ã‚¯ãƒ©ã‚¹ã‚¿ã€ã‚’é¸æŠã™ã‚‹ã¨ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¡ãƒ‹ãƒ¥ãƒ¼ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")
        else:
            df_subset_filter = df_main[df_main['cluster'] == drilldown_target_id].copy()
            
        def on_drill_interval_change():
            if "drill_date_filter_w" in st.session_state: del st.session_state.drill_date_filter_w
            
        if drilldown_target_id != "NONE":
            col1, col2 = st.columns(2)
            with col1:
                if 'year' in df_subset_filter.columns and df_subset_filter['year'].notna().any():
                    drill_bin_interval_w_val = st.selectbox("æœŸé–“ã®ç²’åº¦:", [5, 3, 2, 1], index=0, key="drill_interval_w", on_change=on_drill_interval_change)
                    drill_date_bin_options = get_date_bin_options(df_subset_filter, int(drill_bin_interval_w_val), 'year')
                    drill_date_bin_filter_w = st.selectbox("è¡¨ç¤ºæœŸé–“:", drill_date_bin_options, key="drill_date_filter_w")
                else:
                    drill_date_bin_filter_w = "(å…¨æœŸé–“)"
            with col2:
                drill_applicant_options = [(f"(å…¨å‡ºé¡˜äºº) ({len(df_subset_filter)}ä»¶)", "ALL")]
                if col_map['applicant'] and col_map['applicant'] in df_subset_filter.columns:
                    applicants_drill = df_subset_filter[col_map['applicant']].fillna('').str.split(delimiters['applicant']).explode().str.strip()
                    applicant_counts_drill = applicants_drill.value_counts()
                    unique_applicants_drill = sorted([app for app in applicants_drill.unique() if app])
                    drill_applicant_options += [(f"{app_name} ({applicant_counts_drill.get(app_name, 0)}ä»¶)", app_name) for app_name in unique_applicants_drill if applicant_counts_drill.get(app_name, 0) > 0]
                drill_applicant_filter_w = st.multiselect("å‡ºé¡˜äºº:", drill_applicant_options, default=[drill_applicant_options[0]], format_func=lambda x: x[0], key="drill_applicant_filter_w")
        else:
            drill_date_bin_filter_w = "(å…¨æœŸé–“)"
            drill_applicant_filter_w = []

        st.subheader("ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°è¨­å®š (ãƒ‰ãƒªãƒ«ãƒ€ã‚¦ãƒ³ç”¨)")
        col1, col2, col3 = st.columns(3)
        with col1: drill_min_cluster_size_w = st.number_input('æœ€å°ã‚¯ãƒ©ã‚¹ã‚¿ã‚µã‚¤ã‚º:', min_value=2, value=5, key="drill_min_cluster_size_w")
        with col2: drill_min_samples_w = st.number_input('æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°:', min_value=1, value=5, key="drill_min_samples_w")
        with col3: drill_label_top_n_w = st.number_input('ãƒ©ãƒ™ãƒ«å˜èªæ•°:', min_value=1, value=3, key="drill_label_top_n_w")
        drill_show_labels_chk = st.checkbox('ãƒãƒƒãƒ—ã«ãƒ©ãƒ™ãƒ«ã‚’è¡¨ç¤ºã™ã‚‹', value=True, key="drill_show_labels_chk")

        if st.button("é¸æŠã‚¯ãƒ©ã‚¹ã‚¿ã§å†ãƒãƒƒãƒ—", type="primary", key="drill_run_button"):
            if drilldown_target_id == "NONE":
                st.error("ã‚¨ãƒ©ãƒ¼: åˆ†æå¯¾è±¡ã‚¯ãƒ©ã‚¹ã‚¿ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
            else:
                with st.spinner(f"ã‚¯ãƒ©ã‚¹ã‚¿ {drilldown_target_id} ã®ãƒ‰ãƒªãƒ«ãƒ€ã‚¦ãƒ³ã‚’å®Ÿè¡Œä¸­..."):
                    try:
                        df_subset = df_main[df_main['cluster'] == drilldown_target_id].copy()
                        base_label = df_subset['cluster_label'].iloc[0]
                        
                        if not drill_date_bin_filter_w.startswith("(å…¨æœŸé–“)"):
                            try:
                                date_bin_label = drill_date_bin_filter_w.split(' (')[0].strip() 
                                start_year, end_year = map(int, date_bin_label.split('-'))
                                df_subset = df_subset[(df_subset['year'] >= start_year) & (df_subset['year'] <= end_year)]
                            except: pass 

                        drill_app_values = [val[1] for val in drill_applicant_filter_w]
                        if "ALL" not in drill_app_values:
                            mask_list_drill = [df_subset[col_map['applicant']].fillna('').str.contains(re.escape(app)) for app in drill_app_values]
                            df_subset = df_subset[pd.concat(mask_list_drill, axis=1).any(axis=1)]
                        
                        if len(df_subset) < 10:
                            st.warning(f"ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã™ãã¾ã™ ({len(df_subset)}ä»¶)ã€‚")
                        else:
                            subset_indices = df_subset.index
                            subset_tfidf = tfidf_matrix[subset_indices]
                            subset_sbert = sbert_embeddings[subset_indices]
                            subset_indices_pd = pd.Index(subset_indices)

                            n_neighbors = min(10, len(df_subset) - 1)
                            if n_neighbors < 2: n_neighbors = 2
                            
                            reducer_drill = UMAP(n_neighbors=n_neighbors, min_dist=0.1, n_components=2, random_state=42)
                            embedding_drill = reducer_drill.fit_transform(subset_sbert) 
                            df_subset['drill_x'] = embedding_drill[:, 0]
                            df_subset['drill_y'] = embedding_drill[:, 1]
                            
                            clusterer_drill = hdbscan.HDBSCAN(min_cluster_size=int(drill_min_cluster_size_w), min_samples=int(drill_min_samples_w), metric='euclidean', cluster_selection_method='eom')
                            df_subset['drill_cluster'] = clusterer_drill.fit_predict(embedding_drill)
                            
                            drill_labels_map = {}
                            for cid in sorted(df_subset['drill_cluster'].unique()):
                                if cid == -1:
                                    drill_labels_map[cid] = "ãƒã‚¤ã‚º"
                                    continue
                                idxs = df_subset[df_subset['drill_cluster'] == cid].index
                                tfidf_pos = [subset_indices_pd.get_loc(i) for i in idxs if i in subset_indices_pd]
                                if tfidf_pos:
                                    mean_vec = np.array(subset_tfidf[tfidf_pos].mean(axis=0)).flatten()
                                    top_idx = np.argsort(mean_vec)[::-1][:int(drill_label_top_n_w)]
                                    label = ", ".join([feature_names[i] for i in top_idx])
                                    drill_labels_map[cid] = f"[{cid}] {label}"
                            
                            df_subset['drill_cluster_label'] = df_subset['drill_cluster'].map(drill_labels_map)
                            df_subset = update_drill_hover_text(df_subset)
                            st.session_state.df_drilldown_result = df_subset.copy()
                            st.session_state.drill_labels_map = drill_labels_map.copy()
                            st.session_state.drill_labels_map_original = drill_labels_map.copy()
                            st.session_state.drill_base_label = base_label
                            st.success("å®Œäº†ã—ã¾ã—ãŸã€‚")
                            st.rerun()

                    except Exception as e:
                        st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

        if "df_drilldown_result" in st.session_state:
            df_drill = st.session_state.df_drilldown_result.copy()
            drill_labels_map = st.session_state.drill_labels_map
            
            st.subheader("ã‚µãƒ–ã‚¯ãƒ©ã‚¹ã‚¿ãƒ»ãƒ©ãƒ™ãƒ«ç·¨é›†")
            if "drill_labels_map_original" not in st.session_state:
                 st.session_state.drill_labels_map_original = drill_labels_map.copy()
            drill_label_widgets = _create_label_editor_ui(st.session_state.drill_labels_map_original, st.session_state.drill_labels_map, "drill_label")
            if st.button("ã‚µãƒ–ã‚¯ãƒ©ã‚¹ã‚¿ãƒ»ãƒ©ãƒ™ãƒ«ã‚’æ›´æ–°", key="drill_update_labels"):
                for cid, val in drill_label_widgets.items(): drill_labels_map[cid] = val
                df_drill['drill_cluster_label'] = df_drill['drill_cluster'].map(drill_labels_map)
                st.session_state.df_drilldown_result = update_drill_hover_text(df_drill)
                st.session_state.drill_labels_map = drill_labels_map
                st.rerun()

            st.subheader("ãƒ‰ãƒªãƒ«ãƒ€ã‚¦ãƒ³ãƒãƒƒãƒ—")
            fig_drill = go.Figure()
            fig_drill.add_trace(go.Scatter(
                x=df_drill['drill_x'], y=df_drill['drill_y'], mode='markers',
                marker=dict(color=df_drill['drill_cluster'], colorscale=theme_config["color_sequence"] if isinstance(theme_config["color_sequence"], str) else 'turbo', showscale=False, size=4, opacity=0.5),
                hoverinfo='text', hovertext=df_drill['drill_hover_text'], name='è¡¨ç¤ºå¯¾è±¡'
            ))
            annotations_drill = []
            if drill_show_labels_chk:
                for cid, grp in df_drill[df_drill['drill_cluster'] != -1].groupby('drill_cluster'):
                    mean_pos = grp[['drill_x', 'drill_y']].mean()
                    annotations_drill.append(go.layout.Annotation(
                        x=mean_pos['drill_x'], y=mean_pos['drill_y'], text=drill_labels_map.get(cid, ""), showarrow=False, font=dict(size=10, color=theme_config["text_color"])
                    ))
            fig_drill.update_layout(annotations=annotations_drill)
            update_fig_layout(fig_drill, f'Saturn V ãƒ‰ãƒªãƒ«ãƒ€ã‚¦ãƒ³: {st.session_state.drill_base_label}', theme_config=theme_config)
            fig_drill.update_xaxes(showgrid=False, zeroline=False, showticklabels=False)
            fig_drill.update_yaxes(showgrid=False, zeroline=False, showticklabels=False)
            st.plotly_chart(fig_drill, use_container_width=True)
            
            st.markdown("---")
            st.subheader("ã‚¯ãƒ©ã‚¹ã‚¿ãƒ»ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ (Text Mining)")
            col_tm1, col_tm2 = st.columns(2)
            with col_tm1:
                cooc_top_n = st.slider("å…±èµ·: ä¸Šä½å˜èªæ•°", 30, 100, 50, key="cooc_top_n")
                cooc_threshold = st.slider("å…±èµ·: Jaccardä¿‚æ•° é–¾å€¤", 0.01, 0.3, 0.05, 0.01, key="cooc_threshold")
            
            if st.button("ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã‚’å®Ÿè¡Œ", key="run_text_mining"):
                with st.spinner("åˆ†æä¸­..."):
                    all_text = ""
                    for _, row in df_drill.iterrows():
                        if col_map['title'] and pd.notna(row[col_map['title']]): all_text += row[col_map['title']] + " "
                        if col_map['abstract'] and pd.notna(row[col_map['abstract']]): all_text += row[col_map['abstract']] + " "
                    words = extract_compound_nouns(all_text)
                    
                    if not words: st.warning("æœ‰åŠ¹ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãªã—")
                    else:
                        st.markdown("##### 1. ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰")
                        generate_wordcloud_and_list(words, f"ã‚¯ãƒ©ã‚¹ã‚¿: {st.session_state.drill_base_label}", 30, font_path)
                        
                        st.markdown("##### 2. å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯")
                        word_freq = Counter(words)
                        top_words = [w for w, c in word_freq.most_common(cooc_top_n)]
                        pair_counts = Counter()
                        for _, row in df_drill.iterrows():
                            dt = ""
                            if col_map['title']: dt += str(row[col_map['title']]) + " "
                            if col_map['abstract']: dt += str(row[col_map['abstract']]) + " "
                            dw = set(extract_compound_nouns(dt))
                            dw = {w for w in dw if w in top_words}
                            if len(dw) >= 2:
                                for pair in combinations(sorted(list(dw)), 2): pair_counts[pair] += 1
                        
                        G = nx.Graph()
                        for w in top_words: G.add_node(w, count=word_freq[w])
                        for (w1, w2), c in pair_counts.items():
                            jac = c / (word_freq[w1] + word_freq[w2] - c)
                            if jac >= cooc_threshold: G.add_edge(w1, w2, weight=jac)
                        
                        G.remove_nodes_from(list(nx.isolates(G)))
                        if G.number_of_nodes() == 0: st.warning("å…±èµ·ãƒšã‚¢ãªã—")
                        else:
                            pos = nx.spring_layout(G, k=0.5, seed=42)
                            edge_x, edge_y = [], []
                            for edge in G.edges():
                                x0, y0 = pos[edge[0]]; x1, y1 = pos[edge[1]]
                                edge_x.extend([x0, x1, None]); edge_y.extend([y0, y1, None])
                            edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(width=0.5, color='#888'), hoverinfo='none', mode='lines')
                            
                            node_x, node_y, node_text, node_size = [], [], [], []
                            for node in G.nodes():
                                x, y = pos[node]; node_x.append(x); node_y.append(y)
                                c = G.nodes[node]['count']
                                node_text.append(f"{node} ({c})")
                                node_size.append(np.log(c+1)*10)
                            
                            node_trace = go.Scatter(
                                x=node_x, y=node_y, mode='markers+text', hoverinfo='text', text=list(G.nodes()), textposition="top center",
                                marker=dict(showscale=True, colorscale='YlGnBu', size=node_size, color=node_size, line_width=2)
                            )
                            fig_net = go.Figure(data=[edge_trace, node_trace], layout=go.Layout(title='å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯', showlegend=False, hovermode='closest', margin=dict(b=20,l=5,r=5,t=40), xaxis=dict(showgrid=False, zeroline=False, showticklabels=False), yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))
                            update_fig_layout(fig_net, 'å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯', theme_config=theme_config)
                            st.plotly_chart(fig_net, use_container_width=True)

    # --- C. ç‰¹è¨±ãƒãƒƒãƒ— (çµ±è¨ˆåˆ†æ) ---
    with tab_stats:
        st.subheader("ç‰¹è¨±ãƒãƒƒãƒ—ï¼ˆçµ±è¨ˆåˆ†æï¼‰")
        if st.session_state.saturnv_cluster_done:
            cluster_counts_stats = st.session_state.df_main['cluster_label'].value_counts()
            cluster_options_stats = [(f"(å…¨ã‚¯ãƒ©ã‚¹ã‚¿) ({len(st.session_state.df_main)}ä»¶)", "ALL")] + [
                (f"{st.session_state.saturnv_labels_map.get(cid)} ({cluster_counts_stats.get(st.session_state.saturnv_labels_map.get(cid), 0)}ä»¶)", cid)
                for cid in sorted(st.session_state.df_main['cluster'].unique())
            ]
            stats_cluster_filter_w = st.multiselect("é›†è¨ˆå¯¾è±¡ã‚¯ãƒ©ã‚¹ã‚¿:", cluster_options_stats, default=[cluster_options_stats[0]], format_func=lambda x: x[0], key="stats_cluster_filter")
            
            c1, c2 = st.columns(2)
            with c1:
                if 'stats_start_year' not in st.session_state: st.session_state.stats_start_year = 2010
                if 'stats_end_year' not in st.session_state: st.session_state.stats_end_year = 2024
                s_year = st.number_input('é–‹å§‹å¹´:', key="stats_start_year", step=1)
                e_year = st.number_input('çµ‚äº†å¹´:', key="stats_end_year", step=1)
            with c2:
                n_apps = st.number_input('è¡¨ç¤ºäººæ•°:', min_value=1, value=15, key="stats_num_assignees")
            
            if st.button("ç‰¹è¨±ãƒãƒƒãƒ—ã‚’æç”»", key="stats_run_button"):
                df_s = st.session_state.df_main.copy()
                # ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨
                vals = [v[1] for v in stats_cluster_filter_w]
                if "ALL" not in vals: df_s = df_s[df_s['cluster'].isin(vals)]
                df_s = df_s[(df_s['year'] >= s_year) & (df_s['year'] <= e_year)]
                
                if df_s.empty: st.warning("ãƒ‡ãƒ¼ã‚¿ãªã—")
                else:
                    # 1. æ™‚ç³»åˆ—
                    yc = df_s['year'].value_counts().sort_index().reindex(range(s_year, e_year+1), fill_value=0)
                    fig1 = px.bar(x=yc.index, y=yc.values, labels={'x':'å¹´', 'y':'ä»¶æ•°'}, color_discrete_sequence=[theme_config["color_sequence"][0]])
                    update_fig_layout(fig1, 'å‡ºé¡˜æ¨ç§»', theme_config=theme_config)
                    st.plotly_chart(fig1, use_container_width=True)
                    
                    # 2. ãƒ©ãƒ³ã‚­ãƒ³ã‚°
                    ac = df_s[col_map['applicant']].astype(str).str.split(delimiters['applicant']).explode().str.strip().value_counts().head(n_apps).sort_values(ascending=True)
                    fig2 = px.bar(x=ac.values, y=ac.index, orientation='h', labels={'x':'ä»¶æ•°', 'y':'æ¨©åˆ©è€…'}, color_discrete_sequence=[theme_config["color_sequence"][1]])
                    update_fig_layout(fig2, 'æ¨©åˆ©è€…ãƒ©ãƒ³ã‚­ãƒ³ã‚°', height=max(600, len(ac)*30), theme_config=theme_config)
                    st.plotly_chart(fig2, use_container_width=True)
                    
                    # 3. ãƒãƒ–ãƒ«
                    ae = df_s.explode(col_map['applicant'])
                    ae['ap'] = ae[col_map['applicant']].astype(str).str.strip()
                    top_a = ae['ap'].value_counts().head(n_apps).index.tolist()
                    pd_plot = ae[ae['ap'].isin(top_a)].groupby(['year', 'ap']).size().reset_index(name='count')
                    if not pd_plot.empty:
                        fig3 = px.scatter(pd_plot, x='year', y='ap', size='count', color='ap', labels={'year':'å¹´', 'ap':'æ¨©åˆ©è€…', 'count':'ä»¶æ•°'}, category_orders={'ap': top_a})
                        update_fig_layout(fig3, 'å‡ºé¡˜å¹´åˆ¥å‹•å‘', height=700, theme_config=theme_config)
                        st.plotly_chart(fig3, use_container_width=True)

    # --- D. ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ ---
    with tab_export:
        st.subheader("ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
        if st.session_state.saturnv_cluster_done:
            cols_drop = ['hover_text', 'parsed_date', 'drill_cluster', 'drill_cluster_label', 'drill_hover_text', 'drill_x', 'drill_y', 'temp_date_bin']
            csv = df_main.drop(columns=cols_drop, errors='ignore').to_csv(encoding='utf-8-sig', index=False).encode('utf-8-sig')
            st.download_button("ãƒ¡ã‚¤ãƒ³ãƒãƒƒãƒ—å…¨ãƒ‡ãƒ¼ã‚¿ (CSV)", csv, "APOLLO_SaturnV_Main.csv", "text/csv")
        
        if "df_drilldown_result" in st.session_state:
            cols_drop_d = ['hover_text', 'parsed_date', 'date_bin', 'drill_hover_text', 'drill_date_bin', 'temp_date_bin']
            csv_d = st.session_state.df_drilldown_result.drop(columns=cols_drop_d, errors='ignore').to_csv(encoding='utf-8-sig', index=False).encode('utf-8-sig')
            st.download_button("ãƒ‰ãƒªãƒ«ãƒ€ã‚¦ãƒ³çµæœ (CSV)", csv_d, "APOLLO_SaturnV_Drill.csv", "text/csv")

# --- å…±é€šã‚µã‚¤ãƒ‰ãƒãƒ¼ãƒ•ãƒƒã‚¿ãƒ¼ ---
st.sidebar.markdown("---") 
st.sidebar.caption("ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³:")
st.sidebar.caption("1. Mission Control ã§ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€å‰å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
st.sidebar.caption("2. å·¦ã®ãƒªã‚¹ãƒˆã‹ã‚‰åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é¸æŠã—ã¾ã™ã€‚")
st.sidebar.markdown("---")
st.sidebar.caption("Â© 2025 ã—ã°ã‚„ã¾")