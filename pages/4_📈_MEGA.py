import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
import datetime
import warnings
import io
import unicodedata
import re
import platform
import os
import string
from collections import Counter
from itertools import combinations

# æ©Ÿæ¢°å­¦ç¿’ãƒ»è‡ªç„¶è¨€èªå‡¦ç†
from umap import UMAP
import hdbscan
from wordcloud import WordCloud
from janome.tokenizer import Tokenizer
import networkx as nx
from scipy.spatial import ConvexHull

# æç”»ç”¨
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import japanize_matplotlib

# è­¦å‘Šã‚’éè¡¨ç¤º
warnings.filterwarnings('ignore')

# ==================================================================
# --- 1. ãƒ•ã‚©ãƒ³ãƒˆè¨­å®š ---
# ==================================================================
def get_japanese_font_path():
    """OSã‚’åˆ¤å®šã—ã¦é©åˆ‡ãªæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãƒ‘ã‚¹ã‚’è¿”ã™"""
    system = platform.system()
    font_paths = []
    
    if system == "Darwin": # Mac
        font_paths = [
            "/System/Library/Fonts/ãƒ’ãƒ©ã‚®ãƒè§’ã‚´ã‚·ãƒƒã‚¯ W3.ttc",
            "/System/Library/Fonts/Hiragino Sans W3.ttc",
            "/System/Library/Fonts/Hiragino Kaku Gothic ProN.ttc",
            "/Library/Fonts/AppleGothic.ttf",
            "/System/Library/Fonts/AppleSDGothicNeo.ttc" 
        ]
    elif system == "Windows": # Windows
        font_paths = [
            "C:/Windows/Fonts/meiryo.ttc",
            "C:/Windows/Fonts/msgothic.ttc",
            "C:/Windows/Fonts/yugothr.ttc",
            "C:/Windows/Fonts/YuGothR.ttc"
        ]
    else: # Linux (Streamlit Cloudãªã©)
        font_paths = [
            "/usr/share/fonts/opentype/ipafont-gothic/ipagp.ttf",
            "/usr/share/fonts/truetype/fonts-japanese-gothic.ttf",
            "/usr/share/fonts/noto/NotoSansCJK-Regular.ttc",
            "/usr/share/fonts/noto/NotoSansCJKjp-Regular.otf"
        ]
        
    for path in font_paths:
        if os.path.exists(path): return path
    return None

FONT_PATH = get_japanese_font_path()
if FONT_PATH:
    try:
        prop = fm.FontProperties(fname=FONT_PATH)
        plt.rcParams['font.family'] = prop.get_name()
    except:
        pass

# ==================================================================
# --- 2. ãƒšãƒ¼ã‚¸è¨­å®š ---
# ==================================================================
st.set_page_config(
    page_title="APOLLO | MEGA",
    page_icon="ğŸ“ˆ",
    layout="wide"
)

# ==================================================================
# --- 3. ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ãƒ»ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ ---
# ==================================================================
@st.cache_resource
def load_tokenizer_mega():
    return Tokenizer()

t = load_tokenizer_mega()

# ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰å®šç¾©
_stopwords_original_list = [
    "ã™ã‚‹","ã‚ã‚‹","ãªã‚‹","ãŸã‚","ã“ã¨","ã‚ˆã†","ã‚‚ã®","ã“ã‚Œ","ãã‚Œ","ã‚ã‚Œ","ã“ã“","ãã“","ã©ã‚Œ","ã©ã®",
    "ã“ã®","ãã®","å½“è©²","è©²","ãŠã‚ˆã³","åŠã³","ã¾ãŸã¯","ã¾ãŸ","ä¾‹ãˆã°","ä¾‹ãˆã°ã¯","ã«ãŠã„ã¦","ã«ã‚ˆã‚Š",
    "ã«å¯¾ã—ã¦","ã«é–¢ã—ã¦","ã«ã¤ã„ã¦","ã¨ã—ã¦","ã¨ã—ã¦ã¯","å ´åˆ","ä¸€æ–¹","ä»–æ–¹","ã•ã‚‰ã«","ãã—ã¦","ãŸã ã—",
    "ãªãŠ","ç­‰","ãªã©","ç­‰ã€…","ã„ã‚ã‚†ã‚‹","æ‰€è¬‚","åŒæ§˜","åŒæ™‚","å‰è¨˜","æœ¬","åŒ","å„","å„ç¨®","æ‰€å®š","æ‰€æœ›",
    "ä¸€ä¾‹","ä»–","ä¸€éƒ¨","ä¸€ã¤","è¤‡æ•°","å°‘ãªãã¨ã‚‚","å°‘ãªãã¨ã‚‚ä¸€ã¤","ä¸Šè¨˜","ä¸‹è¨˜","å‰è¿°","å¾Œè¿°","æ—¢è¿°",
    "é–¢ã™ã‚‹","åŸºã¥ã","ç”¨ã„ã‚‹","ä½¿ç”¨","åˆ©ç”¨","æœ‰ã™ã‚‹","å«ã‚€","å‚™ãˆã‚‹","è¨­ã‘ã‚‹","ã™ãªã‚ã¡","å¾“ã£ã¦",
    "ã—ã‹ã—ãªãŒã‚‰","æ¬¡ã«","ç‰¹ã«","å…·ä½“çš„ã«","è©³ç´°ã«","ã„ãšã‚Œ","ã†ã¡","ãã‚Œãã‚Œ","ã¨ã",
    "ã‹ã‹ã‚‹","ã‹ã‚ˆã†ãª","ã‹ã‹ã‚‹å ´åˆ","æœ¬ä»¶","æœ¬é¡˜","æœ¬å‡ºé¡˜","æœ¬æ˜ç´°æ›¸","ã“ã‚Œã‚‰","ãã‚Œã‚‰","å„ã€…","éšæ™‚","é©å®œ",
    "ä»»æ„","å¿…ãšã—ã‚‚","é€šå¸¸","ä¸€èˆ¬ã«","å…¸å‹çš„","ä»£è¡¨çš„",
    "æœ¬ç™ºæ˜","ç™ºæ˜","å®Ÿæ–½ä¾‹","å®Ÿæ–½å½¢æ…‹","å¤‰å½¢ä¾‹","è«‹æ±‚","è«‹æ±‚é …","å›³","å›³é¢","ç¬¦å·","ç¬¦å·ã®èª¬æ˜",
    "å›³é¢ã®ç°¡å˜ãªèª¬æ˜","ç™ºæ˜ã®è©³ç´°ãªèª¬æ˜","æŠ€è¡“åˆ†é‡","èƒŒæ™¯æŠ€è¡“","å¾“æ¥æŠ€è¡“","ç™ºæ˜ãŒè§£æ±ºã—ã‚ˆã†ã¨ã™ã‚‹èª²é¡Œ","èª²é¡Œ",
    "è§£æ±ºæ‰‹æ®µ","åŠ¹æœ","è¦ç´„","ç™ºæ˜ã®åŠ¹æœ","ç›®çš„","æ‰‹æ®µ","æ§‹æˆ","æ§‹é€ ","å·¥ç¨‹","å‡¦ç†","æ–¹æ³•","æ‰‹æ³•","æ–¹å¼",
    "ã‚·ã‚¹ãƒ†ãƒ ","ãƒ—ãƒ­ã‚°ãƒ©ãƒ ","è¨˜æ†¶åª’ä½“","ç‰¹å¾´","ç‰¹å¾´ã¨ã™ã‚‹","ç‰¹å¾´éƒ¨","ã‚¹ãƒ†ãƒƒãƒ—","ãƒ•ãƒ­ãƒ¼","ã‚·ãƒ¼ã‚±ãƒ³ã‚¹","å®šç¾©",
    "é–¢ä¿‚","å¯¾å¿œ","æ•´åˆ","å®Ÿæ–½ã®å½¢æ…‹","å®Ÿæ–½ã®æ…‹æ§˜","æ…‹æ§˜","å¤‰å½¢","ä¿®æ­£ä¾‹","å›³ç¤º","å›³ç¤ºä¾‹","å›³ç¤ºã—ãªã„",
    "å‚ç…§","å‚ç…§ç¬¦å·","æ®µè½","è©³ç´°èª¬æ˜","è¦æ—¨","ä¸€å®Ÿæ–½å½¢æ…‹","ä»–ã®å®Ÿæ–½å½¢æ…‹","ä¸€å®Ÿæ–½ä¾‹","åˆ¥ã®å´é¢","ä»˜è¨˜",
    "é©ç”¨ä¾‹","ç”¨èªã®å®šç¾©","é–‹ç¤º","æœ¬é–‹ç¤º","é–‹ç¤ºå†…å®¹","ä¸Šéƒ¨","ä¸‹éƒ¨","å†…éƒ¨","å¤–éƒ¨","å†…å´","å¤–å´","è¡¨é¢",
    "è£é¢","å´é¢","ä¸Šé¢","ä¸‹é¢","ç«¯é¢","å…ˆç«¯","åŸºç«¯","å¾Œç«¯","ä¸€ç«¯","ä»–ç«¯","ä¸­å¿ƒ","ä¸­å¤®","å‘¨ç¸","å‘¨è¾º",
    "è¿‘å‚","æ–¹å‘","ä½ç½®","ç©ºé–“","é ˜åŸŸ","ç¯„å›²","é–“éš”","è·é›¢","å½¢çŠ¶","å½¢æ…‹","çŠ¶æ…‹","ç¨®é¡","å±¤","è†œ","éƒ¨",
    "éƒ¨æ","éƒ¨ä½","éƒ¨å“","æ©Ÿæ§‹","è£…ç½®","å®¹å™¨","çµ„æˆ","ææ–™","ç”¨é€”","é©ç”¨","é©ç”¨ä¾‹","ç‰‡å´","ä¸¡å´","å·¦å´",
    "å³å´","å‰æ–¹","å¾Œæ–¹","ä¸Šæµ","ä¸‹æµ","éš£æ¥","è¿‘æ¥","é›¢é–“","é–“ç½®","ä»‹åœ¨","é‡ç•³","æ¦‚ã­","ç•¥","ç•¥ä¸­å¤®",
    "å›ºå®šå´","å¯å‹•å´","ä¼¸é•·","åç¸®","ä¿‚åˆ","åµŒåˆ","å–ä»˜","é€£çµéƒ¨","æ”¯æŒä½“","æ”¯æŒéƒ¨","ã‚¬ã‚¤ãƒ‰éƒ¨",
    "ãƒ‡ãƒ¼ã‚¿","æƒ…å ±","ä¿¡å·","å‡ºåŠ›","å…¥åŠ›","åˆ¶å¾¡","æ¼”ç®—","å–å¾—","é€ä¿¡","å—ä¿¡","è¡¨ç¤º","é€šçŸ¥","è¨­å®š","å¤‰æ›´",
    "æ›´æ–°","ä¿å­˜","å‰Šé™¤","è¿½åŠ ","å®Ÿè¡Œ","é–‹å§‹","çµ‚äº†","ç¶™ç¶š","åœæ­¢","åˆ¤å®š","åˆ¤æ–­","æ±ºå®š","é¸æŠ","ç‰¹å®š",
    "æŠ½å‡º","æ¤œå‡º","æ¤œçŸ¥","æ¸¬å®š","è¨ˆæ¸¬","ç§»å‹•","å›è»¢","å¤‰ä½","å¤‰å½¢","å›ºå®š","é…ç½®","ç”Ÿæˆ","ä»˜ä¸","ä¾›çµ¦",
    "é©ç”¨","ç…§åˆ","æ¯”è¼ƒ","ç®—å‡º","è§£æ","åŒå®š","åˆæœŸåŒ–","èª­å‡º","æ›¸è¾¼","ç™»éŒ²","è¨˜éŒ²","é…ä¿¡","é€£æº","åˆ‡æ›¿",
    "èµ·å‹•","å¾©å¸°","ç›£è¦–","é€šçŸ¥å‡¦ç†","å–å¾—å‡¦ç†","æ¼”ç®—å‡¦ç†","è‰¯å¥½","å®¹æ˜“","ç°¡ä¾¿","é©åˆ‡","æœ‰åˆ©","æœ‰ç”¨","æœ‰åŠ¹",
    "åŠ¹æœçš„","é«˜ã„","ä½ã„","å¤§ãã„","å°ã•ã„","æ–°è¦","æ”¹è‰¯","æ”¹å–„","æŠ‘åˆ¶","å‘ä¸Š","ä½æ¸›","å‰Šæ¸›","å¢—åŠ ",
    "æ¸›å°‘","å¯èƒ½","å¥½é©","å¥½ã¾ã—ã„","æœ›ã¾ã—ã„","å„ªã‚Œã‚‹","å„ªã‚ŒãŸ","é«˜æ€§èƒ½","é«˜åŠ¹ç‡","ä½ã‚³ã‚¹ãƒˆ","ã‚³ã‚¹ãƒˆ",
    "ç°¡æ˜“","å®‰å®š","å®‰å®šæ€§","è€ä¹…","è€ä¹…æ€§","ä¿¡é ¼æ€§","ç°¡ç´ ","ç°¡ç•¥","å˜ç´”","æœ€é©","æœ€é©åŒ–","æ±ç”¨","æ±ç”¨æ€§",
    "å®Ÿç¾","é”æˆ","ç¢ºä¿","ç¶­æŒ","é˜²æ­¢","å›é¿","ä¿ƒé€²","ä¸è¦","å¿…è¦","é«˜ç²¾åº¦","çœé›»åŠ›","çœè³‡æº","é«˜ä¿¡é ¼",
    "ä½è² è·","é«˜ç´”åº¦","é«˜å¯†åº¦","é«˜æ„Ÿåº¦","è¿…é€Ÿ","å††æ»‘","ç°¡ç•¥åŒ–","ä½ä¾¡æ ¼","å®ŸåŠ¹çš„","å¯èƒ½åŒ–","æœ‰åŠ¹åŒ–",
    "éå¿…é ˆ","é©åˆ","äº’æ›","å‡ºé¡˜","å‡ºé¡˜äºº","å‡ºé¡˜ç•ªå·","å‡ºé¡˜æ—¥","å‡ºé¡˜æ›¸","å‡ºé¡˜å…¬é–‹","å…¬é–‹","å…¬é–‹ç•ªå·",
    "å…¬é–‹å…¬å ±","å…¬å ±","å…¬å ±ç•ªå·","ç‰¹è¨±","ç‰¹è¨±ç•ªå·","ç‰¹è¨±æ–‡çŒ®","éç‰¹è¨±æ–‡çŒ®","å¼•ç”¨","å¼•ç”¨æ–‡çŒ®","å…ˆè¡ŒæŠ€è¡“",
    "å¯©æŸ»","å¯©æŸ»å®˜","æ‹’çµ¶","æ„è¦‹æ›¸","è£œæ­£æ›¸","å„ªå…ˆ","å„ªå…ˆæ—¥","åˆ†å‰²å‡ºé¡˜","ç¶™ç¶šå‡ºé¡˜","å›½å†…ç§»è¡Œ","å›½éš›å‡ºé¡˜",
    "å›½éš›å…¬é–‹","PCT","ç™»éŒ²","å…¬é–‹æ—¥","å¯©æŸ»è«‹æ±‚","æ‹’çµ¶ç†ç”±","è£œæ­£","è¨‚æ­£","ç„¡åŠ¹å¯©åˆ¤","ç•°è­°","å–æ¶ˆ","å–ä¸‹ã’",
    "äº‹ä»¶ç•ªå·","ä»£ç†äºº","å¼ç†å£«","ä¿‚å±","çµŒé",
    "ç¬¬","ç¬¬ä¸€","ç¬¬äºŒ","ç¬¬ä¸‰","ç¬¬1","ç¬¬ï¼’","ç¬¬ï¼“","ç¬¬ï¼‘","ç¬¬ï¼’","ç¬¬ï¼“","ä¸€","äºŒ","ä¸‰","å››","äº”","å…­","ä¸ƒ","å…«","ä¹","é›¶","æ•°","è¤‡åˆ","å¤šæ•°","å°‘æ•°","å›³1","å›³2","å›³3","å›³4","å›³5","å›³6","å›³7","å›³8","å›³9","è¡¨1","è¡¨2","è¡¨3","å¼1","å¼2","å¼3","%","ï¼…","wt%","vol%","è³ªé‡%","é‡é‡%","å®¹é‡%","mol","mol%","mol/L","M","mm","cm","m","nm","Î¼m","Î¼","rpm","Pa","kPa","MPa","GPa","N","W","V","A","mA","Hz","kHz","MHz","GHz","â„ƒ","Â°C","K","mL","L","g","kg","mg","wt","vol","h","hr","hrs","min","s","sec","ppm","ppb","bar","Î©","ohm","J","kJ","Wh","kWh",
    "æ ªå¼ä¼šç¤¾","æœ‰é™ä¼šç¤¾","åˆè³‡ä¼šç¤¾","åˆåä¼šç¤¾","åˆåŒä¼šç¤¾","Inc","Inc.","Ltd","Ltd.","Co","Co.","Corp","Corp.","LLC",
    "GmbH","AG","BV","B.V.","S.A.","S.p.A.","ï¼ˆæ ªï¼‰","ãˆ±","ï¼ˆæœ‰ï¼‰",
    "æº¶æ¶²","æº¶åª’","è§¦åª’","åå¿œ","ç”Ÿæˆç‰©","åŸæ–™","æˆåˆ†","å«æœ‰","å«æœ‰é‡","é…åˆ","æ··åˆ","æ··åˆç‰©","æ¿ƒåº¦","æ¸©åº¦","æ™‚é–“",
    "å‰²åˆ","æ¯”ç‡","åŸº","å®˜èƒ½åŸº","åŒ–åˆç‰©","çµ„æˆç‰©","æ¨¹è„‚","ãƒãƒªãƒãƒ¼","ãƒ¢ãƒãƒãƒ¼","åŸºæ¿","åŸºæ","ãƒ•ã‚£ãƒ«ãƒ ","ã‚·ãƒ¼ãƒˆ",
    "ç²’å­","ç²‰æœ«","æ¯”è¼ƒä¾‹","å‚è€ƒä¾‹","è©¦é¨“","è©¦æ–™","è©•ä¾¡","æ¡ä»¶","å®Ÿé¨“","å®Ÿé¨“ä¾‹","åå¿œæ¡ä»¶","åå¿œæ™‚é–“","åå¿œæ¸©åº¦",
    "å‡¦ç†è£…ç½®","ç«¯æœ«","ãƒ¦ãƒ‹ãƒƒãƒˆ","ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«","å›è·¯","ç´ å­","é›»æº","é›»åœ§","é›»æµ","ä¿¡å·ç·š","é…ç·š","ç«¯å­","ç«¯éƒ¨","æ¥ç¶š",
    "æ¥ç¶šéƒ¨","æ¼”ç®—éƒ¨","è¨˜æ†¶éƒ¨","è¨˜æ†¶è£…ç½®","è¨˜éŒ²åª’ä½“","ãƒ¦ãƒ¼ã‚¶","åˆ©ç”¨è€…","ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ","ã‚µãƒ¼ãƒ","ç”»é¢","UI","GUI",
    "ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹","ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹","DB","ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯","é€šä¿¡","è¦æ±‚","å¿œç­”","ãƒªã‚¯ã‚¨ã‚¹ãƒˆ","ãƒ¬ã‚¹ãƒãƒ³ã‚¹","ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿",
    "å¼•æ•°","å±æ€§","ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£","ãƒ•ãƒ©ã‚°","ID","ãƒ•ã‚¡ã‚¤ãƒ«","ãƒ‡ãƒ¼ã‚¿æ§‹é€ ","ãƒ†ãƒ¼ãƒ–ãƒ«","ãƒ¬ã‚³ãƒ¼ãƒ‰",
    "è»¸","ã‚·ãƒ£ãƒ•ãƒˆ","ã‚®ã‚¢","ãƒ¢ãƒ¼ã‚¿","ã‚¨ãƒ³ã‚¸ãƒ³","ã‚¢ã‚¯ãƒãƒ¥ã‚¨ãƒ¼ã‚¿","ã‚»ãƒ³ã‚µ","ãƒãƒ«ãƒ–","ãƒãƒ³ãƒ—","ç­ä½“","ãƒã‚¦ã‚¸ãƒ³ã‚°","ãƒ•ãƒ¬ãƒ¼ãƒ ",
    "ã‚·ãƒ£ãƒ¼ã‚·","é§†å‹•","ä¼é”","æ”¯æŒ","é€£çµ","è§£æ±º", "æº–å‚™", "æä¾›", "ç™ºç”Ÿ", "ä»¥ä¸Š", "ååˆ†",
    "ã§ãã‚‹", "ã„ã‚‹", "æ˜ç´°æ›¸", "è¨˜è¼‰", "è¨˜è¿°", "æ²è¼‰", "è¨€åŠ", "å†…å®¹", "è©³ç´°", "èª¬æ˜", "è¡¨è¨˜", "è¡¨ç¾", "ç®‡æ¡æ›¸ã", "ä»¥ä¸‹ã®", "ä»¥ä¸Šã®", "å…¨ã¦ã®", "ä»»æ„ã®", "ç‰¹å®šã®"
]

@st.cache_data
def expand_stopwords_to_full_width(words):
    expanded = set(words)
    hankaku = string.ascii_letters + string.digits
    zenkaku = "ï½ï½‚ï½ƒï½„ï½…ï½†ï½‡ï½ˆï½‰ï½Šï½‹ï½Œï½ï½ï½ï½ï½‘ï½’ï½“ï½”ï½•ï½–ï½—ï½˜ï½™ï½šï¼¡ï¼¢ï¼£ï¼¤ï¼¥ï¼¦ï¼§ï¼¨ï¼©ï¼ªï¼«ï¼¬ï¼­ï¼®ï¼¯ï¼°ï¼±ï¼²ï¼³ï¼´ï¼µï¼¶ï¼·ï¼¸ï¼¹ï¼ºï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™"
    trans = str.maketrans(hankaku, zenkaku)
    for w in words:
        if any(c in hankaku for c in w): expanded.add(w.translate(trans))
    return sorted(list(expanded))

stopwords = set(expand_stopwords_to_full_width(_stopwords_original_list))

_ngram_rows = [
    ("å‚ç…§ç¬¦å·ä»˜ãè¦ç´ ", r"[ä¸€-é¾¥ã-ã‚“ã‚¡-ãƒ³A-Za-z0-9ï¼\-ï¼‹ãƒ»]+?(?:éƒ¨|å±¤|é¢|ä½“|æ¿|å­”|æº|ç‰‡|éƒ¨æ|è¦ç´ |æ©Ÿæ§‹|è£…ç½®|æ‰‹æ®µ|é›»æ¥µ|ç«¯å­|é ˜åŸŸ|åŸºæ¿|å›è·¯|ææ–™|å·¥ç¨‹)\s*[ï¼ˆ(]\s*[0-9ï¼-ï¼™A-Za-z]+[A-Za-z]?\s*[ï¼‰)]", "regex", 1),
    ("å‚ç…§ç¬¦å·ä»˜ãè¦ç´ ", r"(?:ä¸Šè¨˜|å‰è¨˜)?[ä¸€-é¾¥ã-ã‚“ã‚¡-ãƒ³A-Za-z0-9ï¼\-ï¼‹ãƒ»]+?(?:éƒ¨|å±¤|é¢|ä½“|æ¿|å­”|æº|ç‰‡|éƒ¨æ|è¦ç´ |æ©Ÿæ§‹|è£…ç½®|æ‰‹æ®µ|é›»æ¥µ|ç«¯å­|é ˜åŸŸ|åŸºæ¿|å›è·¯|ææ–™|å·¥ç¨‹)\s*[0-9ï¼-ï¼™A-Za-z]+[A-Za-z]?", "regex", 1),
    ("å‚ç…§ç¬¦å·ä»˜ãè¦ç´ ", r"[A-Z]+[0-9]+", "regex", 1),
    ("è¦‹å‡ºã—ãƒ»ç« å¥","ä¸€å®Ÿæ–½å½¢æ…‹ã«ãŠã„ã¦","literal",1), ("è¦‹å‡ºã—ãƒ»ç« å¥","ä»–ã®å®Ÿæ–½å½¢æ…‹ã«ãŠã„ã¦","literal",1), ("è¦‹å‡ºã—ãƒ»ç« å¥","åˆ¥ã®å®Ÿæ–½å½¢æ…‹ã«ãŠã„ã¦","literal",1),
    ("è¦‹å‡ºã—ãƒ»ç« å¥","æœ¬æ˜ç´°æ›¸ã«ãŠã„ã¦","literal",1), ("è¦‹å‡ºã—ãƒ»ç« å¥","æœ¬æ˜ç´°æ›¸ã§ã¯","literal",1), ("è¦‹å‡ºã—ãƒ»ç« å¥","æœ¬ç™ºæ˜ã®ä¸€å´é¢","literal",1),
    ("è¦‹å‡ºã—ãƒ»ç« å¥","ä¸€å®Ÿæ–½ä¾‹ã«ãŠã„ã¦","literal",1), ("è¦‹å‡ºã—ãƒ»ç« å¥","ä»–ã®å®Ÿæ–½ä¾‹ã«ãŠã„ã¦","literal",1), ("è¦‹å‡ºã—ãƒ»ç« å¥","å¥½ã¾ã—ã„æ…‹æ§˜ã¨ã—ã¦","literal",2),
    ("è¦‹å‡ºã—ãƒ»ç« å¥","å¥½é©ã«ã¯","literal",2), ("è¦‹å‡ºã—ãƒ»ç« å¥","ç”¨èªã®å®šç¾©","literal",2), ("è¦‹å‡ºã—ãƒ»ç« å¥","å›³ç¤ºã—ãªã„","literal",2),
    ("å›³è¡¨å‚ç…§", r"å›³[ ã€€]*[ï¼-ï¼™0-9]+ã«ç¤ºã™", "regex", 1), ("å›³è¡¨å‚ç…§", r"è¡¨[ ã€€]*[ï¼-ï¼™0-9]+ã«ç¤ºã™", "regex", 1),
    ("å›³è¡¨å‚ç…§", r"å¼[ ã€€]*[ï¼-ï¼™0-9]+ã«ç¤ºã™", "regex", 1), ("å›³è¡¨å‚ç…§", r"è«‹æ±‚é …[ ã€€]*[ï¼-ï¼™0-9]+", "regex", 1),
    ("å›³è¡¨å‚ç…§", r"(?:ã€|\[)\s*[ï¼-ï¼™0-9]{4,5}\s*(?:ã€‘|\])", "regex", 1), ("å›³è¡¨å‚ç…§", r"[ï¼ˆ(][ï¼-ï¼™0-9]+[ï¼‰)]", "regex", 2),
    ("å›³è¡¨å‚ç…§", r"ç¬¬\s*[ï¼-ï¼™0-9]+ã®?å®Ÿæ–½å½¢æ…‹", "regex", 2), ("å›³è¡¨å‚ç…§", r"æ®µè½\s*[ï¼-ï¼™0-9]+", "regex", 2),
    ("å›³è¡¨å‚ç…§", r"å›³[ ã€€]*[ï¼-ï¼™0-9]+[A-Za-z]?", "regex", 2), ("å®šç¾©å°å…¥", r"ä»¥ä¸‹ã€[^ã€ã€‚]+ã‚’[^ã€ã€‚]+ã¨ç§°ã™ã‚‹", "regex", 1),
    ("å®šç¾©å°å…¥", r"ä»¥ä¸‹ã€[^ã€ã€‚]+ã‚’[^ã€ã€‚]+ã¨ã„ã†", "regex", 1), ("æ©Ÿèƒ½å¥","ã—ã¦ã‚‚ã‚ˆã„","literal",1), ("æ©Ÿèƒ½å¥","ã§ã‚ã£ã¦ã‚‚ã‚ˆã„","literal",1),
    ("æ©Ÿèƒ½å¥","ã™ã‚‹ã“ã¨ãŒã§ãã‚‹","literal",1), ("æ©Ÿèƒ½å¥","è¡Œã†ã“ã¨ãŒã§ãã‚‹","literal",1), ("æ©Ÿèƒ½å¥","ã«é™å®šã•ã‚Œãªã„","literal",1),
    ("æ©Ÿèƒ½å¥","ã«é™ã‚‰ã‚Œãªã„","literal",1), ("æ©Ÿèƒ½å¥","ä¸€ä¾‹ã¨ã—ã¦","literal",2), ("æ©Ÿèƒ½å¥","ä¾‹ç¤ºçš„ã«ã¯","literal",2),
    ("å‚ç…§å¥","å‰è¿°ã®ã¨ãŠã‚Š","literal",2), ("å‚ç…§å¥","å‰è¿°ã®é€šã‚Š","literal",2), ("å‚ç…§å¥","å¾Œè¿°ã™ã‚‹ã‚ˆã†ã«","literal",2),
    ("å‚ç…§å¥","å¾Œè¿°ã®ã¨ãŠã‚Š","literal",2), ("ç¯„å›²è¡¨ç¾", r"å°‘ãªãã¨ã‚‚(?:ä¸€|ï¼‘)ã¤", "regex", 2), ("ç¯„å›²è¡¨ç¾", "å°‘ãªãã¨ã‚‚ä¸€éƒ¨", "literal", 2),
    ("ç¯„å›²è¡¨ç¾", r"è¤‡æ•°ã®(?:å®Ÿæ–½å½¢æ…‹|æ§‹æˆ|è¦ç´ )", "regex", 3), ("èª²é¡Œå¥", r"(?:ä¸Šè¨˜|å‰è¨˜)ã®?èª²é¡Œ", "regex", 1),
    ("æ¥ç¶šãƒ»è«–ç†","ä¸€æ–¹ã§","literal",3), ("æ¥ç¶šãƒ»è«–ç†","ä»–æ–¹ã§","literal",3), ("æ¥ç¶šãƒ»è«–ç†","ã™ãªã‚ã¡","literal",3),
    ("æ¥ç¶šãƒ»è«–ç†","ã—ãŸãŒã£ã¦","literal",3), ("æ¥ç¶šãƒ»è«–ç†","ã—ã‹ã—ãªãŒã‚‰","literal",3), ("æ¥ç¶šãƒ»è«–ç†","ä¾‹ãˆã°","literal",3),
    ("æ¥ç¶šãƒ»è«–ç†","å…·ä½“çš„ã«ã¯","literal",3), ("è£œåŠ©å¥","ä»¥ä¸‹ã«èª¬æ˜ã™ã‚‹","literal",3), ("è£œåŠ©å¥","å‰è¨˜ã®ã¨ãŠã‚Š","literal",3),
    ("è£œåŠ©å¥","ã“ã‚Œã«ã‚ˆã‚Š","literal",3), ("è£œåŠ©å¥","ã“ã®ã‚ˆã†ã«","literal",3)
]

_ngram_compiled = sorted(_ngram_rows, key=lambda x: (x[3], -len(x[1]) if x[2]=="literal" else -50))
_ngram_compiled = [(cat, re.compile(pat) if ptype == "regex" else pat, ptype, pri) for cat, pat, ptype, pri in _ngram_compiled]

def normalize_text(text):
    if not isinstance(text, str): text = "" if pd.isna(text) else str(text)
    text = unicodedata.normalize("NFKC", text)
    text = text.replace("Âµ", "Î¼")
    text = re.sub(r"\s+", " ", text)
    return text

def apply_ngram_filters(text):
    for cat, pat, ptype, pri in _ngram_compiled:
        if ptype == "literal":
            if pat in text: text = text.replace(pat, "")
        else:
            text = pat.sub("", text)
    return text

@st.cache_data
def extract_compound_nouns(text):
    text = normalize_text(text)
    text = apply_ngram_filters(text)
    text = re.sub(r'ã€.*?ã€‘', '', text)
    text = re.sub(r'[!\"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~]', ' ', text)

    tokens = t.tokenize(text)
    words, compound_word = [], ''
    for token in tokens:
        pos = token.part_of_speech.split(',')[0]
        if pos == 'åè©':
            compound_word += token.surface
        else:
            if (len(compound_word) > 1 and
                compound_word not in stopwords and
                not re.fullmatch(r'[\dï¼-ï¼™]+', compound_word) and
                not re.fullmatch(r'(å›³|è¡¨|å¼|ç¬¬)[\dï¼-ï¼™]+.*', compound_word) and
                not re.match(r'^(ä¸Šè¨˜|å‰è¨˜|æœ¬é–‹ç¤º|å½“è©²|è©²)', compound_word) and
                not re.search(r'[0-9ï¼-ï¼™]+[)ï¼‰]?$', compound_word) and
                not re.match(r'[0-9ï¼-ï¼™]+[a-zA-Zï½-ï½šï¼¡-ï¼º]', compound_word)):
                words.append(compound_word)
            compound_word = ''
            
    if (len(compound_word) > 1 and
        compound_word not in stopwords and
        not re.fullmatch(r'[\dï¼-ï¼™]+', compound_word) and
        not re.fullmatch(r'(å›³|è¡¨|å¼|ç¬¬)[\dï¼-ï¼™]+.*', compound_word) and
        not re.match(r'^(ä¸Šè¨˜|å‰è¨˜|æœ¬é–‹ç¤º|å½“è©²|è©²)', compound_word) and
        not re.search(r'[0-9ï¼-ï¼™]+[)ï¼‰]?$', compound_word) and
        not re.match(r'[0-9ï¼-ï¼™]+[a-zA-Zï½-ï½šï¼¡-ï¼º]', compound_word)):
        words.append(compound_word)
    return words

def generate_wordcloud_and_list(words, title, top_n=20, font_path=None):
    if not words:
        st.subheader(title)
        st.warning("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚ã€è¡¨ç¤ºã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")
        return None

    word_freq = Counter(words)

    try:
        wc = WordCloud(
            width=800, height=400, background_color='white',
            font_path=font_path, collocations=False,
            max_words=100
        ).generate_from_frequencies(word_freq)

        fig, ax = plt.subplots(figsize=(12, 6))
        ax.imshow(wc, interpolation='bilinear')
        ax.set_title(title, fontsize=20)
        ax.axis('off')
        st.pyplot(fig)
        
        st.markdown(f"**ä¸Šä½ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ (Top {top_n})**")
        list_data = { "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": [], "å‡ºç¾é »åº¦": [] }
        for word, freq in word_freq.most_common(top_n):
            list_data["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"].append(word)
            list_data["å‡ºç¾é »åº¦"].append(freq)
        st.dataframe(pd.DataFrame(list_data), height=200)
        
    except Exception as e:
        st.error(f"ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã®æç”»ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        if font_path is None:
            st.warning("æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

# ==================================================================
# --- 4. å…±é€šãƒ‡ã‚¶ã‚¤ãƒ³è¨­å®š (CSS) ---
# ==================================================================
st.markdown("""
<style>
    /* åŸºæœ¬ãƒ•ã‚©ãƒ³ãƒˆè¨­å®š */
    html, body { 
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif; 
    }
    
    /* ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‚¿ã‚¤ãƒˆãƒ«è¨­å®š (çµ±ä¸€) */
    [data-testid="stSidebar"] h1 {
        color: #003366;
        font-weight: 900 !important;
        font-size: 2.5rem !important;
    }
    
    /* é‡è¦: æ¨™æº–ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã‚’éè¡¨ç¤ºã«ã™ã‚‹ */
    [data-testid="stSidebarNav"] {
        display: none !important;
    }
    
    /* ã‚µã‚¤ãƒ‰ãƒãƒ¼ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆèª¿æ•´ */
    [data-testid="stSidebar"] .block-container {
        padding-top: 2rem;
        padding-bottom: 1rem;
    }
    
    /* ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢èª¿æ•´ */
    .block-container { 
        padding-top: 2rem; 
        padding-bottom: 2rem; 
    }
    
    /* ãƒœã‚¿ãƒ³ãƒ»ã‚¿ãƒ–ã®ã‚¹ã‚¿ã‚¤ãƒ« */
    .stButton>button {
        font-weight: 600;
    }
    .stTabs [data-baseweb="tab-list"] {
        gap: 8px;
    }
    .stTabs [data-baseweb="tab"] {
        background-color: #f0f2f6;
        border-radius: 8px 8px 0 0;
        padding: 10px 15px;
    }
    .stTabs [aria-selected="true"] {
        background-color: #ffffff;
        border-bottom: 2px solid #003366;
    }
</style>
""", unsafe_allow_html=True)

# ==================================================================
# --- 5. ãƒ‡ã‚¶ã‚¤ãƒ³ãƒ†ãƒ¼ãƒç®¡ç† ---
# ==================================================================

def get_theme_config(theme_name):
    """ãƒ†ãƒ¼ãƒã«å¿œã˜ãŸã‚«ãƒ©ãƒ¼è¨­å®šã‚’è¿”ã™"""
    themes = {
        "APOLLO Standard": {
            "bg_color": "#ffffff",
            "text_color": "#333333",
            "sidebar_bg": "#f8f9fa",
            "plotly_template": "plotly_white",
            "color_sequence": px.colors.qualitative.G10,
            "density_scale": "Blues",
            "accent_color": "#003366",
            "css": """[data-testid="stHeader"] { background-color: #ffffff; } h1, h2, h3 { color: #003366; }"""
        },
        "Modern Presentation": {
            "bg_color": "#fdfdfd",
            "text_color": "#2c3e50",
            "sidebar_bg": "#eaeaea",
            "plotly_template": "plotly_white",
            "color_sequence": ["#264653", "#2a9d8f", "#e9c46a", "#f4a261", "#e76f51", "#8ab17d"],
            "density_scale": "Teal",
            "accent_color": "#264653",
            "css": """[data-testid="stSidebar"] { background-color: #eaeaea; } [data-testid="stHeader"] { background-color: #fdfdfd; } h1, h2, h3 { color: #264653; font-family: "Georgia", serif; } .stButton>button { background-color: #264653; color: white; border-radius: 0px; }"""
        }
    }
    return themes.get(theme_name, themes["APOLLO Standard"])

def update_fig_layout(fig, title, height=1000, width=800, theme_config=None, show_axes=False):
    """Plotlyã‚°ãƒ©ãƒ•ã®å…±é€šãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¨­å®š"""
    if theme_config is None:
        return fig
    
    layout_params = dict(
        template=theme_config["plotly_template"],
        title=dict(text=title, font=dict(size=18, color=theme_config["text_color"])),
        paper_bgcolor=theme_config["bg_color"],
        plot_bgcolor=theme_config["bg_color"],
        font=dict(color=theme_config["text_color"], family="Helvetica Neue"),
        height=height,
        width=width,
        margin=dict(l=20, r=20, t=60, b=20),
        legend=dict(
            orientation="v", yanchor="top", y=1, xanchor="left", x=1.02,
            bgcolor="rgba(255,255,255,0.8)", borderwidth=0
        )
    )

    if not show_axes:
        # åœ°å›³ãƒ¢ãƒ¼ãƒ‰
        layout_params['xaxis'] = dict(visible=False, showgrid=False, zeroline=False, showticklabels=False)
        layout_params['yaxis'] = dict(
            visible=False, showgrid=False, zeroline=False, showticklabels=False,
            scaleanchor="x", scaleratio=1
        )
    else:
        # çµ±è¨ˆã‚°ãƒ©ãƒ•ãƒ¢ãƒ¼ãƒ‰
        if "width" in layout_params: del layout_params["width"]
        layout_params['xaxis'] = dict(
            visible=True, showgrid=False, zeroline=False, showline=False, showticklabels=True
        )
        layout_params['yaxis'] = dict(
            visible=True, showgrid=True, gridcolor='#eee', zeroline=False, showline=False, showticklabels=True
        )

    fig.update_layout(**layout_params)
    return fig

# ==================================================================
# --- 6. ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° (MEGAåˆ†æãƒ­ã‚¸ãƒƒã‚¯) ---
# ==================================================================

@st.cache_data
def _get_top_words_filtered(dense_vector, feature_names, top_n=5):
    """TF-IDFãƒ™ã‚¯ãƒˆãƒ«ã‹ã‚‰ä¸Šä½èªã‚’æŠ½å‡ºï¼ˆã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å¤–ï¼‰"""
    indices = np.argsort(dense_vector)[::-1]
    top_words = []
    for i in indices:
        word = feature_names[i]
        if word not in stopwords and not re.fullmatch(r'[\dï¼-ï¼™]+', word) and len(word) > 1:
            top_words.append(word)
        if len(top_words) >= top_n:
            break
    return ", ".join(top_words)

@st.cache_data
def _calculate_cagr(row, cagr_end_year_val):
    valid_years = row[row > 0].index
    if not any(valid_years): return np.nan
    valid_years_in_range = valid_years[valid_years <= cagr_end_year_val]
    if not any(valid_years_in_range): return np.nan
    start_year = min(valid_years_in_range)
    end_year = max(valid_years_in_range)
    if start_year >= end_year: return np.nan
    start_value = row[start_year]
    end_value = row[end_year]
    num_years = end_year - start_year
    try: return ((end_value / start_value) ** (1 / num_years)) - 1
    except: return np.nan

@st.cache_data
def _calculate_metrics(pivot_df, cagr_end_year, y_axis_years, current_year, past_offset=0):
    target_cagr_end = cagr_end_year - past_offset
    target_current_year = current_year - past_offset
    y_start = target_current_year - y_axis_years + 1
    y_cols = [col for col in pivot_df.columns if col >= y_start and col <= target_current_year]
    y_axis = pivot_df[y_cols].sum(axis=1) if y_cols else pd.Series(0, index=pivot_df.index)
    
    if past_offset == 0: bubble_size = pivot_df.sum(axis=1)
    else:
        bubble_cols = [col for col in pivot_df.columns if col <= target_current_year]
        bubble_size = pivot_df[bubble_cols].sum(axis=1) if bubble_cols else pd.Series(0, index=pivot_df.index)

    cagr_cols = [col for col in pivot_df.columns if col <= target_cagr_end]
    if not cagr_cols: x_axis = pd.Series(np.nan, index=pivot_df.index)
    else: x_axis = pivot_df[cagr_cols].apply(_calculate_cagr, axis=1, cagr_end_year_val=target_cagr_end)
    return x_axis, y_axis, bubble_size

def _calculate_single_point_metrics(row_data, year_point, cagr_base_year, y_axis_years):
    y_start = year_point - y_axis_years + 1
    y_val = 0
    for y in range(y_start, year_point + 1):
        if y in row_data.index: y_val += row_data[y]
    size_val = 0
    for y in row_data.index:
        if y <= year_point: size_val += row_data[y]
    x_val = _calculate_cagr(row_data, year_point)
    return x_val, y_val, size_val

@st.cache_data
def _prepare_momentum_data(df_main, axis_col):
    df = df_main[['app_num_main', 'year', axis_col]].copy()
    df.dropna(subset=['app_num_main', 'year', axis_col], inplace=True)
    df_exploded = df.explode(axis_col)
    df_exploded[axis_col] = df_exploded[axis_col].str.strip()
    df_exploded.dropna(subset=[axis_col], inplace=True)
    df_exploded = df_exploded[df_exploded[axis_col] != '']
    df_unique = df_exploded.drop_duplicates(subset=['app_num_main', axis_col], keep='first')
    pivot_df = pd.pivot_table(df_unique, index=axis_col, columns='year', aggfunc='size', fill_value=0)
    pivot_df.columns = pivot_df.columns.astype(int)
    return pivot_df

def _hex_to_rgba(hex_color, alpha):
    hex_color = hex_color.lstrip('#')
    return f'rgba({int(hex_color[0:2], 16)},{int(hex_color[2:4], 16)},{int(hex_color[4:6], 16)},{alpha})'

def _get_hover_template_mode2(is_past=False):
    return f"""<b>%{{hovertext}}</b><br><br>æˆ¦ç•¥ã‚°ãƒ«ãƒ¼ãƒ—: %{{customdata[0]}}<br>X (å‹¢ã„): %{{x:.1%}}<br>Y (æ´»å‹•é‡): %{{y:,.0f}}<br>Bubble (ç·ä»¶æ•°): %{{marker.size}}<br><extra></extra>"""

@st.cache_data
def convert_df_to_csv(df):
    return df.to_csv(encoding='utf-8-sig').encode('utf-8-sig')

# ==================================================================
# --- 7. Streamlit UIæ§‹æˆ ---
# ==================================================================

with st.sidebar:
    st.title("APOLLO") 
    st.markdown("Advanced Patent & Overall Landscape-analytics Logic Orbiter")
    st.markdown("**v.3**")
    st.markdown("---")
    st.subheader("Home")
    st.page_link("Home.py", label="Mission Control", icon="ğŸ›°ï¸")
    st.subheader("Modules")
    st.page_link("pages/1_ğŸŒ_ATLAS.py", label="ATLAS", icon="ğŸŒ")
    st.page_link("pages/2_ğŸ’¡_CORE.py", label="CORE", icon="ğŸ’¡")
    st.page_link("pages/3_ğŸš€_Saturn_V.py", label="Saturn V", icon="ğŸš€")
    st.page_link("pages/4_ğŸ“ˆ_MEGA.py", label="MEGA", icon="ğŸ“ˆ")
    st.page_link("pages/5_ğŸ§­_Explorer.py", label="Explorer", icon="ğŸ§­")
    st.page_link("pages/6_ğŸ”—_CREW.py", label="CREW", icon="ğŸ”—")
    st.markdown("---")
    st.caption("ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³:\n1. Mission Control ã§ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€å‰å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\n2. ä¸Šã®ãƒªã‚¹ãƒˆã‹ã‚‰åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é¸æŠã—ã¾ã™ã€‚")
    st.markdown("---")
    st.caption("Â© 2025 ã—ã°ã‚„ã¾")

st.title("ğŸ“ˆ MEGA")
st.markdown("æŠ€è¡“å‹•æ…‹ï¼ˆãƒã‚¯ãƒ­ï¼‰ã¨æŠ€è¡“ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªï¼ˆãƒŸã‚¯ãƒ­ï¼‰ã‚’åˆ†æã—ã¾ã™ã€‚")

col_theme, col_dummy = st.columns([1, 3])
with col_theme:
    selected_theme = st.selectbox("è¡¨ç¤ºãƒ†ãƒ¼ãƒ:", ["APOLLO Standard", "Modern Presentation"], key="mega_theme_selector")
theme_config = get_theme_config(selected_theme)
st.markdown(f"<style>{theme_config['css']}</style>", unsafe_allow_html=True)

# ==================================================================
# --- 8. ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ & åˆæœŸåŒ– ---
# ==================================================================

if not st.session_state.get("preprocess_done", False):
    st.error("åˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    st.warning("å…ˆã«ã€ŒMission Controlã€ï¼ˆãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ï¼‰ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€ã€Œåˆ†æã‚¨ãƒ³ã‚¸ãƒ³èµ·å‹•ã€ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    st.stop()
else:
    try:
        df_main = st.session_state.df_main
        col_map = st.session_state.col_map
        sbert_embeddings = st.session_state.sbert_embeddings
        tfidf_matrix = st.session_state.tfidf_matrix
        feature_names = st.session_state.feature_names
    except Exception as e:
        st.error(f"ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        st.stop()

# ==================================================================
# --- 9. MEGA ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ ---
# ==================================================================

tab_b, tab_c, tab_d = st.tabs([
    "Landscape Analysis (PULSE)",
    "Technology Probe (TELESCOPE)",
    "Data Export"
])

# --- A. å‹•æ…‹åˆ†æ (PULSE) ---
with tab_b:
    st.subheader("åˆ†æãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
    col1, col2 = st.columns(2)
    with col1:
        axis_options = [('å‡ºé¡˜äºº', 'applicant_main'), ('IPC (ãƒ¡ã‚¤ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—)', 'ipc_main_group')]
        if st.session_state.col_map.get('fterm'): axis_options.append(('Fã‚¿ãƒ¼ãƒ  (ãƒ†ãƒ¼ãƒã‚³ãƒ¼ãƒ‰)', 'fterm_main'))
        analysis_axis = st.selectbox("åˆ†æè»¸:", options=axis_options, format_func=lambda x: x[0], key="mega_analysis_axis")
        yaxis_slider = st.slider("Yè»¸ (ç¾åœ¨) ã®é›†è¨ˆå¹´æ•°:", min_value=1, max_value=10, value=5, key="mega_yaxis")
        cagr_end_year = st.number_input("Xè»¸ (éå»ã®å‹¢ã„) è¨ˆç®—ã®æœ€çµ‚å¹´:", value=datetime.datetime.now().year - 1, key="mega_cagr_year")
    with col2:
        trajectory_past = st.number_input("è»Œè·¡ (éå»ã¸ã®é¡ã‚Šå¹´æ•°):", min_value=1, value=5, key="mega_trajectory")
        min_patents = st.number_input("æœ€å°ãƒ•ã‚£ãƒ«ã‚¿ä»¶æ•° (æç”»å¯¾è±¡):", min_value=1, value=10, key="mega_min_patents")

    st.subheader("ãƒã‚¤ãƒ©ã‚¤ãƒˆã¨è»Œè·¡")
    highlight_options = st.session_state.get("mega_highlight_options", [])
    highlight_targets = st.multiselect("æ³¨ç›®å¯¾è±¡ (è»Œè·¡ã‚’è¡¨ç¤º):", options=highlight_options, format_func=lambda x: x[0])

    st.subheader("å‹•æ…‹åˆ†æãƒãƒƒãƒ—å®Ÿè¡Œ")
    if st.button("å‹•æ…‹åˆ†æãƒãƒƒãƒ—ã‚’æç”»", type="primary", key="mega_run_map"):
        with st.spinner("å‹•æ…‹åˆ†æãƒãƒƒãƒ—ã‚’è¨ˆç®—ä¸­..."):
            try:
                axis_col, axis_label = analysis_axis[1], analysis_axis[0]
                y_axis_years, past_offset, current_year = int(yaxis_slider), int(trajectory_past), datetime.datetime.now().year
                min_patents_threshold = int(min_patents)

                pivot_df = _prepare_momentum_data(df_main, axis_col)
                st.session_state.mega_pivot_df = pivot_df 
                if pivot_df.empty:
                    st.error(f"ã‚¨ãƒ©ãƒ¼: åˆ†æè»¸ ({axis_label}) ã®æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                    st.stop()

                x_present, y_present, bubble_present = _calculate_metrics(pivot_df, cagr_end_year, y_axis_years, current_year, past_offset=0)
                
                # --- [ä¿®æ­£] ä»¶æ•°ãŒå¤šã„é †ã«ã‚½ãƒ¼ãƒˆ ---
                options_with_counts = [(f"{name} ({int(count)}ä»¶)", name) for name, count in bubble_present.sort_values(ascending=False).items()]
                st.session_state.mega_highlight_options = options_with_counts

                start_years = pivot_df[pivot_df > 0].apply(lambda row: row.first_valid_index(), axis=1)
                cagr_start_year_min = start_years.min() if not start_years.empty else cagr_end_year
                st.session_state.cagr_start_year_min = cagr_start_year_min
                st.session_state.cagr_end_year_val = cagr_end_year

                df_result = pd.DataFrame({'X_Present': x_present, 'Y_Present': y_present, 'Bubble_Present': bubble_present}).astype('float')
                df_result.index.name = axis_label
                df_result.replace([np.inf, -np.inf], np.nan, inplace=True)
                df_result.dropna(subset=['X_Present', 'Y_Present'], inplace=True)
                df_result = df_result[df_result['Bubble_Present'] >= min_patents_threshold].copy()

                if df_result.empty:
                    st.error("ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®çµæœãŒ0ä»¶ã§ã™ã€‚")
                    st.stop()

                x_threshold, y_threshold = df_result['X_Present'].mean(), df_result['Y_Present'].mean()
                st.session_state.mega_x_threshold = x_threshold
                st.session_state.mega_y_threshold = y_threshold

                def assign_relative_label(row):
                    if row['Y_Present'] <= 0: return 'è¡°é€€ãƒ»ãƒ‹ãƒƒãƒ (Declining/Niche)'
                    if (row['X_Present'] > x_threshold) and (row['Y_Present'] > y_threshold): return 'ãƒªãƒ¼ãƒ€ãƒ¼ (Leaders)'
                    elif (row['X_Present'] > x_threshold) and (row['Y_Present'] <= y_threshold): return 'æ–°èˆˆãƒ»é«˜ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ« (Emerging)'
                    elif (row['X_Present'] <= x_threshold) and (row['Y_Present'] > y_threshold): return 'æˆç†Ÿãƒ»æ—¢å­˜å‹¢åŠ› (Established)'
                    else: return 'è¡°é€€ãƒ»ãƒ‹ãƒƒãƒ (Declining/Niche)'

                df_result['Group_Auto'] = df_result.apply(assign_relative_label, axis=1)
                st.session_state.df_momentum_result = df_result.copy()
                st.session_state.mega_axis_label = axis_label
                st.session_state.mega_past_offset = past_offset
                st.session_state.mega_y_axis_years = y_axis_years

                df_filtered = df_result[df_result['Group_Auto'] != 'N/A']
                
                # --- [ä¿®æ­£] ä»¶æ•°ãŒå¤šã„é †ã«ã‚½ãƒ¼ãƒˆ ---
                drilldown_options = [('(åˆ†æå¯¾è±¡ã‚’é¸æŠ)', '(åˆ†æå¯¾è±¡ã‚’é¸æŠ)')] + [
                    (f"{name} ({int(row['Bubble_Present'])}ä»¶)", name) 
                    for name, row in df_filtered.sort_values('Bubble_Present', ascending=False).iterrows()
                ]
                st.session_state.mega_drilldown_options = drilldown_options
                st.success("å®Œäº†")
                st.rerun()
            except Exception as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

    # --- ãƒ©ãƒ™ãƒ«ç·¨é›† & æç”» (PULSE) ---
    st.subheader("ãƒ©ãƒ™ãƒ«ç·¨é›†")
    base_color_map = {'ãƒªãƒ¼ãƒ€ãƒ¼ (Leaders)': '#28a745', 'æ–°èˆˆãƒ»é«˜ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ« (Emerging)': '#ffc107', 'æˆç†Ÿãƒ»æ—¢å­˜å‹¢åŠ› (Established)': '#007bff', 'è¡°é€€ãƒ»ãƒ‹ãƒƒãƒ (Declining/Niche)': '#6c757d', 'N/A': '#ced4da'}

    if "df_momentum_result" in st.session_state:
        df_to_plot = st.session_state.df_momentum_result.copy()
        st.session_state.mega_group_map_custom = {}
        c1, c2 = st.columns(2)
        with c1:
            st.session_state.mega_group_map_custom['ãƒªãƒ¼ãƒ€ãƒ¼ (Leaders)'] = st.text_input("ãƒªãƒ¼ãƒ€ãƒ¼", "ãƒªãƒ¼ãƒ€ãƒ¼ (Leaders)", key="label_leader")
            st.session_state.mega_group_map_custom['æ–°èˆˆãƒ»é«˜ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ« (Emerging)'] = st.text_input("æ–°èˆˆ", "æ–°èˆˆãƒ»é«˜ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ« (Emerging)", key="label_emerging")
        with c2:
            st.session_state.mega_group_map_custom['æˆç†Ÿãƒ»æ—¢å­˜å‹¢åŠ› (Established)'] = st.text_input("æˆç†Ÿ", "æˆç†Ÿãƒ»æ—¢å­˜å‹¢åŠ› (Established)", key="label_established")
            st.session_state.mega_group_map_custom['è¡°é€€ãƒ»ãƒ‹ãƒƒãƒ (Declining/Niche)'] = st.text_input("è¡°é€€", "è¡°é€€ãƒ»ãƒ‹ãƒƒãƒ (Declining/Niche)", key="label_declining")

        df_to_plot['Group_Custom'] = df_to_plot['Group_Auto'].map(st.session_state.mega_group_map_custom).fillna('N/A')
        current_color_map = {st.session_state.mega_group_map_custom.get(k, k): v for k, v in base_color_map.items()}
        
        axis_label = st.session_state.mega_axis_label
        cagr_start = int(st.session_state.get('cagr_start_year_min', 2000))
        cagr_end = int(st.session_state.get('cagr_end_year_val', datetime.datetime.now().year))
        xaxis_title_label = f"éå»ã®å‹¢ã„ (CAGR, {cagr_start}-{cagr_end}å¹´å†…ã®æ´»å‹•æœŸé–“)"
        
        fig = go.Figure()
        
        # è»Œè·¡æç”»
        if highlight_targets:
            highlight_values = [t[1] for t in highlight_targets]
            palette = theme_config["color_sequence"]
            pivot_df = st.session_state.mega_pivot_df
            max_bubble = df_to_plot['Bubble_Present'].max()
            
            for i, target in enumerate(highlight_values):
                if target not in pivot_df.index: continue
                row = pivot_df.loc[target]
                base_color = palette[i % len(palette)]
                traj_x, traj_y, traj_s, traj_t, traj_c, traj_yr = [], [], [], [], [], []
                
                yr_range = list(range(datetime.datetime.now().year - st.session_state.mega_past_offset, datetime.datetime.now().year + 1))
                for idx, y in enumerate(yr_range):
                    xv, yv, sv = _calculate_single_point_metrics(row, y, y, st.session_state.mega_y_axis_years)
                    if pd.notna(xv) and pd.notna(yv):
                        traj_x.append(xv); traj_y.append(yv); traj_yr.append(y)
                        scaled_s = (sv / max_bubble) * 60 if max_bubble > 0 else 10
                        traj_s.append(max(5, scaled_s))
                        traj_t.append(f"'{str(y)[-2:]}")
                        alpha = 0.2 + 0.8 * (idx / max(1, len(yr_range)-1))
                        traj_c.append(_hex_to_rgba(base_color, alpha))
                
                if traj_x:
                    fig.add_trace(go.Scatter(x=traj_x, y=traj_y, mode='lines', line=dict(color=base_color, width=1), opacity=0.5, showlegend=False, hoverinfo='skip'))
                    fig.add_trace(go.Scatter(x=traj_x, y=traj_y, mode='markers+text', name=target, marker=dict(size=traj_s, color=traj_c, line=dict(width=1, color='white')), text=traj_t, textposition="top center", textfont=dict(size=10, color=base_color)))
        else:
            # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰
            df_filt = df_to_plot[df_to_plot['Group_Custom'] != 'N/A'].copy()
            if not df_filt.empty:
                df_filt['Y_Present_Plot'] = df_filt['Y_Present'].replace(0, 0.1)
                
                fig = px.scatter(
                    df_filt.reset_index(), 
                    x='X_Present', 
                    y='Y_Present_Plot',
                    size='Bubble_Present', 
                    size_max=60, 
                    color='Group_Custom', 
                    color_discrete_map=current_color_map, 
                    hover_name=st.session_state.mega_axis_label, 
                    log_y=True
                )
                fig.update_traces(hovertemplate=_get_hover_template_mode2())

        fig.add_vline(x=st.session_state.mega_x_threshold, line_width=1, line_dash="dash", line_color="gray")
        fig.add_hline(y=st.session_state.mega_y_threshold, line_width=1, line_dash="dash", line_color="gray")
        
        update_fig_layout(fig, "MEGA å‹•æ…‹åˆ†æãƒãƒƒãƒ—", height=800, theme_config=theme_config, show_axes=True)
        fig.update_layout(
            xaxis_title=f"â† å‹¢ã„æ¸›é€Ÿ | {xaxis_title_label} | å‹¢ã„åŠ é€Ÿ â†’ (åå­—ç·š: {st.session_state.mega_x_threshold:.1%})",
            yaxis_title="â† æ´»å‹•éˆåŒ– | ç¾åœ¨ã®æ´»å‹•é‡ | æ´»å‹•æ´»ç™º â†’",
            xaxis_tickformat='.0%', 
            yaxis_type="log",
            xaxis=dict(showgrid=True, zeroline=True, showline=True),
            yaxis=dict(showgrid=True, zeroline=False, showline=True)
        )
        
        st.plotly_chart(fig, use_container_width=True)
        st.session_state.df_momentum_export = df_to_plot.copy()


# --- C. ãƒ‰ãƒªãƒ«ãƒ€ã‚¦ãƒ³åˆ†æ (TELESCOPE) ---
with tab_c:
    st.subheader("åˆ†æå¯¾è±¡ã®é¸æŠ")
    drilldown_options = st.session_state.get("mega_drilldown_options", [('(åˆ†æå¯¾è±¡ã‚’é¸æŠ)', '(åˆ†æå¯¾è±¡ã‚’é¸æŠ)')])
    drilldown_target = st.selectbox("ãƒ‰ãƒªãƒ«ãƒ€ã‚¦ãƒ³å¯¾è±¡:", options=drilldown_options, format_func=lambda x: x[0], key="drill_target")[1]

    # --- ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°è¨­å®š ---
    st.subheader("ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°è¨­å®š (ãƒ‰ãƒªãƒ«ãƒ€ã‚¦ãƒ³ç”¨)")
    
    col1, col2, col3 = st.columns(3)
    with col1: drill_min_cluster_size = st.number_input('æœ€å°ã‚¯ãƒ©ã‚¹ã‚¿ã‚µã‚¤ã‚º:', min_value=2, value=5, key="drill_min_cluster_size")
    with col2: drill_min_samples = st.number_input('æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°:', min_value=1, value=5, key="drill_min_samples")
    with col3: drill_label_top_n = st.number_input('ãƒ©ãƒ™ãƒ«å˜èªæ•°:', min_value=1, value=3, key="drill_label_top_n")

    if st.button("é¸æŠå¯¾è±¡ã®æŠ€è¡“ãƒãƒƒãƒ—ã‚’æç”»", type="primary", key="drill_run_map"):
        if drilldown_target == '(åˆ†æå¯¾è±¡ã‚’é¸æŠ)': st.error("é¸æŠã—ã¦ãã ã•ã„ã€‚")
        else:
            with st.spinner("è¨ˆç®—ä¸­..."):
                try:
                    axis_col = "applicant_main" if st.session_state.mega_axis_label == "å‡ºé¡˜äºº" else "ipc_main_group" if "IPC" in st.session_state.mega_axis_label else "fterm_main"
                    mask = df_main[axis_col].apply(lambda l: drilldown_target in l)
                    df_filtered = df_main[mask].copy()
                    
                    if df_filtered.empty: st.error("ãƒ‡ãƒ¼ã‚¿ãªã—"); st.stop()
                    
                    emb = sbert_embeddings[df_main[mask].index]
                    tfidf = tfidf_matrix[df_main[mask].index]
                    original_indices = df_main[mask].index.tolist()

                    n_neighbors = min(10, len(original_indices) - 1)
                    if n_neighbors < 2: n_neighbors = 2
                    
                    umap_res = UMAP(n_components=2, n_neighbors=n_neighbors, min_dist=0.1, random_state=42).fit_transform(emb)
                    df_plot = pd.DataFrame(umap_res, columns=['x', 'y'])
                    
                    clusterer = hdbscan.HDBSCAN(
                        min_cluster_size=int(drill_min_cluster_size), 
                        min_samples=int(drill_min_samples),
                        metric='euclidean',
                        cluster_selection_method='eom'
                    )
                    cluster_labels = clusterer.fit_predict(df_plot[['x', 'y']].values)
                    
                    df_plot['cluster_id'] = cluster_labels
                    df_plot['year'] = df_filtered['year'].values
                    df_plot[col_map['title']] = df_filtered[col_map['title']].values
                    if col_map['abstract']: df_plot[col_map['abstract']] = df_filtered[col_map['abstract']].values
                    
                    label_map = {}
                    for cid in sorted(df_plot['cluster_id'].unique()):
                        if cid == -1: label_map[cid] = "ãƒã‚¤ã‚º"
                        else:
                            vecs = tfidf[(df_plot['cluster_id'] == cid).values]
                            mean_vector = np.asarray(vecs.mean(axis=0)).flatten()
                            top_words = _get_top_words_filtered(mean_vector, feature_names, top_n=int(drill_label_top_n))
                            label_map[cid] = f"[{cid}] {top_words}"
                    
                    df_plot['label'] = df_plot['cluster_id'].map(label_map)
                    st.session_state.df_drilldown = df_plot
                    st.session_state.sbert_sub_cluster_map_auto = label_map
                    st.session_state.drilldown_target_name = drilldown_target
                    st.rerun()
                except Exception as e:
                    st.error(f"Error: {e}")
                    import traceback
                    st.exception(traceback.format_exc())

    # --- æç”» ---
    if "df_drilldown" in st.session_state:
        df_d = st.session_state.df_drilldown.copy()
        
        map_mode = st.radio("è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰:", ["æ•£å¸ƒå›³ (Scatter)", "å¯†åº¦ãƒãƒƒãƒ— (Density)", "ã‚¯ãƒ©ã‚¹ã‚¿é ˜åŸŸ (Clusters)"], horizontal=True, key="mega_map_mode")
        
        c1, c2, c3 = st.columns(3)
        with c1:
            mesh_size = st.number_input("ãƒ¡ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚º (Grid)", 40, 200, 40, step=5, key="mega_mesh")
            use_abs = st.checkbox("å¯†åº¦ã‚¹ã‚±ãƒ¼ãƒ«å›ºå®š", False, key="mega_abs") if map_mode == "å¯†åº¦ãƒãƒƒãƒ— (Density)" else False
        with c2:
            no_noise = st.checkbox("ãƒã‚¤ã‚ºã‚’é™¤ã", False, key="mega_noise")
        with c3:
            show_label = st.checkbox("ãƒ©ãƒ™ãƒ«ã‚’è¡¨ç¤º", True, key="mega_label")
            
        c_t1, c_t2 = st.columns(2)
        with c_t1: interval = st.selectbox("æœŸé–“ã®ç²’åº¦:", [1, 2, 3, 5], index=2, key="mega_interval")
        
        min_y, max_y = df_d['year'].min(), df_d['year'].max()
        bins = pd.date_range(start=f"{int(min_y)}-01-01", end=f"{int(max_y)}-12-31", freq=f'{interval}YE')
        labels = [f"{bins[i].year}-{bins[i+1].year}" for i in range(len(bins)-1)]
        df_d['year_bin'] = pd.cut(df_d['year'], bins=[b.year for b in bins], labels=labels, include_lowest=True)
        
        bin_opts = ["(å…¨æœŸé–“)"] + sorted([l for l in labels if l in df_d['year_bin'].unique()])
        with c_t2: date_filter = st.selectbox("è¡¨ç¤ºæœŸé–“:", bin_opts, key="mega_date")

        if no_noise: df_d = df_d[df_d['cluster_id'] != -1]
        
        if date_filter == "(å…¨æœŸé–“)":
            df_in, df_out = df_d, pd.DataFrame()
            title_s = ""
        else:
            df_in = df_d[df_d['year_bin'] == date_filter]
            df_out = df_d[df_d['year_bin'] != date_filter]
            title_s = f" ({date_filter})"

        fig = go.Figure()
        
        # å¯†åº¦ãƒãƒƒãƒ—
        if map_mode == "å¯†åº¦ãƒãƒƒãƒ— (Density)" and not df_in.empty:
            colors = [[0, "rgba(255,255,255,0)"], [0.1, "rgba(225,245,254,0.3)"], [1, "rgba(2,119,189,0.9)"]]
            fig.add_trace(go.Histogram2dContour(
                x=df_in['x'], y=df_in['y'], colorscale=colors, nbinsx=mesh_size, nbinsy=mesh_size,
                contours=dict(coloring='fill', showlines=True), 
                line=dict(width=0.5, color='rgba(0,0,0,0.2)'),
                showscale=False,
                hoverinfo='skip'
            ))

        # ã‚¯ãƒ©ã‚¹ã‚¿é ˜åŸŸ
        if map_mode == "ã‚¯ãƒ©ã‚¹ã‚¿é ˜åŸŸ (Clusters)" and not df_in.empty:
            colors = theme_config["color_sequence"]
            u_cls = sorted(df_in['cluster_id'].unique())
            for i, cid in enumerate(u_cls):
                if cid == -1: continue
                pts = df_in[df_in['cluster_id'] == cid][['x', 'y']].values
                if len(pts) >= 3:
                    try:
                        hull = ConvexHull(pts)
                        h_pts = pts[hull.vertices]
                        h_pts = np.append(h_pts, [h_pts[0]], axis=0)
                        col = colors[i % len(colors)]
                        fig.add_trace(go.Scatter(x=h_pts[:,0], y=h_pts[:,1], mode='lines', fill='toself', fillcolor=col, opacity=0.1, line=dict(color=col, width=2), showlegend=False, hoverinfo='skip'))
                    except: pass

        # Ghost
        if not df_out.empty:
            fig.add_trace(go.Scattergl(x=df_out['x'], y=df_out['y'], mode='markers', marker=dict(color='#cccccc', size=3, opacity=0.5), name='æœŸé–“å¤–', hoverinfo='skip'))

        # Focus Scatter
        m_line = dict(width=1, color='white') if map_mode == "å¯†åº¦ãƒãƒƒãƒ— (Density)" else dict(width=0)
        colorscale = theme_config["color_sequence"] if isinstance(theme_config["color_sequence"], str) else 'turbo'
        
        fig.add_trace(go.Scattergl(
            x=df_in['x'], y=df_in['y'], mode='markers', 
            marker=dict(color=df_in['cluster_id'], colorscale=colorscale, size=5, line=m_line),
            hovertext=df_in['label'] + "<br>" + df_in[col_map['title']], name='æœŸé–“å†…'
        ))

        # Labels
        if show_label:
            u_cls = sorted(df_in['cluster_id'].unique())
            colors = theme_config["color_sequence"]
            all_cls = sorted(df_d['cluster_id'].unique())
            
            for cid in u_cls:
                if cid == -1: continue
                grp = df_in[df_in['cluster_id'] == cid]
                if grp.empty: continue
                
                mx, my = grp['x'].mean(), grp['y'].mean()
                label_txt = st.session_state.sbert_sub_cluster_map_auto.get(cid, str(cid))
                
                try: b_col = colors[all_cls.index(cid) % len(colors)]
                except: b_col = "#333"
                
                fig.add_annotation(x=mx, y=my, text=label_txt, showarrow=False, font=dict(size=10, color='black'), bgcolor='rgba(255,255,255,0.8)', bordercolor=b_col, borderwidth=2, borderpad=4)

        update_fig_layout(fig, f"æŠ€è¡“ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ª: {st.session_state.drilldown_target_name}{title_s}", height=1000, width=800, theme_config=theme_config, show_axes=False)
        st.plotly_chart(fig, use_container_width=True)
        st.session_state.df_drilldown_export = df_d

        st.markdown("---")
        st.subheader("ã‚¯ãƒ©ã‚¹ã‚¿ãƒ»ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ (Text Mining)")
        
        col_tm1, col_tm2 = st.columns(2)
        with col_tm1:
            cooc_top_n = st.slider("å…±èµ·: ä¸Šä½å˜èªæ•°", 30, 100, 50, key="mega_cooc_top_n")
            cooc_threshold = st.slider("å…±èµ·: Jaccardä¿‚æ•° é–¾å€¤", 0.01, 0.3, 0.05, 0.01, key="mega_cooc_threshold")
        
        if st.button("ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã‚’å®Ÿè¡Œ", key="mega_run_text_mining"):
            with st.spinner("åˆ†æä¸­..."):
                all_text = ""
                for _, row in df_in.iterrows():
                    if col_map['title'] and pd.notna(row[col_map['title']]): all_text += str(row[col_map['title']]) + " "
                    if col_map.get('abstract') and col_map['abstract'] in row and pd.notna(row[col_map['abstract']]): 
                        all_text += str(row[col_map['abstract']]) + " "
                
                words = extract_compound_nouns(all_text)
                
                if not words: st.warning("æœ‰åŠ¹ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãªã—")
                else:
                    st.markdown("##### 1. ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰")
                    generate_wordcloud_and_list(words, f"å¯¾è±¡: {st.session_state.drilldown_target_name}{title_s}", 30, FONT_PATH)
                    
                    st.markdown("##### 2. å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯")
                    word_freq = Counter(words)
                    top_words = [w for w, c in word_freq.most_common(cooc_top_n)]
                    pair_counts = Counter()
                    
                    for _, row in df_in.iterrows():
                        dt = ""
                        if col_map['title']: dt += str(row[col_map['title']]) + " "
                        if col_map.get('abstract') and col_map['abstract'] in row: 
                            dt += str(row[col_map['abstract']]) + " "
                        dw = set(extract_compound_nouns(dt))
                        dw = {w for w in dw if w in top_words}
                        if len(dw) >= 2:
                            for pair in combinations(sorted(list(dw)), 2): pair_counts[pair] += 1
                    
                    G = nx.Graph()
                    for w in top_words: G.add_node(w, count=word_freq[w])
                    for (w1, w2), c in pair_counts.items():
                        jac = c / (word_freq[w1] + word_freq[w2] - c)
                        if jac >= cooc_threshold: G.add_edge(w1, w2, weight=jac)
                    
                    G.remove_nodes_from(list(nx.isolates(G)))
                    if G.number_of_nodes() == 0: st.warning("å…±èµ·ãƒšã‚¢ãªã—")
                    else:
                        pos = nx.spring_layout(G, k=0.5, seed=42)
                        edge_x, edge_y = [], []
                        for edge in G.edges():
                            x0, y0 = pos[edge[0]]; x1, y1 = pos[edge[1]]
                            edge_x.extend([x0, x1, None]); edge_y.extend([y0, y1, None])
                        edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(width=0.5, color='#888'), hoverinfo='none', mode='lines')
                        
                        node_x, node_y, node_text, node_size = [], [], [], []
                        for node in G.nodes():
                            x, y = pos[node]; node_x.append(x); node_y.append(y)
                            c = G.nodes[node]['count']
                            node_text.append(f"{node} ({c})")
                            node_size.append(np.log(c+1)*10)
                        
                        node_trace = go.Scatter(
                            x=node_x, y=node_y, mode='markers+text', hoverinfo='text', text=list(G.nodes()), textposition="top center",
                            marker=dict(showscale=True, colorscale='YlGnBu', size=node_size, color=node_size, line_width=2)
                        )
                        fig_net = go.Figure(data=[edge_trace, node_trace], layout=go.Layout(title='å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯', showlegend=False, hovermode='closest', margin=dict(b=20,l=5,r=5,t=40), xaxis=dict(showgrid=False, zeroline=False, showticklabels=False), yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))
                        update_fig_layout(fig_net, 'å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯', theme_config=theme_config, show_axes=False)
                        fig_net.update_xaxes(visible=False); fig_net.update_yaxes(visible=False)
                        st.plotly_chart(fig_net, use_container_width=True)

# --- D. ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ ---
with tab_d:
    st.subheader("ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
    if "df_momentum_export" in st.session_state:
        st.download_button("å‹•æ…‹åˆ†æãƒ‡ãƒ¼ã‚¿ (CSV)", convert_df_to_csv(st.session_state.df_momentum_export), "MEGA_PULSE.csv", "text/csv")
    if "df_drilldown_export" in st.session_state:
        st.download_button("ãƒ‰ãƒªãƒ«ãƒ€ã‚¦ãƒ³ãƒ‡ãƒ¼ã‚¿ (CSV)", convert_df_to_csv(st.session_state.df_drilldown_export), "MEGA_TELESCOPE.csv", "text/csv")