import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
import plotly.io as pio
import warnings
import unicodedata
import re
import traceback

from janome.tokenizer import Tokenizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import euclidean_distances

# è¨­å®š
pio.templates.default = "plotly_white"
warnings.filterwarnings('ignore')

# ==================================================================
# --- 1. ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° & ãƒªã‚½ãƒ¼ã‚¹ãƒ­ãƒ¼ãƒ‰ ---
# ==================================================================

@st.cache_resource
def load_tokenizer_core():
    """Janome Tokenizerã‚’ãƒ­ãƒ¼ãƒ‰ãŠã‚ˆã³ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
    return Tokenizer()

t = load_tokenizer_core()

# åˆ†æé™¤å¤–ç”¨ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰
STOP_WORDS = {
    "ã™ã‚‹","ã‚ã‚‹","ãªã‚‹","ãŸã‚","ã“ã¨","ã‚ˆã†","ã‚‚ã®","ã“ã‚Œ","ãã‚Œ","ã‚ã‚Œ","ã“ã“","ãã“","ã©ã‚Œ","ã©ã®","ã“ã®","ãã®","å½“è©²","è©²","ãŠã‚ˆã³","åŠã³","ã¾ãŸã¯","ã¾ãŸ","ä¾‹ãˆã°","ä¾‹ãˆã°ã¯","ã«ãŠã„ã¦","ã«ã‚ˆã‚Š","ã«å¯¾ã—ã¦","ã«é–¢ã—ã¦","ã«ã¤ã„ã¦","ã¨ã—ã¦","ã¨ã—ã¦ã¯","å ´åˆ","ä¸€æ–¹","ä»–æ–¹","ã•ã‚‰ã«","ãã—ã¦","ãŸã ã—","ãªãŠ","ç­‰","ãªã©","ç­‰ã€…","ã„ã‚ã‚†ã‚‹","æ‰€è¬‚","åŒæ§˜","åŒæ™‚","å‰è¨˜","æœ¬","åŒ","å„","å„ç¨®","æ‰€å®š","æ‰€æœ›","ä¸€ä¾‹","ä»–","ä¸€éƒ¨","ä¸€ã¤","è¤‡æ•°","å°‘ãªãã¨ã‚‚","å°‘ãªãã¨ã‚‚ä¸€ã¤","ä¸Šè¨˜","ä¸‹è¨˜","å‰è¿°","å¾Œè¿°","æ—¢è¿°","é–¢ã™ã‚‹","åŸºã¥ã","ç”¨ã„ã‚‹","ä½¿ç”¨","åˆ©ç”¨","æœ‰ã™ã‚‹","å«ã‚€","å‚™ãˆã‚‹","è¨­ã‘ã‚‹","ã™ãªã‚ã¡","å¾“ã£ã¦","ã—ã‹ã—ãªãŒã‚‰","æ¬¡ã«","ç‰¹ã«","å…·ä½“çš„ã«","è©³ç´°ã«","ã„ãšã‚Œ","ã†ã¡","ãã‚Œãã‚Œ","ã¨ã","ã‹ã‹ã‚‹","ã‹ã‚ˆã†ãª","ã‹ã‹ã‚‹å ´åˆ","æœ¬ä»¶","æœ¬é¡˜","æœ¬å‡ºé¡˜","æœ¬æ˜ç´°æ›¸",
    "ã§ãã‚‹", "ã„ã‚‹", "æä¾›", "æ˜ç´°æ›¸", 
    "æœ¬ç™ºæ˜","ç™ºæ˜","å®Ÿæ–½ä¾‹","å®Ÿæ–½å½¢æ…‹","å¤‰å½¢ä¾‹","è«‹æ±‚","è«‹æ±‚é …","å›³","å›³é¢","ç¬¦å·","ç¬¦å·ã®èª¬æ˜","å›³é¢ã®ç°¡å˜ãªèª¬æ˜","ç™ºæ˜ã®è©³ç´°ãªèª¬æ˜","æŠ€è¡“åˆ†é‡","èƒŒæ™¯æŠ€è¡“","å¾“æ¥æŠ€è¡“","ç™ºæ˜ãŒè§£æ±ºã—ã‚ˆã†ã¨ã™ã‚‹èª²é¡Œ","èª²é¡Œ","è§£æ±ºæ‰‹æ®µ","åŠ¹æœ","è¦ç´„","ç™ºæ˜ã®åŠ¹æœ","ç›®çš„","æ‰‹æ®µ", "å®Ÿæ–½ã®å½¢æ…‹","å®Ÿæ–½ã®æ…‹æ§˜","æ…‹æ§˜","å¤‰å½¢","ä¿®æ­£ä¾‹","å›³ç¤º","å›³ç¤ºä¾‹","å›³ç¤ºã—ãªã„","å‚ç…§","å‚ç…§ç¬¦å·","æ®µè½","è©³ç´°èª¬æ˜","è¦æ—¨","ä¸€å®Ÿæ–½å½¢æ…‹","ä»–ã®å®Ÿæ–½å½¢æ…‹","ä¸€å®Ÿæ–½ä¾‹","åˆ¥ã®å´é¢","ä»˜è¨˜","é©ç”¨ä¾‹","ç”¨èªã®å®šç¾©","é–‹ç¤º","æœ¬é–‹ç¤º","é–‹ç¤ºå†…å®¹",
    "å‡ºé¡˜","å‡ºé¡˜äºº","å‡ºé¡˜ç•ªå·","å‡ºé¡˜æ—¥","å‡ºé¡˜æ›¸","å‡ºé¡˜å…¬é–‹","å…¬é–‹","å…¬é–‹ç•ªå·","å…¬é–‹å…¬å ±","å…¬å ±","å…¬å ±ç•ªå·","ç‰¹è¨±","ç‰¹è¨±ç•ªå·","ç‰¹è¨±æ–‡çŒ®","éç‰¹è¨±æ–‡çŒ®","å¼•ç”¨","å¼•ç”¨æ–‡çŒ®","å…ˆè¡ŒæŠ€è¡“","å¯©æŸ»","å¯©æŸ»å®˜","æ‹’çµ¶","æ„è¦‹æ›¸","è£œæ­£æ›¸","å„ªå…ˆ","å„ªå…ˆæ—¥","åˆ†å‰²å‡ºé¡˜","ç¶™ç¶šå‡ºé¡˜","å›½å†…ç§»è¡Œ","å›½éš›å‡ºé¡˜","å›½éš›å…¬é–‹","PCT","ç™»éŒ²","å…¬é–‹æ—¥","å¯©æŸ»è«‹æ±‚","æ‹’çµ¶ç†ç”±","è£œæ­£","è¨‚æ­£","ç„¡åŠ¹å¯©åˆ¤","ç•°è­°","å–æ¶ˆ","å–ä¸‹ã’","äº‹ä»¶ç•ªå·","ä»£ç†äºº","å¼ç†å£«","ä¿‚å±","çµŒé",
    "ç¬¬","ç¬¬ä¸€","ç¬¬äºŒ","ç¬¬ä¸‰","ç¬¬1","ç¬¬ï¼’","ç¬¬ï¼“","ç¬¬ï¼‘","ç¬¬ï¼’","ç¬¬ï¼“","ä¸€","äºŒ","ä¸‰","å››","äº”","å…­","ä¸ƒ","å…«","ä¹","é›¶","æ•°","è¤‡åˆ","å¤šæ•°","å°‘æ•°","å›³1","å›³2","å›³3","å›³4","å›³5","å›³6","å›³7","å›³8","å›³9","è¡¨1","è¡¨2","è¡¨3","å¼1","å¼2","å¼3",
    "%","ï¼…","wt%","vol%","è³ªé‡%","é‡é‡%","å®¹é‡%","mol","mol%","mol/L","M","mm","cm","m","nm","Î¼m","Î¼","rpm","Pa","kPa","MPa","GPa","N","W","V","A","mA","Hz","kHz","MHz","GHz","â„ƒ","Â°C","K","mL","L","g","kg","mg","wt","vol","h","hr","hrs","min","s","sec","ppm","ppb","bar","Î©","ohm","J","kJ","Wh","kWh",
    "æ ªå¼ä¼šç¤¾","æœ‰é™ä¼šç¤¾","åˆè³‡ä¼šç¤¾","åˆåä¼šç¤¾","åˆåŒä¼šç¤¾","Inc","Inc.","Ltd","Ltd.","Co","Co.","Corp","Corp.","LLC", "GmbH","AG","BV","B.V.","S.A.","S.p.A.","ï¼ˆæ ªï¼‰","ãˆ±","ï¼ˆæœ‰ï¼‰"
}

@st.cache_data
def _core_text_preprocessor(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†ï¼ˆæ­£è¦åŒ–ã€è¨˜å·é™¤å»ï¼‰"""
    if not isinstance(text, str): return ""
    text = unicodedata.normalize('NFKC', text).lower()
    text = re.sub(r'[ï¼ˆ(][^ï¼‰)]{1,80}[ï¼‰)]', ' ', text)
    text = re.sub(r'(?:å›³|Fig|FIG|fig)[. ã€€]*\d+', ' ', text)
    text = re.sub(r'[!ï¼?"â€œâ€#$%ï¼†&\'()ï¼ˆï¼‰*ï¼‹+,\-ï¼.\:ï¼š;ï¼›<=>?ï¼Ÿ@\[\]ï¼»ï¼½\\^_`{|}~ã€œã€œï¼/]', ' ', text)
    return text

@st.cache_data
def advanced_tokenize_core(text):
    """å½¢æ…‹ç´ è§£æã¨è¤‡åˆåè©æŠ½å‡º"""
    text = _core_text_preprocessor(text)
    if not text: return ""

    tokens = list(t.tokenize(text))
    processed_tokens = []
    i = 0
    while i < len(tokens):
        token1 = tokens[i]
        base1 = token1.base_form if token1.base_form != '*' else token1.surface
        if base1 in STOP_WORDS:
            i += 1
            continue
        
        part_of_speech = token1.part_of_speech.split(',')
        pos_major = part_of_speech[0]
        pos_minor = part_of_speech[1] if len(part_of_speech) > 1 else ''
        
        if len(base1) < 2 and pos_major != 'åè©':
            i += 1
            continue
        if pos_major == 'åè©' and pos_minor == 'æ•°':
            i += 1
            continue
            
        # è¤‡åˆåè©çµåˆ
        if (i + 1) < len(tokens):
            token2 = tokens[i+1]
            base2 = token2.base_form if token2.base_form != '*' else token2.surface
            part_of_speech_2 = token2.part_of_speech.split(',')
            pos_major_2 = part_of_speech_2[0]
            pos_minor_2 = part_of_speech_2[1] if len(part_of_speech_2) > 1 else ''
            
            if pos_major == 'åè©' and pos_major_2 == 'åè©' and \
               base2 not in STOP_WORDS and pos_minor_2 != 'æ•°':
                compound_word = base1 + base2
                processed_tokens.append(compound_word)
                i += 2
                continue
        
        if pos_major == 'åè©':
            processed_tokens.append(base1)
        elif pos_major == 'å‹•è©' and pos_minor == 'è‡ªç«‹':
            processed_tokens.append(base1)
        elif pos_major == 'å½¢å®¹è©' and pos_minor == 'è‡ªç«‹':
            processed_tokens.append(base1)
        i += 1
    return " ".join(processed_tokens)

# --- CORE æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ç”¨é–¢æ•° ---

def build_regex_pattern(keyword):
    return re.escape(keyword)

def build_near_regex(a, b, n):
    a_b = r'{}.{{0,{}}}?{}'.format(a, n, b)
    b_a = r'{}.{{0,{}}}?{}'.format(b, n, a)
    return r'(?:{}|{})'.format(a_b, b_a)

def build_adj_regex(a, b, n):
    return r'{}.{{0,{}}}?{}'.format(a, n, b)

def build_or_regex(a, b):
    return r'(?:{}|{})'.format(a, b)

def split_by_operator(text, operator):
    """æ‹¬å¼§ã®å¤–å´ã«ã‚ã‚‹æ¼”ç®—å­ã§ã®ã¿åˆ†å‰²ã™ã‚‹"""
    parts = []
    balance = 0
    current_chunk_start = 0
    for i, char in enumerate(text):
        if char == '(':
            balance += 1
        elif char == ')':
            balance -= 1
        elif char == operator and balance == 0:
            parts.append(text[current_chunk_start:i].strip())
            current_chunk_start = i + 1
    parts.append(text[current_chunk_start:].strip())
    return parts

@st.cache_data
def parse_core_rule(rule_str):
    """COREè«–ç†å¼ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦æ­£è¦è¡¨ç¾ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç”Ÿæˆã™ã‚‹"""
    tokens = re.findall(r'\(|\)|' r'\bnear\d+\b|' r'\badj\d+\b|' r'[\+]|' r'[^()\s\+]+', rule_str, re.IGNORECASE)
    tokens = [t.strip() for t in tokens if t and t.strip()]
    output_queue, op_stack = [], []
    op_precedence = {}
    
    for op in tokens:
        op_lower = op.lower()
        if op_lower == '+': op_precedence[op] = 1
        elif op_lower.startswith('near'): op_precedence[op] = 3
        elif op_lower.startswith('adj'): op_precedence[op] = 3
        
    for token in tokens:
        token_lower = token.lower()
        if token == '(':
            op_stack.append(token)
        elif token == ')':
            while op_stack and op_stack[-1] != '(':
                output_queue.append(op_stack.pop())
            if not op_stack: raise ValueError(f"æ–‡æ³•ã‚¨ãƒ©ãƒ¼: æ‹¬å¼§ã®å¯¾å¿œãŒå–ã‚Œã¾ã›ã‚“ (ã€Œ{rule_str}ã€)")
            op_stack.pop() 
        elif token_lower in op_precedence:
            while (op_stack and op_stack[-1] != '(' and op_precedence.get(op_stack[-1].lower(), 0) >= op_precedence[token_lower]):
                output_queue.append(op_stack.pop())
            op_stack.append(token)
        else:
            output_queue.append(token)
            
    while op_stack:
        op = op_stack.pop()
        if op == '(': raise ValueError(f"æ–‡æ³•ã‚¨ãƒ©ãƒ¼: æ‹¬å¼§ã®å¯¾å¿œãŒå–ã‚Œã¾ã›ã‚“ (ã€Œ{rule_str}ã€)")
        output_queue.append(op)
        
    regex_stack = []
    for token in output_queue:
        token_lower = token.lower()
        if token_lower not in op_precedence and token not in '()':
            normalized_token = unicodedata.normalize('NFKC', token).lower()
            if not normalized_token:
                raise ValueError(f"æ–‡æ³•ã‚¨ãƒ©ãƒ¼: ç©ºã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ (ã€Œ{rule_str}ã€)")
            regex_stack.append(build_regex_pattern(normalized_token))
        else:
            if len(regex_stack) < 2: raise ValueError(f"æ–‡æ³•ã‚¨ãƒ©ãƒ¼: æ¼”ç®—å­ '{token}' ãŒä¸æ­£ã§ã™ (ã€Œ{rule_str}ã€)")
            b, a = regex_stack.pop(), regex_stack.pop()
            if token_lower == '+':
                regex_stack.append(build_or_regex(a, b))
            elif token_lower.startswith('near'):
                n = int(re.findall(r'(\d+)', token_lower)[0])
                regex_stack.append(build_near_regex(a, b, n))
            elif token_lower.startswith('adj'):
                n = int(re.findall(r'(\d+)', token_lower)[0])
                regex_stack.append(build_adj_regex(a, b, n))
                
    if len(regex_stack) != 1: raise ValueError(f"æ–‡æ³•ã‚¨ãƒ©ãƒ¼: æœ€çµ‚å¼ãŒä¸æ­£ã§ã™ (ã€Œ{rule_str}ã€)")
    return re.compile(regex_stack[0], re.IGNORECASE | re.DOTALL) 

@st.cache_data
def prepare_axis_data_core(df, axis_col_name, delimiter):
    """ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—æç”»ç”¨ã®è»¸ãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç†ã™ã‚‹"""
    df_processed = df.copy()
    if axis_col_name not in df_processed.columns:
        st.error(f"ã‚¨ãƒ©ãƒ¼: ã‚«ãƒ©ãƒ  '{axis_col_name}' ãŒãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        return pd.DataFrame() 
    
    df_processed[axis_col_name] = df_processed[axis_col_name].fillna('N/A')
    
    if axis_col_name == 'year':
        df_processed[axis_col_name] = df_processed[axis_col_name].apply(
            lambda x: str(int(x)) if pd.notna(x) else 'N/A'
        )
        
    if delimiter:
        df_processed[axis_col_name] = df_processed[axis_col_name].astype(str).str.split(delimiter)
        df_processed = df_processed.explode(axis_col_name)
    
    df_processed[axis_col_name] = df_processed[axis_col_name].astype(str).str.strip()
    df_processed[axis_col_name] = df_processed[axis_col_name].replace('', 'N/A')
    return df_processed

@st.cache_data
def convert_df_to_csv_core(df):
    return df.to_csv(encoding='utf-8-sig').encode('utf-8-sig')


# ==================================================================
# --- 2. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆæœŸåŒ– & UIæ§‹æˆ ---
# ==================================================================

st.set_page_config(
    page_title="APOLLO | CORE", 
    page_icon="ğŸ’¡", 
    layout="wide"
)

# å…±é€šCSSã®é©ç”¨
st.markdown("""
<style>
    html, body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif; }
    [data-testid="stSidebar"] h1 { color: #003366; font-weight: 900 !important; font-size: 2.5rem !important; }
    h1 { color: #003366; font-weight: 700; }
    h2, h3 { color: #333333; font-weight: 500; border-bottom: 2px solid #f0f0f0; padding-bottom: 5px; }
    [data-testid="stSidebarNav"] { display: none !important; }
    [data-testid="stSidebar"] .block-container { padding-top: 2rem; padding-bottom: 1rem; }
    .block-container { padding-top: 2rem; padding-bottom: 2rem; }
    .stButton>button { font-weight: 600; }
    .stTabs [data-baseweb="tab-list"] { gap: 8px; }
    .stTabs [data-baseweb="tab"] { background-color: #f0f2f6; border-radius: 8px 8px 0 0; padding: 10px 15px; }
    .stTabs [aria-selected="true"] { background-color: #ffffff; border-bottom: 2px solid #003366; }
</style>
""", unsafe_allow_html=True)

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ ---
with st.sidebar:
    st.title("APOLLO") 
    st.markdown("Advanced Patent & Overall Landscape-analytics Logic Orbiter")
    st.markdown("---")
    st.subheader("Home")
    st.page_link("Home.py", label="Mission Control", icon="ğŸ›°ï¸")
    st.subheader("Modules")
    st.page_link("pages/1_ğŸŒ_ATLAS.py", label="ATLAS", icon="ğŸŒ")
    st.page_link("pages/2_ğŸ’¡_CORE.py", label="CORE", icon="ğŸ’¡")
    st.page_link("pages/3_ğŸš€_Saturn_V.py", label="Saturn V", icon="ğŸš€")
    st.page_link("pages/4_ğŸ“ˆ_MEGA.py", label="MEGA", icon="ğŸ“ˆ")
    st.page_link("pages/5_ğŸ§­_Explorer.py", label="Explorer", icon="ğŸ§­")
    st.markdown("---")
    st.caption("ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³:\n1. Mission Control ã§ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€å‰å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\n2. ä¸Šã®ãƒªã‚¹ãƒˆã‹ã‚‰åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é¸æŠã—ã¾ã™ã€‚")
    st.markdown("---")
    st.caption("Â© 2025 ã—ã°ã‚„ã¾")

# --- ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ ---
st.title("ğŸ’¡ CORE")
st.markdown("Contextual Operator & Rule Engine: **è«–ç†å¼ãƒ™ãƒ¼ã‚¹ã®ç‰¹è¨±åˆ†é¡ãƒ„ãƒ¼ãƒ«**ã§ã™ã€‚")

# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
if not st.session_state.get("preprocess_done", False):
    st.error("åˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    st.warning("å…ˆã«ã€ŒMission Controlã€ï¼ˆãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ï¼‰ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€ã€Œåˆ†æã‚¨ãƒ³ã‚¸ãƒ³èµ·å‹•ã€ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    st.stop()
else:
    df_main = st.session_state.df_main
    col_map = st.session_state.col_map

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if "core_classification_rules" not in st.session_state:
    st.session_state.core_classification_rules = {}
if "core_df_classified" not in st.session_state:
    st.session_state.core_df_classified = None
if "core_current_axis" not in st.session_state:
    st.session_state.core_current_axis = ""
if "core_reanalyze_result" not in st.session_state:
    st.session_state.core_reanalyze_result = ""


# ==================================================================
# --- 3. CORE ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ ---
# ==================================================================

phase_options = [
    "ãƒ•ã‚§ãƒ¼ã‚º 1: AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ (KMeans)",
    "ãƒ•ã‚§ãƒ¼ã‚º 2: åˆ†é¡ãƒ«ãƒ¼ãƒ«å®šç¾©",
    "ãƒ•ã‚§ãƒ¼ã‚º 3: åˆ†é¡å®Ÿè¡Œ",
    "ãƒ•ã‚§ãƒ¼ã‚º 4: ç‰¹è¨±ãƒãƒƒãƒ—ä½œæˆ"
]

current_phase = st.radio(
    "ãƒ•ã‚§ãƒ¼ã‚ºé¸æŠ:", 
    phase_options, 
    horizontal=True, 
    key="core_phase_selector"
)

st.markdown("---")

# --- ãƒ•ã‚§ãƒ¼ã‚º 1: AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ ---
if current_phase == phase_options[0]:
    st.subheader("ãƒ•ã‚§ãƒ¼ã‚º 1: AIã«ã‚ˆã‚‹åˆ†é¡ã‚µã‚¸ã‚§ã‚¹ãƒˆ (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)")
    st.markdown("K-Meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã«åŸºã¥ãã€åˆ†é¡ãƒ«ãƒ¼ãƒ«ä½œæˆã®ãŸã‚ã®AIãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚")
    
    col_map_options = [v for k, v in col_map.items() if k in ['title', 'abstract', 'claim']]
    target_column = st.selectbox("åˆ†æå¯¾è±¡ã‚«ãƒ©ãƒ :", options=col_map_options, key="core_target_col")
    
    col1, col2 = st.columns(2)
    with col1:
        ai_k_w = st.number_input("ãƒˆãƒ”ãƒƒã‚¯æ•° (K: ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ç”¨):", min_value=2, value=8, key="core_k")
        ai_n_w = st.number_input("å„ãƒˆãƒ”ãƒƒã‚¯ã®ä»£è¡¨æ–‡çŒ®æ•° (N):", min_value=1, value=5, key="core_n")
    
    use_mece = st.checkbox("MECEãƒ¢ãƒ¼ãƒ‰ (AIãŒæœ€é©ãªåˆ†é¡æ•°ã‚’è‡ªå‹•æ±ºå®š)", value=True, key="core_use_mece")

    if not use_mece:
        st.markdown("<b>ç”Ÿæˆã™ã‚‹åˆ†é¡ã®æ•° (æ‰‹å‹•è¨­å®š):</b>", unsafe_allow_html=True)
        col_c1, col_c2, col_c3 = st.columns(3)
        with col_c1:
            ai_cat_count_tech = st.number_input("æŠ€è¡“åˆ†é¡:", min_value=1, value=6, key="core_cat_tech")
        with col_c2:
            ai_cat_count_prob = st.number_input("èª²é¡Œåˆ†é¡:", min_value=1, value=6, key="core_cat_prob")
        with col_c3:
            ai_cat_count_sol = st.number_input("è§£æ±ºæ‰‹æ®µåˆ†é¡:", min_value=1, value=6, key="core_cat_sol")

    if st.button("AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ", key="core_run_ai"):
        if not target_column or target_column not in df_main.columns:
            st.error("ã‚¨ãƒ©ãƒ¼: åˆ†æå¯¾è±¡ã‚«ãƒ©ãƒ ã‚’æ­£ã—ãé¸æŠã—ã¦ãã ã•ã„ã€‚")
        else:
            try:
                k = int(ai_k_w)
                n = int(ai_n_w)
                
                with st.spinner(f"K-Means (K={k}) ã¨ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° (N={n}) ã‚’å®Ÿè¡Œä¸­..."):
                    texts_raw = df_main[target_column].astype(str).fillna('')
                    tokenized_texts = texts_raw.apply(advanced_tokenize_core)
                    
                    vectorizer = TfidfVectorizer(min_df=1, max_df=0.9, token_pattern=r"(?u)\b\w+\b")
                    tfidf_matrix = vectorizer.fit_transform(tokenized_texts)
                    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
                    clusters = kmeans.fit_predict(tfidf_matrix)
                    centroids = kmeans.cluster_centers_
                    
                    sampled_abstracts = []
                    for cluster_id in range(k):
                        cluster_indices = np.where(clusters == cluster_id)[0]
                        if len(cluster_indices) == 0: continue
                        centroid = centroids[cluster_id]
                        distances = euclidean_distances(tfidf_matrix[cluster_indices], centroid.reshape(1, -1))
                        closest_indices_in_cluster = distances.flatten().argsort()[:n]
                        original_indices = cluster_indices[closest_indices_in_cluster]
                        sampled_abstracts.append(f"\n--- (AIã«ã‚ˆã‚‹æ¨å®š) ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id} ã®ä»£è¡¨æ–‡çŒ® ---")
                        for original_index in original_indices:
                            abstract_original = texts_raw.iloc[original_index]
                            abstract_processed = _core_text_preprocessor(abstract_original)
                            sampled_abstracts.append(f"ãƒ» {abstract_processed}")
                    
                    if use_mece:
                        instruction_text = """
                        f"ã“ã®ç‰¹è¨±æ¯é›†å›£å…¨ä½“ã‚’ç¶²ç¾…çš„ã«åˆ†é¡ã™ã‚‹ãŸã‚ã®ã€**ã€ŒæŠ€è¡“åˆ†é¡ã€ã€Œèª²é¡Œåˆ†é¡ã€ã€Œè§£æ±ºæ‰‹æ®µåˆ†é¡ã€**ã®3ã¤ã®åˆ†é¡è»¸ã«ã¤ã„ã¦ã€**åˆ†é¡å®šç¾©**ï¼ˆåˆ†é¡åã€å®šç¾©ã€COREè«–ç†å¼ã®ã‚»ãƒƒãƒˆï¼‰ã‚’è¨­è¨ˆã—ã¦ãã ã•ã„ã€‚",
                        \n# é‡è¦: MECE (Mutually Exclusive, Collectively Exhaustive) ã®åŸå‰‡
                        - ç”Ÿæˆã™ã‚‹å„åˆ†é¡è»¸å†…ã®ã‚«ãƒ†ã‚´ãƒªã¯ã€ç›¸äº’ã«æ’ä»–çš„ï¼ˆãƒ€ãƒ–ã‚ŠãŒãªã„ï¼‰ã§ã‚ã‚Šã€ã‹ã¤å…¨ä½“ã¨ã—ã¦ç¶²ç¾…çš„ï¼ˆãƒ¢ãƒ¬ãŒãªã„ï¼‰ã§ã‚ã‚‹ã‚ˆã†ã«è¨­è¨ˆã—ã¦ãã ã•ã„ã€‚
                        - å„è»¸ã®ã‚«ãƒ†ã‚´ãƒªæ•°ã¯ã€MECEã‚’æº€ãŸã™ã®ã«æœ€é©ã ã¨ã‚ãªãŸãŒåˆ¤æ–­ã™ã‚‹æ•°ï¼ˆç›®å®‰ã¨ã—ã¦5ã€œ10å€‹ç¨‹åº¦ï¼‰ã«ã—ã¦ãã ã•ã„ã€‚
                        """
                    else:
                        instruction_text = [
                            f"ã“ã®ç‰¹è¨±æ¯é›†å›£å…¨ä½“ã‚’ç¶²ç¾…çš„ã«åˆ†é¡ã™ã‚‹ãŸã‚ã®ã€ä»¥ä¸‹ã®3ã¤ã®åˆ†é¡è»¸ã«ã¤ã„ã¦ã€æŒ‡å®šã•ã‚ŒãŸå€‹æ•°ã§**åˆ†é¡å®šç¾©**ï¼ˆåˆ†é¡åã€å®šç¾©ã€COREè«–ç†å¼ã®ã‚»ãƒƒãƒˆï¼‰ã‚’è¨­è¨ˆã—ã¦ãã ã•ã„ã€‚",
                            f"- **æŠ€è¡“åˆ†é¡**: {ai_cat_count_tech}å€‹",
                            f"- **èª²é¡Œåˆ†é¡**: {ai_cat_count_prob}å€‹",
                            f"- **è§£æ±ºæ‰‹æ®µåˆ†é¡**: {ai_cat_count_sol}å€‹"
                        ]
                        instruction_text = "\n".join(instruction_text)

                    prompt_parts = [
                        "ã‚ãªãŸã¯å„ªç§€ãªç‰¹è¨±æƒ…å ±ã‚¹ãƒˆãƒ©ãƒ†ã‚¸ã‚¹ãƒˆã§ã™ã€‚",
                        "\n# ä¾é ¼å†…å®¹",
                        f"ä»¥ä¸‹ã®ã€Œä»£è¡¨æ–‡çŒ®ã‚µãƒ³ãƒ—ãƒ«ã€ã¯ã€ã‚ã‚‹ç‰¹è¨±æ¯é›†å›£ï¼ˆ{len(df_main)}ä»¶ï¼‰ã‚’K-Meansæ³•ã§{k}å€‹ã®ã‚¯ãƒ©ã‚¹ã‚¿ã«åˆ†é¡ã—ã€å„ã‚¯ãƒ©ã‚¹ã‚¿ã‹ã‚‰ä»£è¡¨çš„ãªæ–‡çŒ®ã®ã€Œ{target_column}ã€ã‚’{n}ä»¶ãšã¤æŠ½å‡ºã—ãŸã‚‚ã®ã§ã™ã€‚",
                        instruction_text,
                        "\n# ã‚ãªãŸï¼ˆAIï¼‰ã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹",
                        "1. **ç†Ÿèª­:** ã¾ãšã€`# ä»£è¡¨æ–‡çŒ®ã‚µãƒ³ãƒ—ãƒ«` ã‚’**ã™ã¹ã¦**ç†Ÿèª­ã—ã€ã“ã®æŠ€è¡“åˆ†é‡ã®å…¨ä½“åƒï¼ˆã©ã®ã‚ˆã†ãªæŠ€è¡“ãƒˆãƒ”ãƒƒã‚¯ãŒã‚ã‚Šã€ã©ã®ã‚ˆã†ãªèª²é¡ŒãŒè­°è«–ã•ã‚Œã¦ã„ã‚‹ã‹ï¼‰ã‚’æŠŠæ¡ã—ã¾ã™ã€‚",
                        "2. **åˆ†é¡:** æ¬¡ã«ã€å„æ–‡çŒ®ã®æ–‡è„ˆã‹ã‚‰ã€æŠ€è¡“ã®ã€Œç›®çš„ï¼ˆèª²é¡Œï¼‰ã€ã¨ã€Œæ‰‹æ®µï¼ˆè§£æ±ºç­–ï¼‰ã€ã¨ã€Œæ ¸ã¨ãªã‚‹æŠ€è¡“è¦ç´ ã€ã‚’å¿ƒã®ä¸­ã§åˆ†é¡ã—ã¾ã™ã€‚",
                        "3. **ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é¸å®š:** å„åˆ†é¡è»¸ï¼ˆæŠ€è¡“ãƒ»èª²é¡Œãƒ»è§£æ±ºæ‰‹æ®µï¼‰ã«ãµã•ã‚ã—ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’é¸å®šã—ã¾ã™ã€‚",
                        "4. **ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ‹¡å¼µ:** ã€Œ**æœ€é‡è¦ãƒ«ãƒ¼ãƒ«**ã€ã«åŸºã¥ãã€é¸å®šã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®**é¡ç¾©èªã€é–¢é€£èªã€ä¸Šä½/ä¸‹ä½æ¦‚å¿µã€ç‰¹è¨±ç‰¹æœ‰ã®è¡¨ç¾ã€è¡¨è¨˜ã‚†ã‚Œï¼ˆã‚«ã‚¿ã‚«ãƒŠã€ã²ã‚‰ãŒãªã€æ¼¢å­—ï¼‰**ã‚’ã€ã‚ãªãŸã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ç¶²ç¾…çš„ã«æƒ³èµ·ã—ã¾ã™ã€‚",
                        "   - **ç‰¹è¨±ç”¨èªã®ç¶²ç¾…:** ï¼ˆä¾‹: ã€Œä¿æŒã€â†’ã€Œæ‹…æŒã€ã€Œå›ºç€ã€ã€Œä¿‚æ­¢ã€ãªã©ã€ç‰¹è¨±ã§ä½¿ã‚ã‚Œã‚‹è¨€ã„æ›ãˆã‚’ç¶²ç¾…ï¼‰",
                        "   - **æ¦‚å¿µã®éšå±¤åŒ–:** ä¸Šä½æ¦‚å¿µï¼ˆä¾‹: ã€Œè»Šä¸¡ã€ï¼‰ã¨ä¸‹ä½æ¦‚å¿µï¼ˆä¾‹: ã€Œè‡ªå‹•è»Šã€ã€ŒäºŒè¼ªè»Šã€ï¼‰ã®ä¸¡æ–¹ã‚’å«ã‚ã€å–ã‚Šã“ã¼ã—ã‚’é˜²ãã¾ã™ã€‚",
                        "5. **è«–ç†å¼æ§‹ç¯‰:** ã“ã‚Œã‚‰ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç¾¤ã‚’ã€Œ**COREè«–ç†å¼æ–‡æ³•**ã€ã‚’é§†ä½¿ã—ã¦çµ„ã¿åˆã‚ã›ã€**ã€Œãã®ä»–ã€ã«åˆ†é¡ã•ã‚Œã‚‹ç‰¹è¨±ã‚’æ¥µé™ã¾ã§æ¸›ã‚‰ã›ã‚‹ã‚ˆã†ãªã€ç¶²ç¾…æ€§ã®é«˜ã„**è«–ç†å¼ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚",
                        f"6. **å‡ºåŠ›:** æœ€å¾Œã«ã€ã€Œ### è‰¯ã„å‡ºåŠ›ä¾‹ã€ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å³å¯†ã«å¾“ã£ã¦ã€åˆ†é¡è»¸ã‚’ç”Ÿæˆã—ã¾ã™ã€‚",
                        "\n# COREè«–ç†å¼æ–‡æ³• (å³å®ˆ)",
                        "- `A + B` (OR): A ã¾ãŸã¯ B",
                        "- `A * B` (AND): A ã‹ã¤ B (é †åºå•ã‚ãš)",
                        "- `A nearN B` (è¿‘å‚): Aã¨BãŒ**Næ–‡å­—**ä»¥å†…ã§å‡ºç¾ (é †åºå•ã‚ãš)ã€‚Nã¯10ã€œ40ç¨‹åº¦ã‚’æ¨å¥¨ã€‚",
                        "- `A adjN B` (é †åºæŒ‡å®šè¿‘å‚): AãŒBã®**Næ–‡å­—**ä»¥å†…ã«Aâ†’Bã®é †ã§å‡ºç¾ã€‚Nã¯1ã€œ10ç¨‹åº¦ã‚’æ¨å¥¨ã€‚",
                        "- **é‡è¦:** ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯ã‚¹ãƒšãƒ¼ã‚¹ã‚’å«ã¾ãªã„å˜ä¸€èªï¼ˆä¾‹: `äºŒé…¸åŒ–ç‚­ç´ `ï¼‰ã«ã—ã¦ãã ã•ã„ã€‚ã‚¹ãƒšãƒ¼ã‚¹ã‚’å«ã‚€ãƒ•ãƒ¬ãƒ¼ã‚ºï¼ˆä¾‹: `AI agent`ï¼‰ã¯ã€`AI adj1 agent` ã®ã‚ˆã†ã«æ¼”ç®—å­ã§è¡¨ç¾ã—ã¦ãã ã•ã„ã€‚",
                        "\n# æœ€é‡è¦ãƒ«ãƒ¼ãƒ« (ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ‹¡å¼µã¨è¡¨è¨˜ã‚†ã‚Œ)",
                        "- ã‚µãƒ³ãƒ—ãƒ«ã«å­˜åœ¨ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãã®ã¾ã¾ä½¿ã†ã ã‘ã§ã¯ä¸ååˆ†ã§ã™ã€‚",
                        "- AIã®çŸ¥è­˜ã‚’æ´»ç”¨ã—ã€ãã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®**é¡ç¾©èªã€é–¢é€£èªã€ä¸Šä½/ä¸‹ä½æ¦‚å¿µ**ã‚’æƒ³èµ·ã—ã¦ãã ã•ã„ã€‚",
                        "- **ç‰¹ã«ã€ã‚«ã‚¿ã‚«ãƒŠï¼ˆä¾‹: `ãƒãƒªãƒãƒ¼`ï¼‰ã€ã²ã‚‰ãŒãªï¼ˆä¾‹: `ã°ã­`ï¼‰ã€æ¼¢å­—ï¼ˆä¾‹: `æ¨¹è„‚`ï¼‰**ã¨ã„ã£ãŸ**æ—¥æœ¬èªã®**è¡¨è¨˜ã‚†ã‚Œã‚’ `+` æ¼”ç®—å­ã§ç¶²ç¾…ã—ã¦ãã ã•ã„ã€‚",
                        "- **æ³¨æ„:** è«–ç†å¼ã«è‹±èªï¼ˆè‹±å˜èªï¼‰ã¯å«ã‚ãšã€æ—¥æœ¬èªï¼ˆæ¼¢å­—ã€ã‚«ã‚¿ã‚«ãƒŠã€ã²ã‚‰ãŒãªï¼‰ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚",
                        "- **ã‚«ã‚¿ã‚«ãƒŠ:** ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ã‚«ã‚¿ã‚«ãƒŠã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€**å¿…ãšå…¨è§’ï¼ˆä¾‹: `ãƒãƒªãƒãƒ¼`ï¼‰**ã‚’ä½¿ç”¨ã—ã€**åŠè§’ï¼ˆä¾‹: `ï¾ï¾Ÿï¾˜ï¾ï½°`ï¼‰ã¯çµ¶å¯¾ã«ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„**ã€‚",
                        "\n### è‰¯ã„å‡ºåŠ›ä¾‹",
                        "```",
                        "## æŠ€è¡“åˆ†é¡",
                        "1.  **CO2åˆ†é›¢è†œ**",
                        "    * **å®šç¾©:** CO2ã‚’åˆ†é›¢ãƒ»å›åã™ã‚‹ãŸã‚ã®è†œï¼ˆä¸­ç©ºç³¸è†œã€é«˜åˆ†å­è†œãªã©ï¼‰ã«é–¢é€£ã™ã‚‹æŠ€è¡“ã€‚",
                        "    * **è«–ç†å¼:** (CO2 + äºŒé…¸åŒ–ç‚­ç´  + ç‚­é…¸ã‚¬ã‚¹) * (è†œ + åˆ†é›¢è†œ + ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ + ä¸­ç©ºç³¸ + ãƒ¡ãƒ³ãƒ–ãƒ¬ãƒ³)",
                        "2.  **ã‚¢ãƒŸãƒ³å¸åæ¶²**",
                        "    * **å®šç¾©:** ã‚¢ãƒŸãƒ³åŒ–åˆç‰©ï¼ˆMEA, MDEAç­‰ï¼‰ã‚’ç”¨ã„ãŸåŒ–å­¦å¸åæ¶²ã«ã‚ˆã‚‹CO2å›åæŠ€è¡“ã€‚",
                        "    * **è«–ç†å¼:** (ã‚¢ãƒŸãƒ³ + å¸åæ¶² + å¸åå‰¤) + (MEA + MDEA + ãƒ¢ãƒã‚¨ã‚¿ãƒãƒ¼ãƒ«ã‚¢ãƒŸãƒ³)",
                        "\n## èª²é¡Œåˆ†é¡",
                        "1.  **è€ä¹…æ€§ã®å‘ä¸Š**",
                        "    * **å®šç¾©:** è†œã‚„å¸åæ¶²ã®åŠ£åŒ–ã‚’æŠ‘åˆ¶ã—ã€é•·æœŸé–“å®‰å®šã—ã¦ä½¿ç”¨å¯èƒ½ã«ã™ã‚‹ã“ã¨ã€‚",
                        "    * **è«–ç†å¼:** (è€ä¹…æ€§ + ä¿¡é ¼æ€§ + åŠ£åŒ– + å¯¿å‘½ + å®‰å®šæ€§ + è€ç†±æ€§ + è€è–¬å“æ€§) * (å‘ä¸Š + æ”¹å–„ + æŠ‘åˆ¶ + é«˜ã‚ã‚‹ + é˜²æ­¢ + ç¶­æŒ)",
                        "2.  **ã‚³ã‚¹ãƒˆã®å‰Šæ¸›**",
                        "    * **å®šç¾©:** è£½é€ ã‚³ã‚¹ãƒˆã‚„é‹ç”¨ã‚³ã‚¹ãƒˆã‚’ä½æ¸›ã—ã€çµŒæ¸ˆæ€§ã‚’é«˜ã‚ã‚‹ã“ã¨ã€‚",
                        "    * **è«–ç†å¼:** (ã‚³ã‚¹ãƒˆ + è£½é€ è²»ç”¨ + å®‰ä¾¡ + ä½å»‰ + çµŒæ¸ˆæ€§ + çœã‚¨ãƒãƒ«ã‚®ãƒ¼ + çœé›»åŠ›) * (å‰Šæ¸› + ä½æ¸› + å®‰ã + åŠ¹ç‡åŒ–)",
                        "\n## è§£æ±ºæ‰‹æ®µåˆ†é¡",
                        "1.  **å¤šå­”è³ªæ‹…ä½“ã®åˆ©ç”¨**",
                        "    * **å®šç¾©:** ã‚¼ã‚ªãƒ©ã‚¤ãƒˆã€MOFã€æ´»æ€§ç‚­ãªã©ã®å¤šå­”è³ªãªæ‹…ä½“ã«æ©Ÿèƒ½æ€§ææ–™ã‚’æ‹…æŒã•ã›ã‚‹æ‰‹æ³•ã€‚",
                        "    * **è«–ç†å¼:** (å¤šå­”è³ª + ãƒãƒ¼ãƒ©ã‚¹ + æ‹…ä½“ + ç´°å­” + ãƒãƒ‹ã‚«ãƒ ) + (ã‚¼ã‚ªãƒ©ã‚¤ãƒˆ + MOF + æ´»æ€§ç‚­ + é‡‘å±æœ‰æ©Ÿæ§‹é€ ä½“)",
                        "2.  **æ–°è¦ã‚¢ãƒŸãƒ³ã®æ·»åŠ **",
                        "    * **å®šç¾©:** æ—¢å­˜ã®ã‚¢ãƒŸãƒ³å¸åæ¶²ã«ã€æ€§èƒ½å‘ä¸Šã®ãŸã‚ã®æ–°è¦ã‚¢ãƒŸãƒ³åŒ–åˆç‰©ã‚’æ·»åŠ ã™ã‚‹æ‰‹æ³•ã€‚",
                        "    * **è«–ç†å¼:** (ã‚¢ãƒŸãƒ³ + æº¶å‰¤) adj10 (æ–°è¦ + æ·»åŠ  + æ··åˆ + é–‹ç™º + é…åˆ)",
                        "```",
                        "\n# ä»£è¡¨æ–‡çŒ®ã‚µãƒ³ãƒ—ãƒ«",
                        "\n".join(sampled_abstracts)
                    ]
                    final_prompt = "\n".join(prompt_parts)
                    
                    st.success("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
                    st.text_area("ä»¥ä¸‹ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼ã—ã€AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã«è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„ã€‚", final_prompt, height=400)

            except Exception as e:
                st.error(f"åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                import traceback
                st.exception(traceback.format_exc())

# --- ãƒ•ã‚§ãƒ¼ã‚º 2: åˆ†é¡ãƒ«ãƒ¼ãƒ«å®šç¾© ---
elif current_phase == phase_options[1]:
    st.subheader("ãƒ•ã‚§ãƒ¼ã‚º 2: åˆ†é¡ãƒ«ãƒ¼ãƒ«å®šç¾©")
    st.markdown("AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å‡ºåŠ›ã‚’å‚è€ƒã«ã€åˆ†é¡ãƒ«ãƒ¼ãƒ«ã‚’å®šç¾©ã—ã¾ã™ã€‚")
    
    existing_axes = list(st.session_state.core_classification_rules.keys())
    
    # è»¸ã®é¸æŠãƒ»å…¥åŠ›
    if existing_axes:
        input_mode = st.radio("è»¸ã®æŒ‡å®šæ–¹æ³•:", ["æ–°è¦ã«è»¸ã‚’ä½œæˆã™ã‚‹", "æ—¢å­˜ã®è»¸ã«è¿½åŠ ã™ã‚‹"], horizontal=True, key="core_axis_mode")
        if input_mode == "æ—¢å­˜ã®è»¸ã«è¿½åŠ ã™ã‚‹":
            default_index = 0
            if st.session_state.core_current_axis in existing_axes:
                default_index = existing_axes.index(st.session_state.core_current_axis)
            axis_name_text = st.selectbox("è¿½åŠ å…ˆã®è»¸ã‚’é¸æŠ:", existing_axes, index=default_index, key="core_axis_select")
        else:
            axis_name_text = st.text_input("æ–°ã—ã„åˆ†é¡è»¸ã®åå‰:", placeholder="ä¾‹: èª²é¡Œã€è§£æ±ºæ‰‹æ®µã€æŠ€è¡“è¦ç´ ãªã©", key="core_axis_name")
    else:
        axis_name_text = st.text_input("åˆ†é¡è»¸ã®åå‰:", placeholder="ä¾‹: èª²é¡Œã€è§£æ±ºæ‰‹æ®µã€æŠ€è¡“è¦ç´ ãªã©", key="core_axis_name")

    category_name_text = st.text_input("åˆ†é¡å:", placeholder="ä¾‹: è€ä¹…æ€§ã€ã‚³ã‚¹ãƒˆãƒ€ã‚¦ãƒ³ãªã©", key="core_category_name")
    category_def_text = st.text_area("å®šç¾©:", placeholder="ã“ã®åˆ†é¡ã®å®šç¾©ã‚„AIãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”¨ã®èª¬æ˜ã‚’å…¥åŠ›...", key="core_category_def")
    
    st.markdown("""
    <b>è«–ç†å¼æ–‡æ³• (N = æ–‡å­—æ•°)</b>
    <ul>
        <li><b><code>A + B</code></b> (OR): A ã¾ãŸã¯ B</li>
        <li><b><code>A * B</code></b> (AND): A ã‹ã¤ B (é †åºå•ã‚ãš)</li>
        <li><b><code>A nearN B</code></b> (è¿‘å‚): Aã¨BãŒ<b>Næ–‡å­—</b>ä»¥å†…ã§å‡ºç¾ (é †åºå•ã‚ãš)</li>
        <li><b><code>A adjN B</code></b> (é †åºæŒ‡å®šè¿‘å‚): AãŒBã®<b>Næ–‡å­—</b>ä»¥å†…ã«Aâ†’Bã®é †ã§å‡ºç¾</li>
        <li><b><code>( )</code></b> (æ‹¬å¼§): æ¼”ç®—ã®å„ªå…ˆé †ä½ã‚’æŒ‡å®š</li>
    </ul>
    """, unsafe_allow_html=True) 

    keywords_text = st.text_input("è«–ç†å¼:", placeholder="ä¾‹: (æ¨¹è„‚ + ãƒãƒªãƒãƒ¼) * (é«˜å¼·åº¦ near50 è€ä¹…æ€§)", key="core_keywords")
    
    # ãƒœã‚¿ãƒ³ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ã“ã®åˆ†é¡ãƒ«ãƒ¼ãƒ«ã‚’è¿½åŠ ", key="core_add_rule"):
            axis_name = axis_name_text
            category_name = category_name_text
            category_def = category_def_text
            rule_str = keywords_text
            
            if not all([axis_name, category_name, rule_str]):
                st.warning("ã€Œåˆ†é¡è»¸ã®åå‰ã€ã€Œåˆ†é¡åã€ã€Œè«–ç†å¼ã€ã¯å¿…é ˆã§ã™ã€‚")
            else:
                try:
                    or_clauses_str = split_by_operator(rule_str, '+')
                    compiled_or_clauses = []
                    for or_part_str in or_clauses_str:
                        and_clauses_str = split_by_operator(or_part_str, '*')
                        compiled_and_clauses = []
                        for and_part_str in and_clauses_str:
                            sub_rule = and_part_str.strip()
                            if not sub_rule: raise ValueError("ç©ºã®ãƒ«ãƒ¼ãƒ«ãŒã‚ã‚Šã¾ã™")
                            compiled_and_clauses.append(parse_core_rule(sub_rule))
                        compiled_or_clauses.append(compiled_and_clauses)
                    
                    if axis_name not in st.session_state.core_classification_rules:
                        st.session_state.core_classification_rules[axis_name] = {}
                    
                    st.session_state.core_classification_rules[axis_name][category_name] = {
                        'rule': rule_str,
                        'compiled': compiled_or_clauses,
                        'definition': category_def
                    }
                    
                    st.session_state.core_current_axis = axis_name
                    st.success(f"ãƒ«ãƒ¼ãƒ«ã‚’è¿½åŠ ã—ã¾ã—ãŸ: {category_name}")
                    st.rerun()
                except Exception as e:
                    st.error(f"æ–‡æ³•ã‚¨ãƒ©ãƒ¼: {e}")

    with col2:
        if st.button("ã“ã®åˆ†é¡è»¸ã®å®šç¾©ã‚’å®Œäº† (æ¬¡ã®è»¸ã¸)", key="core_finish_axis"):
            axis_name = st.session_state.core_current_axis
            if not axis_name or axis_name not in st.session_state.core_classification_rules:
                st.warning(f"è»¸ '{axis_name}' ã«ãƒ«ãƒ¼ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            else:
                st.success(f"è»¸ '{axis_name}' ã®å®šç¾©ã‚’å®Œäº†ã€‚")
                st.session_state.core_current_axis = ""
                st.rerun()

    st.markdown("---")
    st.subheader("ãƒ«ãƒ¼ãƒ«ã®ç®¡ç†ãƒ»ä¿®æ­£")
    
    if st.button("å…¨ãƒ«ãƒ¼ãƒ«ã‚’ãƒªã‚»ãƒƒãƒˆ", type="secondary", key="core_reset_all"):
        st.session_state.core_classification_rules = {}
        st.session_state.core_current_axis = ""
        st.success("ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸã€‚")
        st.rerun()

    if st.session_state.core_classification_rules:
        rule_items = []
        for axis, categories in st.session_state.core_classification_rules.items():
            for cat, data in categories.items():
                if isinstance(data, tuple): # æ—§ãƒ‡ãƒ¼ã‚¿å½¢å¼å¯¾å¿œ
                    rule_str = data[0]
                    def_str = ""
                else:
                    rule_str = data['rule']
                    def_str = data.get('definition', "")
                
                rule_items.append({
                    "label": f"[{axis}] {cat}: {rule_str[:20]}...",
                    "axis": axis, "cat": cat, "rule": rule_str, "def": def_str
                })
        
        if rule_items:
            selected_rule_label = st.selectbox("ç·¨é›†/å‰Šé™¤ã™ã‚‹ãƒ«ãƒ¼ãƒ«:", [r["label"] for r in rule_items], key="core_select_rule_edit")
            target_rule = next((r for r in rule_items if r["label"] == selected_rule_label), None)
            
            col_edit, col_del = st.columns(2)
            
            def load_rule_to_edit(axis, cat, rule, definition):
                if existing_axes:
                    st.session_state.core_axis_mode = "æ—¢å­˜ã®è»¸ã«è¿½åŠ ã™ã‚‹"
                    st.session_state.core_axis_select = axis
                else:
                    st.session_state.core_axis_name = axis
                st.session_state.core_current_axis = axis
                st.session_state.core_category_name = cat
                st.session_state.core_keywords = rule
                st.session_state.core_category_def = definition

            with col_edit:
                if target_rule:
                    st.button("ç·¨é›† (å…¥åŠ›æ¬„ã«ã‚»ãƒƒãƒˆ)", key="core_btn_edit", on_click=load_rule_to_edit, args=(target_rule["axis"], target_rule["cat"], target_rule["rule"], target_rule["def"]))
            with col_del:
                if st.button("å‰Šé™¤", key="core_btn_delete"):
                    if target_rule:
                        del st.session_state.core_classification_rules[target_rule["axis"]][target_rule["cat"]]
                        if not st.session_state.core_classification_rules[target_rule["axis"]]:
                            del st.session_state.core_classification_rules[target_rule["axis"]]
                        st.success("å‰Šé™¤ã—ã¾ã—ãŸã€‚")
                        st.rerun()

    st.markdown("---")
    st.subheader("å®šç¾©æ¸ˆã¿ãƒ«ãƒ¼ãƒ«ãƒ­ã‚°")
    if st.session_state.core_classification_rules:
        for axis, rules in st.session_state.core_classification_rules.items():
            st.markdown(f"**è»¸: {axis}**")
            for category, data in rules.items():
                if isinstance(data, tuple): r_str = data[0]
                else: r_str = data['rule']
                st.code(f"  - {category}: {r_str}", language="text")

# --- ãƒ•ã‚§ãƒ¼ã‚º 3: åˆ†é¡å®Ÿè¡Œ ---
elif current_phase == phase_options[2]:
    st.subheader("ãƒ•ã‚§ãƒ¼ã‚º 3: åˆ†é¡å®Ÿè¡Œ")
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚«ãƒ©ãƒ ã®å–å¾—ã¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    target_column = st.session_state.get("core_target_col")
    if not target_column and col_map.get('abstract'):
        target_column = col_map['abstract']
    
    if st.button("ã™ã¹ã¦ã®åˆ†é¡ã‚’å®Ÿè¡Œ", type="primary", key="core_run_classification"):
        if not st.session_state.core_classification_rules:
            st.error("ã‚¨ãƒ©ãƒ¼: ãƒ«ãƒ¼ãƒ«ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        elif not target_column or target_column not in df_main.columns:
            st.error("ã‚¨ãƒ©ãƒ¼: åˆ†æå¯¾è±¡ã‚«ãƒ©ãƒ ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
        else:
            with st.spinner("åˆ†é¡å‡¦ç†ã‚’å®Ÿè¡Œä¸­..."):
                try:
                    df_classified = df_main.copy()
                    status_area = st.empty()
                    progress_bar = st.progress(0, "åˆ†é¡å‡¦ç†ä¸­...")
                    rules = st.session_state.core_classification_rules
                    total_axes = len(rules)
                    
                    for i, (axis_name, ruleset) in enumerate(rules.items()):
                        status_area.write(f"è»¸ '{axis_name}' ã®åˆ†é¡å‡¦ç†ä¸­... ({i+1}/{total_axes})")
                        df_classified[axis_name] = ''
                        target_texts = df_classified[target_column].astype(str).fillna("")
                        
                        def apply_rules_for_axis(search_text):
                            search_text_processed = _core_text_preprocessor(search_text)
                            found_categories = []
                            for category, data in ruleset.items():
                                if isinstance(data, tuple): compiled_or = data[1]
                                else: compiled_or = data['compiled']
                                
                                is_or_match = False
                                for compiled_and in compiled_or:
                                    is_and_match = True
                                    for sub_regex in compiled_and:
                                        if not sub_regex.search(search_text_processed): 
                                            is_and_match = False; break 
                                    if is_and_match: is_or_match = True; break
                                if is_or_match: found_categories.append(category)
                            return ";".join(found_categories) if found_categories else 'ãã®ä»–'
                                
                        df_classified[axis_name] = target_texts.apply(apply_rules_for_axis)
                        progress_bar.progress((i + 1) / total_axes)

                    st.session_state.core_df_classified = df_classified.copy()
                    st.session_state.core_reanalyze_result = ""
                    status_area.empty(); progress_bar.empty()
                    st.success("å®Œäº†ã—ã¾ã—ãŸã€‚")
                    
                    # çµæœè¡¨ç¤º
                    st.subheader("åˆ†é¡çµæœã‚µãƒãƒªãƒ¼")
                    for axis_name in rules.keys():
                        st.markdown(f"**è»¸: {axis_name}**")
                        for cat in rules[axis_name].keys():
                            count = df_classified[axis_name].str.contains(re.escape(cat), na=False).sum()
                            st.write(f"- {cat}: {count}ä»¶")
                        st.write(f"- ãã®ä»–: {(df_classified[axis_name] == 'ãã®ä»–').sum()}ä»¶")

                    csv_core = convert_df_to_csv_core(df_classified)
                    st.download_button("åˆ†é¡çµæœCSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", csv_core, "CORE_classified.csv", "text/csv")
                    
                except Exception as e:
                    st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
                    import traceback
                    st.exception(traceback.format_exc())

    # --- å†åˆ†æã‚»ã‚¯ã‚·ãƒ§ãƒ³ ---
    st.markdown("---")
    st.subheader("ğŸ” æœªåˆ†é¡ãƒ‡ãƒ¼ã‚¿ã®å†åˆ†æ (ã€ãã®ä»–ã€ã‚’æ¸›ã‚‰ã™)")
    
    if st.session_state.core_df_classified is not None:
        rules = st.session_state.core_classification_rules
        if rules:
            col_re1, col_re2 = st.columns(2)
            with col_re1:
                reanalyze_axis = st.selectbox("å†åˆ†æã™ã‚‹è»¸ã‚’é¸æŠ:", list(rules.keys()), key="core_reanalyze_axis")
            
            with st.expander("è©³ç´°è¨­å®š (AIãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿)", expanded=False):
                col_p1, col_p2 = st.columns(2)
                with col_p1:
                    re_k = st.number_input("ãƒˆãƒ”ãƒƒã‚¯æ•° (K)", min_value=1, value=5, key="core_re_k")
                with col_p2:
                    re_n = st.number_input("ã‚µãƒ³ãƒ—ãƒ«æ•° (N)", min_value=1, value=3, key="core_re_n")
                
                re_use_mece = st.checkbox("MECEãƒ¢ãƒ¼ãƒ‰ (è‡ªå‹•æ±ºå®š)", value=True, key="core_re_mece")
                
                re_cat_count = 3 
                if not re_use_mece:
                    re_cat_count = st.number_input("è¿½åŠ ã™ã‚‹åˆ†é¡æ•°", min_value=1, value=3, key="core_re_count")

            def run_reanalysis():
                try:
                    df_classified = st.session_state.core_df_classified
                    target_col = st.session_state.get("core_target_col", col_map.get('abstract'))
                    axis = st.session_state.core_reanalyze_axis
                    k_in = st.session_state.core_re_k
                    n_in = st.session_state.core_re_n
                    use_mece_in = st.session_state.core_re_mece
                    cat_count_in = st.session_state.core_re_count if not use_mece_in else None
                    
                    others_df = df_classified[df_classified[axis] == 'ãã®ä»–']
                    
                    if others_df.empty:
                        st.session_state.core_reanalyze_result = "ã€ãã®ä»–ã€ã®ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"
                        return

                    with st.spinner(f"ã€ãã®ä»–ã€({len(others_df)}ä»¶) ã‚’åˆ†æä¸­..."):
                        texts_others = others_df[target_col].astype(str).fillna('')
                        tokenized_others = texts_others.apply(advanced_tokenize_core)
                        vec_others = TfidfVectorizer(min_df=1, max_df=0.9, token_pattern=r"(?u)\b\w+\b")
                        tfidf_others = vec_others.fit_transform(tokenized_others)
                        
                        actual_k = min(int(k_in), len(others_df))
                        if actual_k < 2: actual_k = 1
                        
                        kmeans_others = KMeans(n_clusters=actual_k, random_state=42, n_init=10)
                        clusters_others = kmeans_others.fit_predict(tfidf_others)
                        centroids_others = kmeans_others.cluster_centers_
                        
                        sampled_others_text = []
                        for cid in range(actual_k):
                            c_indices = np.where(clusters_others == cid)[0]
                            if len(c_indices) == 0: continue
                            centroid = centroids_others[cid]
                            dists = euclidean_distances(tfidf_others[c_indices], centroid.reshape(1, -1))
                            actual_n = int(n_in)
                            closest_idx = dists.flatten().argsort()[:actual_n]
                            orig_indices = c_indices[closest_idx]
                            sampled_others_text.append(f"\n--- ãã®ä»–ã‚°ãƒ«ãƒ¼ãƒ— {cid} ã®ä»£è¡¨æ–‡çŒ® ---")
                            for o_idx in orig_indices:
                                raw_txt = texts_others.iloc[o_idx]
                                sampled_others_text.append(f"ãƒ» {_core_text_preprocessor(raw_txt)}")
                        
                        existing_rules_str = []
                        for cat, data in rules[axis].items():
                            if isinstance(data, tuple): r_s, d_s = data[0], ""
                            else: r_s, d_s = data['rule'], data.get('definition', "")
                            existing_rules_str.append(f"- {cat}: {d_s} (è«–ç†å¼: {r_s})")
                        
                        if use_mece_in:
                            instruction_part = "MECEï¼ˆæ¼ã‚Œãªããƒ€ãƒ–ã‚Šãªãï¼‰ã‚’æ„è­˜ã—ã€æ—¢å­˜ã®åˆ†é¡ã®éš™é–“ã‚’åŸ‹ã‚ã‚‹ã‚ˆã†ãªå®šç¾©ã«ã—ã¦ãã ã•ã„ã€‚ã‚«ãƒ†ã‚´ãƒªæ•°ã¯ã‚ãªãŸãŒæœ€é©ã¨è€ƒãˆã‚‹æ•°ã«ã—ã¦ãã ã•ã„ã€‚"
                        else:
                            instruction_part = f"æ—¢å­˜ã®åˆ†é¡ã®éš™é–“ã‚’åŸ‹ã‚ã‚‹ã‚ˆã†ãªå®šç¾©ã§ã€**{cat_count_in}å€‹** ã®æ–°ã—ã„åˆ†é¡ã‚«ãƒ†ã‚´ãƒªã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚"

                        context_intro = f"ä»¥ä¸‹ã®ã€Œæœªåˆ†é¡ï¼ˆãã®ä»–ï¼‰ç‰¹è¨±ã®ã‚µãƒ³ãƒ—ãƒ«ã€ã¯ã€ã€ãã®ä»–ã€ã‚°ãƒ«ãƒ¼ãƒ—ã‚’K-Meansæ³•ã§{actual_k}å€‹ã®ã‚¯ãƒ©ã‚¹ã‚¿ã«åˆ†é¡ã—ã€å„ã‚¯ãƒ©ã‚¹ã‚¿ã‹ã‚‰ä»£è¡¨çš„ãªæ–‡çŒ®ã®ã€Œ{target_col}ã€ã‚’{actual_n}ä»¶ãšã¤æŠ½å‡ºã—ãŸã‚‚ã®ã§ã™ã€‚"

                        reanalyze_prompt = [
                            "ã‚ãªãŸã¯å„ªç§€ãªç‰¹è¨±æƒ…å ±ã‚¹ãƒˆãƒ©ãƒ†ã‚¸ã‚¹ãƒˆã§ã™ã€‚",
                            f"ç¾åœ¨ã€åˆ†é¡è»¸ã€Œ{axis}ã€ã‚’ä½œæˆä¸­ã§ã™ãŒã€ä»¥ä¸‹ã®ã€Œæ—¢å­˜ã®åˆ†é¡ã€ã«å½“ã¦ã¯ã¾ã‚‰ãªã„ç‰¹è¨±ãŒã€Œãã®ä»–ã€ã¨ã—ã¦æ®‹ã£ã¦ã„ã¾ã™ã€‚",
                            "\n# æ—¢å­˜ã®åˆ†é¡ãƒªã‚¹ãƒˆï¼ˆã“ã‚Œã‚‰ã¨ã¯é‡è¤‡ã—ãªã„ã‚ˆã†ã«ã—ã¦ãã ã•ã„ï¼‰",
                            "\n".join(existing_rules_str),
                            "\n# ä¾é ¼å†…å®¹",
                            context_intro,
                            "ã“ã‚Œã‚’åˆ†æã—ã€**æ—¢å­˜ã®åˆ†é¡ã¨ã¯æ¦‚å¿µçš„ã«é‡è¤‡ã—ãªã„ã€æ–°ã—ã„åˆ†é¡ã‚«ãƒ†ã‚´ãƒªï¼ˆåˆ†é¡åã€å®šç¾©ã€è«–ç†å¼ï¼‰**ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚",
                            instruction_part,
                            "\n# ã‚ãªãŸï¼ˆAIï¼‰ã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹",
                            "1. **ç†Ÿèª­:** ã€Œæœªåˆ†é¡ç‰¹è¨±ã®ã‚µãƒ³ãƒ—ãƒ«ã€ã‚’èª­ã¿ã€æ—¢å­˜åˆ†é¡ã§æ‹¾ã„ãã‚Œãªã‹ã£ãŸæŠ€è¡“æ¦‚å¿µã‚’ç‰¹å®šã—ã¾ã™ã€‚",
                            "2. **å®šç¾©:** æ—¢å­˜ã®åˆ†é¡ã¨è¢«ã‚‰ãªã„ã‚ˆã†ã«ã€æ–°ã—ã„æ¦‚å¿µã®å®šç¾©ã‚’æ˜ç¢ºã«ã—ã¾ã™ã€‚",
                            "3. **ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ‹¡å¼µ:** ã€Œ**æœ€é‡è¦ãƒ«ãƒ¼ãƒ«**ã€ã«åŸºã¥ãã€é¡ç¾©èªã€é–¢é€£èªã€ä¸Šä½/ä¸‹ä½æ¦‚å¿µã€ç‰¹è¨±ç‰¹æœ‰ã®è¡¨ç¾ã‚’ç¶²ç¾…ã—ã¾ã™ã€‚",
                            "4. **è«–ç†å¼æ§‹ç¯‰:** ã€Œ**COREè«–ç†å¼æ–‡æ³•**ã€ã‚’é§†ä½¿ã—ã¦ã€ãƒã‚¤ã‚ºã‚’é¿ã‘ã¤ã¤ç¶²ç¾…æ€§ã®é«˜ã„è«–ç†å¼ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚",
                            "\n# COREè«–ç†å¼æ–‡æ³• (å³å®ˆ)",
                            "- `A + B` (OR): A ã¾ãŸã¯ B",
                            "- `A * B` (AND): A ã‹ã¤ B (é †åºå•ã‚ãš)",
                            "- `A nearN B` (è¿‘å‚): Aã¨BãŒ**Næ–‡å­—**ä»¥å†…ã§å‡ºç¾ (é †åºå•ã‚ãš)ã€‚Nã¯10ã€œ40ç¨‹åº¦ã‚’æ¨å¥¨ã€‚",
                            "- `A adjN B` (é †åºæŒ‡å®šè¿‘å‚): AãŒBã®**Næ–‡å­—**ä»¥å†…ã«Aâ†’Bã®é †ã§å‡ºç¾ã€‚Nã¯1ã€œ10ç¨‹åº¦ã‚’æ¨å¥¨ã€‚",
                            "- **é‡è¦:** ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯ã‚¹ãƒšãƒ¼ã‚¹ã‚’å«ã¾ãªã„å˜ä¸€èªï¼ˆä¾‹: `äºŒé…¸åŒ–ç‚­ç´ `ï¼‰ã«ã—ã¦ãã ã•ã„ã€‚",
                            "\n# æœ€é‡è¦ãƒ«ãƒ¼ãƒ« (ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ‹¡å¼µã¨è¡¨è¨˜ã‚†ã‚Œ)",
                            "- AIã®çŸ¥è­˜ã‚’æ´»ç”¨ã—ã€ãã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®**é¡ç¾©èªã€é–¢é€£èªã€ä¸Šä½/ä¸‹ä½æ¦‚å¿µ**ã‚’æƒ³èµ·ã—ã¦ãã ã•ã„ã€‚",
                            "- **ç‰¹è¨±ç”¨èªã®ç¶²ç¾…:** ï¼ˆä¾‹: ã€Œä¿æŒã€â†’ã€Œæ‹…æŒã€ã€Œå›ºç€ã€ã€Œä¿‚æ­¢ã€ãªã©ï¼‰",
                            "- **æ¦‚å¿µã®éšå±¤åŒ–:** ä¸Šä½æ¦‚å¿µï¼ˆä¾‹: ã€Œè»Šä¸¡ã€ï¼‰ã¨ä¸‹ä½æ¦‚å¿µï¼ˆä¾‹: ã€Œè‡ªå‹•è»Šã€ã€ŒäºŒè¼ªè»Šã€ï¼‰ã®ä¸¡æ–¹ã‚’å«ã‚ã‚‹ã“ã¨ã€‚",
                            "- **ã‚«ã‚¿ã‚«ãƒŠ:** ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ã‚«ã‚¿ã‚«ãƒŠã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€**å¿…ãšå…¨è§’**ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚",
                            "\n# æœªåˆ†é¡ï¼ˆãã®ä»–ï¼‰ç‰¹è¨±ã®ã‚µãƒ³ãƒ—ãƒ«",
                            "\n".join(sampled_others_text),
                            "\n### å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ",
                            "```",
                            f"## {axis} (è¿½åŠ ææ¡ˆ)",
                            "1. **[åˆ†é¡å]**",
                            "   * **å®šç¾©:** [å®šç¾©]",
                            "   * **è«–ç†å¼:** (ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰A + ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰B) * (ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰C + ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰D)",
                            "```"
                        ]
                        st.session_state.core_reanalyze_result = "\n".join(reanalyze_prompt)

                except Exception as e:
                    st.session_state.core_reanalyze_result = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

            with col_re2:
                st.button("ã€ãã®ä»–ã€ã‚’åˆ†æã—ã¦æ–°ãƒ«ãƒ¼ãƒ«ã‚’ææ¡ˆ", key="core_btn_reanalyze", on_click=run_reanalysis)
        
        if st.session_state.core_reanalyze_result:
            if "ã‚¨ãƒ©ãƒ¼" in st.session_state.core_reanalyze_result:
                st.error(st.session_state.core_reanalyze_result)
            else:
                st.success("å†åˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚")
                st.text_area("ä»¥ä¸‹ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’AIã«è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„ã€‚", st.session_state.core_reanalyze_result, height=400)


# --- ãƒ•ã‚§ãƒ¼ã‚º 4: ç‰¹è¨±ãƒãƒƒãƒ—ä½œæˆ ---
elif current_phase == phase_options[3]:
    st.subheader("ãƒ•ã‚§ãƒ¼ã‚º 4: ç‰¹è¨±ãƒãƒƒãƒ—ä½œæˆ (ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—)")
    
    if st.session_state.core_df_classified is None:
        st.info("å…ˆã«ã€Œãƒ•ã‚§ãƒ¼ã‚º 3: åˆ†é¡å®Ÿè¡Œã€ã‚¿ãƒ–ã§åˆ†é¡ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    else:
        df_graph = st.session_state.core_df_classified
        
        st.subheader("ãƒãƒƒãƒ—è¨­å®š")
        
        core_axes = list(st.session_state.core_classification_rules.keys())
        app_py_axes = []
        
        if 'year' in df_graph.columns:
            app_py_axes.append("å‡ºé¡˜å¹´")
        
        if col_map.get('applicant') and col_map['applicant'] in df_graph.columns:
            app_py_axes.append("å‡ºé¡˜äºº")
        
        all_axis_options = core_axes + app_py_axes
        
        if len(all_axis_options) < 2:
            st.error("ã‚¨ãƒ©ãƒ¼: ã‚°ãƒ©ãƒ•åŒ–ã§ãã‚‹è»¸ï¼ˆåˆ†é¡è»¸ã€å‡ºé¡˜å¹´ã€å‡ºé¡˜äººã®ã†ã¡2ã¤ä»¥ä¸Šï¼‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            st.stop()
            
        col1, col2 = st.columns(2)
        with col1:
            x_axis_name = st.selectbox("Xè»¸:", all_axis_options, key="core_x_axis", index=min(0, len(all_axis_options)-1))
            x_top_n = st.number_input("Xè»¸ è¡¨ç¤ºä»¶æ•° (Top N):", min_value=1, value=20, key="core_x_top_n", help="ã€Œå‡ºé¡˜å¹´ã€ã‚’è»¸ã«ã—ãŸå ´åˆã¯ã€ã“ã®è¨­å®šã¯ç„¡è¦–ã•ã‚Œã¾ã™ã€‚")
            x_exclude_other_w = st.checkbox("Xè»¸ã‹ã‚‰ã€Œãã®ä»–ã€ã‚’é™¤å¤–", value=False, key="core_x_exclude_other")
            
        with col2:
            y_axis_name = st.selectbox("Yè»¸:", all_axis_options, key="core_y_axis", index=min(1, len(all_axis_options)-1))
            y_top_n = st.number_input("Yè»¸ è¡¨ç¤ºä»¶æ•° (Top N):", min_value=1, value=20, key="core_y_top_n", help="ã€Œå‡ºé¡˜å¹´ã€ã‚’è»¸ã«ã—ãŸå ´åˆã¯ã€ã“ã®è¨­å®šã¯ç„¡è¦–ã•ã‚Œã¾ã™ã€‚")
            y_exclude_other_w = st.checkbox("Yè»¸ã‹ã‚‰ã€Œãã®ä»–ã€ã‚’é™¤å¤–", value=False, key="core_y_exclude_other")
        
        delimiter_w = st.text_input("åŒºåˆ‡ã‚Šæ–‡å­— (åˆ†é¡è»¸ãƒ»å‡ºé¡˜äººç”¨):", value=';', key="core_delimiter")

        x_is_year = (st.session_state.core_x_axis == "å‡ºé¡˜å¹´")
        y_is_year = (st.session_state.core_y_axis == "å‡ºé¡˜å¹´")
        
        if x_is_year or y_is_year:
            st.markdown("---")
            st.subheader("æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿è¨­å®š")
            st.info("Xè»¸ã¾ãŸã¯Yè»¸ã«ã€Œå‡ºé¡˜å¹´ã€ãŒé¸æŠã•ã‚ŒãŸãŸã‚ã€ä»¥ä¸‹ã®æœŸé–“ã§ãƒ‡ãƒ¼ã‚¿ã‚’çµã‚Šè¾¼ã¿ã¾ã™ã€‚")
            
            def callback_autoset_core_year():
                if 'year' in df_graph.columns and df_graph['year'].notna().any():
                    valid_years = df_graph['year'].dropna().astype(int)
                    st.session_state.core_start_year = int(valid_years.min())
                    st.session_state.core_end_year = int(valid_years.max())
                else:
                    st.session_state.core_start_year = 2010
                    st.session_state.core_end_year = 2024
            
            if 'core_start_year' not in st.session_state:
                callback_autoset_core_year()

            d_col1, d_col2, d_col3 = st.columns([1, 1, 2])
            with d_col1:
                st.number_input("é–‹å§‹å¹´:", key="core_start_year", step=1, format="%d")
            with d_col2:
                st.number_input("çµ‚äº†å¹´:", key="core_end_year", step=1, format="%d")
            with d_col3:
                st.button("ï¼ˆå…¨æœŸé–“ã‚’è‡ªå‹•è¨­å®šï¼‰", on_click=callback_autoset_core_year, key="core_autoset_year")
        
        st.markdown("---")
        st.subheader("ãƒãƒƒãƒ—ã®å®Ÿè¡Œã¨è¡¨ç¤º")
        
        if st.button("5. ç‰¹è¨±ãƒãƒƒãƒ—ã‚’ä½œæˆ", type="primary", key="core_run_graph"):
            x_axis_key = st.session_state.core_x_axis
            y_axis_key = st.session_state.core_y_axis
            
            if x_axis_key == y_axis_key:
                st.error("ã‚¨ãƒ©ãƒ¼: ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§ã¯Xè»¸ã¨Yè»¸ã«ç•°ãªã‚‹ã‚«ãƒ©ãƒ ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
            else:
                with st.spinner("ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’ä½œæˆä¸­..."):
                    try:
                        delimiter = delimiter_w.strip()
                        
                        df_filtered = df_graph.copy()
                        if x_is_year or y_is_year:
                            start_year_val = int(st.session_state.core_start_year)
                            end_year_val = int(st.session_state.core_end_year)
                            df_filtered = df_filtered[
                                (df_filtered['year'].notna()) &
                                (df_filtered['year'] >= start_year_val) & 
                                (df_filtered['year'] <= end_year_val)
                            ]

                        if x_axis_key == "å‡ºé¡˜å¹´":
                            x_col_name = 'year'
                            x_delimiter = None
                        elif x_axis_key == "å‡ºé¡˜äºº":
                            x_col_name = col_map['applicant']
                            x_delimiter = delimiter
                        else:
                            x_col_name = x_axis_key 
                            x_delimiter = delimiter

                        if y_axis_key == "å‡ºé¡˜å¹´":
                            y_col_name = 'year'
                            y_delimiter = None
                        elif y_axis_key == "å‡ºé¡˜äºº":
                            y_col_name = col_map['applicant']
                            y_delimiter = delimiter
                        else:
                            y_col_name = y_axis_key
                            y_delimiter = delimiter

                        df_plot_x = prepare_axis_data_core(df_filtered, x_col_name, x_delimiter)
                        if df_plot_x.empty: st.stop()
                        
                        df_plot_xy = prepare_axis_data_core(df_plot_x, y_col_name, y_delimiter)
                        if df_plot_xy.empty: st.stop()
                        
                        if st.session_state.core_x_exclude_other:
                            df_plot_xy = df_plot_xy[df_plot_xy[x_col_name] != 'ãã®ä»–']
                        
                        if st.session_state.core_y_exclude_other:
                            df_plot_xy = df_plot_xy[df_plot_xy[y_col_name] != 'ãã®ä»–']
                        
                        x_top_n_val = int(st.session_state.core_x_top_n)
                        y_top_n_val = int(st.session_state.core_y_top_n)

                        if x_axis_key != "å‡ºé¡˜å¹´":
                            x_top_labels = df_plot_xy[
                                (df_plot_xy[x_col_name] != 'N/A') & 
                                (df_plot_xy[x_col_name] != 'ãã®ä»–')
                            ][x_col_name].value_counts().head(x_top_n_val).index.tolist()
                            
                            x_allowed_labels = x_top_labels + ['N/A']
                            if not st.session_state.core_x_exclude_other:
                                x_allowed_labels.append('ãã®ä»–')
                                
                            df_plot_xy = df_plot_xy[df_plot_xy[x_col_name].isin(x_allowed_labels)]
                        
                        if y_axis_key != "å‡ºé¡˜å¹´":
                            y_top_labels = df_plot_xy[
                                (df_plot_xy[y_col_name] != 'N/A') & 
                                (df_plot_xy[y_col_name] != 'ãã®ä»–')
                            ][y_col_name].value_counts().head(y_top_n_val).index.tolist()
                            
                            y_allowed_labels = y_top_labels + ['N/A']
                            if not st.session_state.core_y_exclude_other:
                                y_allowed_labels.append('ãã®ä»–')

                            df_plot_xy = df_plot_xy[df_plot_xy[y_col_name].isin(y_allowed_labels)]

                        df_plot_final = df_plot_xy[
                            (df_plot_xy[x_col_name] != 'N/A') & 
                            (df_plot_xy[y_col_name] != 'N/A')
                        ]
                        
                        if df_plot_final.empty:
                            st.warning("è©²å½“ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ï¼ˆTop N ãƒ•ã‚£ãƒ«ã‚¿ã‚„ N/A é™¤å¤–ã®çµæœã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ï¼‰")
                        else:
                            matrix = pd.crosstab(df_plot_final[y_col_name], df_plot_final[x_col_name])
                            
                            # è»¸ã‚’æ–‡å­—åˆ—å‹ã«çµ±ä¸€ã—ã€ã‚½ãƒ¼ãƒˆ
                            matrix.index = matrix.index.astype(str)
                            matrix.columns = matrix.columns.astype(str)

                            if x_axis_key == "å‡ºé¡˜å¹´":
                                x_category_order = sorted(matrix.columns, key=lambda x: int(x) if x.isdigit() else x)
                            else:
                                x_category_order = matrix.sum(axis=0).sort_values(ascending=False).index.tolist()
                            
                            if y_axis_key == "å‡ºé¡˜å¹´":
                                y_category_order = sorted(matrix.index, key=lambda x: int(x) if x.isdigit() else x)
                            else:
                                y_category_order = matrix.sum(axis=1).sort_values(ascending=False).index.tolist()
                            
                            # 0åŸ‹ã‚ã—ã¦NaNã‚’å›é¿
                            matrix = matrix.reindex(index=y_category_order, columns=x_category_order).fillna(0)
                            z = matrix.values
                            z_max = z.max() if z.size > 0 else 1
                            
                            x_labels = matrix.columns.tolist()
                            y_labels = matrix.index.tolist()
                            x_indices = np.arange(len(x_labels))
                            y_indices = np.arange(len(y_labels))

                            annotations = []
                            for i, row_idx in enumerate(y_indices):
                                for j, col_idx in enumerate(x_indices):
                                    val = z[i][j]
                                    text_val = str(int(val))
                                    color = "white" if val > z_max * 0.6 else "black"
                                    annotations.append(dict(
                                        x=col_idx, y=row_idx, text=text_val,
                                        xref="x", yref="y", showarrow=False,
                                        font=dict(color=color, size=14)
                                    ))
                            
                            h = max(600, len(matrix.index) * 100 + 200)
                            
                            fig = go.Figure(data=go.Heatmap(
                                z=z, x=x_indices, y=y_indices,
                                colorscale='YlGnBu', showscale=True
                            ))
                            fig.update_layout(
                                title=f"'{y_axis_key}' Ã— '{x_axis_key}' ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—",
                                height=h, annotations=annotations,
                                xaxis=dict(
                                    title=x_axis_key, 
                                    tickmode='array', 
                                    tickvals=x_indices, 
                                    ticktext=x_labels, 
                                    side='bottom', 
                                    tickangle=-90
                                ),
                                yaxis=dict(
                                    title=y_axis_key, 
                                    tickmode='array', 
                                    tickvals=y_indices, 
                                    ticktext=y_labels, 
                                    autorange='reversed'
                                ),
                                margin=dict(l=150, b=150)
                            )
                            st.plotly_chart(fig, use_container_width=True)
                        
                    except Exception as e:
                        st.error(f"ã‚°ãƒ©ãƒ•ä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                        import traceback
                        st.exception(traceback.format_exc())

# --- å…±é€šã‚µã‚¤ãƒ‰ãƒãƒ¼ãƒ•ãƒƒã‚¿ãƒ¼ ---
st.sidebar.markdown("---") 
st.sidebar.caption("ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³:")
st.sidebar.caption("1. Mission Control ã§ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€å‰å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
st.sidebar.caption("2. å·¦ã®ãƒªã‚¹ãƒˆã‹ã‚‰åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é¸æŠã—ã¾ã™ã€‚")
st.sidebar.markdown("---")
st.sidebar.caption("Â© 2025 ã—ã°ã‚„ã¾")