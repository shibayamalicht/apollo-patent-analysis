import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
import plotly.io as pio
import warnings
import unicodedata
import re
import json
import traceback

from janome.tokenizer import Tokenizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import euclidean_distances

# ==================================================================
# --- 1. ãƒšãƒ¼ã‚¸è¨­å®š ---
# ==================================================================
st.set_page_config(
    page_title="APOLLO | CORE", 
    page_icon="ğŸ’¡", 
    layout="wide"
)

pio.templates.default = "plotly_white"
warnings.filterwarnings('ignore')

# ==================================================================
# --- 2. ãƒ‡ã‚¶ã‚¤ãƒ³ãƒ†ãƒ¼ãƒè¨­å®š & å…±é€šCSS ---
# ==================================================================
st.markdown("""
<style>
    html, body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif; }
    [data-testid="stSidebar"] h1 { color: #003366; font-weight: 900 !important; font-size: 2.5rem !important; }
    [data-testid="stSidebarNav"] { display: none !important; }
    [data-testid="stSidebar"] .block-container { padding-top: 2rem; padding-bottom: 1rem; }
    .block-container { padding-top: 2rem; padding-bottom: 2rem; }
    .stButton>button { font-weight: 600; }
    .stTabs [data-baseweb="tab-list"] { gap: 8px; }
    .stTabs [data-baseweb="tab"] { background-color: #f0f2f6; border-radius: 8px 8px 0 0; padding: 10px 15px; }
    .stTabs [aria-selected="true"] { background-color: #ffffff; border-bottom: 2px solid #003366; }
</style>
""", unsafe_allow_html=True)

def get_theme_config(theme_name):
    themes = {
        "APOLLO Standard": { "bg_color": "#ffffff", "text_color": "#333333", "plotly_template": "plotly_white", "color_sequence": px.colors.qualitative.G10, "css": """[data-testid="stHeader"] { background-color: #ffffff; } h1, h2, h3 { color: #003366; }""" },
        "Modern Presentation": { "bg_color": "#fdfdfd", "text_color": "#2c3e50", "plotly_template": "plotly_white", "color_sequence": ["#264653", "#2a9d8f", "#e9c46a", "#f4a261", "#e76f51", "#8ab17d"], "css": """[data-testid="stSidebar"] { background-color: #eaeaea; } [data-testid="stHeader"] { background-color: #fdfdfd; } h1, h2, h3 { color: #264653; font-family: "Georgia", serif; } .stButton>button { background-color: #264653; color: white; border-radius: 0px; }""" }
    }
    return themes.get(theme_name, themes["APOLLO Standard"])

# ==================================================================
# --- 3. ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° & ãƒªã‚½ãƒ¼ã‚¹ãƒ­ãƒ¼ãƒ‰ ---
# ==================================================================
@st.cache_resource
def load_tokenizer_core(): return Tokenizer()
t = load_tokenizer_core()

STOP_WORDS = {
    "ã™ã‚‹","ã‚ã‚‹","ãªã‚‹","ãŸã‚","ã“ã¨","ã‚ˆã†","ã‚‚ã®","ã“ã‚Œ","ãã‚Œ","ã‚ã‚Œ","ã“ã“","ãã“","ã©ã‚Œ","ã©ã®","ã“ã®","ãã®","å½“è©²","è©²","ãŠã‚ˆã³","åŠã³","ã¾ãŸã¯","ã¾ãŸ","ä¾‹ãˆã°","ä¾‹ãˆã°ã¯","ã«ãŠã„ã¦","ã«ã‚ˆã‚Š","ã«å¯¾ã—ã¦","ã«é–¢ã—ã¦","ã«ã¤ã„ã¦","ã¨ã—ã¦","ã¨ã—ã¦ã¯","å ´åˆ","ä¸€æ–¹","ä»–æ–¹","ã•ã‚‰ã«","ãã—ã¦","ãŸã ã—","ãªãŠ","ç­‰","ãªã©","ç­‰ã€…","ã„ã‚ã‚†ã‚‹","æ‰€è¬‚","åŒæ§˜","åŒæ™‚","å‰è¨˜","æœ¬","åŒ","å„","å„ç¨®","æ‰€å®š","æ‰€æœ›","ä¸€ä¾‹","ä»–","ä¸€éƒ¨","ä¸€ã¤","è¤‡æ•°","å°‘ãªãã¨ã‚‚","å°‘ãªãã¨ã‚‚ä¸€ã¤","ä¸Šè¨˜","ä¸‹è¨˜","å‰è¿°","å¾Œè¿°","æ—¢è¿°","é–¢ã™ã‚‹","åŸºã¥ã","ç”¨ã„ã‚‹","ä½¿ç”¨","åˆ©ç”¨","æœ‰ã™ã‚‹","å«ã‚€","å‚™ãˆã‚‹","è¨­ã‘ã‚‹","ã™ãªã‚ã¡","å¾“ã£ã¦","ã—ã‹ã—ãªãŒã‚‰","æ¬¡ã«","ç‰¹ã«","å…·ä½“çš„ã«","è©³ç´°ã«","ã„ãšã‚Œ","ã†ã¡","ãã‚Œãã‚Œ","ã¨ã","ã‹ã‹ã‚‹","ã‹ã‚ˆã†ãª","ã‹ã‹ã‚‹å ´åˆ","æœ¬ä»¶","æœ¬é¡˜","æœ¬å‡ºé¡˜","æœ¬æ˜ç´°æ›¸",
    "ã§ãã‚‹", "ã„ã‚‹", "æä¾›", "æ˜ç´°æ›¸", 
    "æœ¬ç™ºæ˜","ç™ºæ˜","å®Ÿæ–½ä¾‹","å®Ÿæ–½å½¢æ…‹","å¤‰å½¢ä¾‹","è«‹æ±‚","è«‹æ±‚é …","å›³","å›³é¢","ç¬¦å·","ç¬¦å·ã®èª¬æ˜","å›³é¢ã®ç°¡å˜ãªèª¬æ˜","ç™ºæ˜ã®è©³ç´°ãªèª¬æ˜","æŠ€è¡“åˆ†é‡","èƒŒæ™¯æŠ€è¡“","å¾“æ¥æŠ€è¡“","ç™ºæ˜ãŒè§£æ±ºã—ã‚ˆã†ã¨ã™ã‚‹èª²é¡Œ","èª²é¡Œ","è§£æ±ºæ‰‹æ®µ","åŠ¹æœ","è¦ç´„","ç™ºæ˜ã®åŠ¹æœ","ç›®çš„","æ‰‹æ®µ", "å®Ÿæ–½ã®å½¢æ…‹","å®Ÿæ–½ã®æ…‹æ§˜","æ…‹æ§˜","å¤‰å½¢","ä¿®æ­£ä¾‹","å›³ç¤º","å›³ç¤ºä¾‹","å›³ç¤ºã—ãªã„","å‚ç…§","å‚ç…§ç¬¦å·","æ®µè½","è©³ç´°èª¬æ˜","è¦æ—¨","ä¸€å®Ÿæ–½å½¢æ…‹","ä»–ã®å®Ÿæ–½å½¢æ…‹","ä¸€å®Ÿæ–½ä¾‹","åˆ¥ã®å´é¢","ä»˜è¨˜","é©ç”¨ä¾‹","ç”¨èªã®å®šç¾©","é–‹ç¤º","æœ¬é–‹ç¤º","é–‹ç¤ºå†…å®¹","è¨˜è¼‰","è¨˜è¿°","æ²è¼‰","è¨€åŠ","å†…å®¹","è©³ç´°","èª¬æ˜","è¡¨è¨˜","è¡¨ç¾","ç®‡æ¡æ›¸ã","ä»¥ä¸‹ã®","ä»¥ä¸Šã®","å…¨ã¦ã®","ä»»æ„ã®","ç‰¹å®šã®",
    "ä¸Šéƒ¨","ä¸‹éƒ¨","å†…éƒ¨","å¤–éƒ¨","å†…å´","å¤–å´","è¡¨é¢","è£é¢","å´é¢","ä¸Šé¢","ä¸‹é¢","ç«¯é¢","å…ˆç«¯","åŸºç«¯","å¾Œç«¯","ä¸€ç«¯","ä»–ç«¯","ä¸­å¿ƒ","ä¸­å¤®","å‘¨ç¸","å‘¨è¾º","è¿‘å‚","æ–¹å‘","ä½ç½®","ç©ºé–“","é ˜åŸŸ","ç¯„å›²","é–“éš”","è·é›¢","å½¢çŠ¶","å½¢æ…‹","çŠ¶æ…‹","ç¨®é¡","å±¤","è†œ","éƒ¨","éƒ¨æ","éƒ¨ä½","éƒ¨å“","æ©Ÿæ§‹","è£…ç½®","å®¹å™¨","çµ„æˆ","ææ–™","ç”¨é€”","é©ç”¨","é©ç”¨ä¾‹","ç‰‡å´","ä¸¡å´","å·¦å´","å³å´","å‰æ–¹","å¾Œæ–¹","ä¸Šæµ","ä¸‹æµ","éš£æ¥","è¿‘æ¥","é›¢é–“","é–“ç½®","ä»‹åœ¨","é‡ç•³","æ¦‚ã­","ç•¥","ç•¥ä¸­å¤®","å›ºå®šå´","å¯å‹•å´","ä¼¸é•·","åç¸®","ä¿‚åˆ","åµŒåˆ","å–ä»˜","é€£çµéƒ¨","æ”¯æŒä½“","æ”¯æŒéƒ¨","ã‚¬ã‚¤ãƒ‰éƒ¨",
    "ãƒ‡ãƒ¼ã‚¿","æƒ…å ±","ä¿¡å·","å‡ºåŠ›","å…¥åŠ›","åˆ¶å¾¡","æ¼”ç®—","å–å¾—","é€ä¿¡","å—ä¿¡","è¡¨ç¤º","é€šçŸ¥","è¨­å®š","å¤‰æ›´","æ›´æ–°","ä¿å­˜","å‰Šé™¤","è¿½åŠ ","å®Ÿè¡Œ","é–‹å§‹","çµ‚äº†","ç¶™ç¶š","åœæ­¢","åˆ¤å®š","åˆ¤æ–­","æ±ºå®š","é¸æŠ","ç‰¹å®š","æŠ½å‡º","æ¤œå‡º","æ¤œçŸ¥","æ¸¬å®š","è¨ˆæ¸¬","ç§»å‹•","å›è»¢","å¤‰ä½","å¤‰å½¢","å›ºå®š","é…ç½®","ç”Ÿæˆ","ä»˜ä¸","ä¾›çµ¦","é©ç”¨","ç…§åˆ","æ¯”è¼ƒ","ç®—å‡º","è§£æ","åŒå®š","åˆæœŸåŒ–","èª­å‡º","æ›¸è¾¼","ç™»éŒ²","è¨˜éŒ²","é…ä¿¡","é€£æº","åˆ‡æ›¿","èµ·å‹•","å¾©å¸°","ç›£è¦–","é€šçŸ¥å‡¦ç†","å–å¾—å‡¦ç†","æ¼”ç®—å‡¦ç†",
    "è‰¯å¥½","å®¹æ˜“","ç°¡ä¾¿","é©åˆ‡","æœ‰åˆ©","æœ‰ç”¨","æœ‰åŠ¹","åŠ¹æœçš„","é«˜ã„","ä½ã„","å¤§ãã„","å°ã•ã„","æ–°è¦","æ”¹è‰¯","æ”¹å–„","æŠ‘åˆ¶","å‘ä¸Š","ä½æ¸›","å‰Šæ¸›","å¢—åŠ ","æ¸›å°‘","å¯èƒ½","å¥½é©","å¥½ã¾ã—ã„","æœ›ã¾ã—ã„","å„ªã‚Œã‚‹","å„ªã‚ŒãŸ","é«˜æ€§èƒ½","é«˜åŠ¹ç‡","ä½ã‚³ã‚¹ãƒˆ","ã‚³ã‚¹ãƒˆ","ç°¡æ˜“","å®‰å®š","å®‰å®šæ€§","è€ä¹…","è€ä¹…æ€§","ä¿¡é ¼æ€§","ç°¡ç´ ","ç°¡ç•¥","å˜ç´”","æœ€é©","æœ€é©åŒ–","æ±ç”¨","æ±ç”¨æ€§","å®Ÿç¾","é”æˆ","ç¢ºä¿","ç¶­æŒ","é˜²æ­¢","å›é¿","ä¿ƒé€²","ä¸è¦","å¿…è¦","é«˜ç²¾åº¦","çœé›»åŠ›","çœè³‡æº","é«˜ä¿¡é ¼","ä½è² è·","é«˜ç´”åº¦","é«˜å¯†åº¦","é«˜æ„Ÿåº¦","è¿…é€Ÿ","å††æ»‘","ç°¡ç•¥åŒ–","ä½ä¾¡æ ¼","å®ŸåŠ¹çš„","å¯èƒ½åŒ–","æœ‰åŠ¹åŒ–","éå¿…é ˆ","é©åˆ","äº’æ›",
    "å‡ºé¡˜","å‡ºé¡˜äºº","å‡ºé¡˜ç•ªå·","å‡ºé¡˜æ—¥","å‡ºé¡˜æ›¸","å‡ºé¡˜å…¬é–‹","å…¬é–‹","å…¬é–‹ç•ªå·","å…¬é–‹å…¬å ±","å…¬å ±","å…¬å ±ç•ªå·","ç‰¹è¨±","ç‰¹è¨±ç•ªå·","ç‰¹è¨±æ–‡çŒ®","éç‰¹è¨±æ–‡çŒ®","å¼•ç”¨","å¼•ç”¨æ–‡çŒ®","å…ˆè¡ŒæŠ€è¡“","å¯©æŸ»","å¯©æŸ»å®˜","æ‹’çµ¶","æ„è¦‹æ›¸","è£œæ­£æ›¸","å„ªå…ˆ","å„ªå…ˆæ—¥","åˆ†å‰²å‡ºé¡˜","ç¶™ç¶šå‡ºé¡˜","å›½å†…ç§»è¡Œ","å›½éš›å‡ºé¡˜","å›½éš›å…¬é–‹","PCT","ç™»éŒ²","å…¬é–‹æ—¥","å¯©æŸ»è«‹æ±‚","æ‹’çµ¶ç†ç”±","è£œæ­£","è¨‚æ­£","ç„¡åŠ¹å¯©åˆ¤","ç•°è­°","å–æ¶ˆ","å–ä¸‹ã’","äº‹ä»¶ç•ªå·","ä»£ç†äºº","å¼ç†å£«","ä¿‚å±","çµŒé",
    "ç¬¬","ç¬¬ä¸€","ç¬¬äºŒ","ç¬¬ä¸‰","ç¬¬1","ç¬¬ï¼’","ç¬¬ï¼“","ç¬¬ï¼‘","ç¬¬ï¼’","ç¬¬ï¼“","ä¸€","äºŒ","ä¸‰","å››","äº”","å…­","ä¸ƒ","å…«","ä¹","é›¶","æ•°","è¤‡åˆ","å¤šæ•°","å°‘æ•°","å›³1","å›³2","å›³3","å›³4","å›³5","å›³6","å›³7","å›³8","å›³9","è¡¨1","è¡¨2","è¡¨3","å¼1","å¼2","å¼3",
    "%","ï¼…","wt%","vol%","è³ªé‡%","é‡é‡%","å®¹é‡%","mol","mol%","mol/L","M","mm","cm","m","nm","Î¼m","Î¼","rpm","Pa","kPa","MPa","GPa","N","W","V","A","mA","Hz","kHz","MHz","GHz","â„ƒ","Â°C","K","mL","L","g","kg","mg","wt","vol","h","hr","hrs","min","s","sec","ppm","ppb","bar","Î©","ohm","J","kJ","Wh","kWh",
    "æ ªå¼ä¼šç¤¾","æœ‰é™ä¼šç¤¾","åˆè³‡ä¼šç¤¾","åˆåä¼šç¤¾","åˆåŒä¼šç¤¾","Inc","Inc.","Ltd","Ltd.","Co","Co.","Corp","Corp.","LLC", "GmbH","AG","BV","B.V.","S.A.","S.p.A.","ï¼ˆæ ªï¼‰","ãˆ±","ï¼ˆæœ‰ï¼‰",
    "æº¶æ¶²","æº¶åª’","è§¦åª’","åå¿œ","ç”Ÿæˆç‰©","åŸæ–™","æˆåˆ†","å«æœ‰","å«æœ‰é‡","é…åˆ","æ··åˆ","æ··åˆç‰©","æ¿ƒåº¦","æ¸©åº¦","æ™‚é–“","å‰²åˆ","æ¯”ç‡","åŸº","å®˜èƒ½åŸº","åŒ–åˆç‰©","çµ„æˆç‰©","æ¨¹è„‚","ãƒãƒªãƒãƒ¼","ãƒ¢ãƒãƒãƒ¼","åŸºæ¿","åŸºæ","ãƒ•ã‚£ãƒ«ãƒ ","ã‚·ãƒ¼ãƒˆ","ç²’å­","ç²‰æœ«","æ¯”è¼ƒä¾‹","å‚è€ƒä¾‹","è©¦é¨“","è©¦æ–™","è©•ä¾¡","æ¡ä»¶","å®Ÿé¨“","å®Ÿé¨“ä¾‹","åå¿œæ¡ä»¶","åå¿œæ™‚é–“","åå¿œæ¸©åº¦",
    "å‡¦ç†è£…ç½®","ç«¯æœ«","ãƒ¦ãƒ‹ãƒƒãƒˆ","ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«","å›è·¯","ç´ å­","é›»æº","é›»åœ§","é›»æµ","ä¿¡å·ç·š","é…ç·š","ç«¯å­","ç«¯éƒ¨","æ¥ç¶š", "æ¥ç¶šéƒ¨","æ¼”ç®—éƒ¨","è¨˜æ†¶éƒ¨","è¨˜æ†¶è£…ç½®","è¨˜éŒ²åª’ä½“","ãƒ¦ãƒ¼ã‚¶","åˆ©ç”¨è€…","ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ","ã‚µãƒ¼ãƒ","ç”»é¢","UI","GUI","ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹","ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹","DB","ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯","é€šä¿¡","è¦æ±‚","å¿œç­”","ãƒªã‚¯ã‚¨ã‚¹ãƒˆ","ãƒ¬ã‚¹ãƒãƒ³ã‚¹","ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿","å¼•æ•°","å±æ€§","ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£","ãƒ•ãƒ©ã‚°","ID","ãƒ•ã‚¡ã‚¤ãƒ«","ãƒ‡ãƒ¼ã‚¿æ§‹é€ ","ãƒ†ãƒ¼ãƒ–ãƒ«","ãƒ¬ã‚³ãƒ¼ãƒ‰",
    "è»¸","ã‚·ãƒ£ãƒ•ãƒˆ","ã‚®ã‚¢","ãƒ¢ãƒ¼ã‚¿","ã‚¨ãƒ³ã‚¸ãƒ³","ã‚¢ã‚¯ãƒãƒ¥ã‚¨ãƒ¼ã‚¿","ã‚»ãƒ³ã‚µ","ãƒãƒ«ãƒ–","ãƒãƒ³ãƒ—","ç­ä½“","ãƒã‚¦ã‚¸ãƒ³ã‚°","ãƒ•ãƒ¬ãƒ¼ãƒ ","ã‚·ãƒ£ãƒ¼ã‚·","é§†å‹•","ä¼é”","æ”¯æŒ","é€£çµ"
}

@st.cache_data
def _core_text_preprocessor(text):
    if not isinstance(text, str): return ""
    text = unicodedata.normalize('NFKC', text).lower()
    text = re.sub(r'[ï¼ˆ(][^ï¼‰)]{1,80}[ï¼‰)]', ' ', text)
    text = re.sub(r'(?:å›³|Fig|FIG|fig)[. ã€€]*\d+', ' ', text)
    text = re.sub(r'[!ï¼?"â€œâ€#$%ï¼†&\'()ï¼ˆï¼‰*ï¼‹+,\-ï¼.\:ï¼š;ï¼›<=>?ï¼Ÿ@\[\]ï¼»ï¼½\\^_`{|}~ã€œã€œï¼/]', ' ', text)
    return text

@st.cache_data
def advanced_tokenize_core(text):
    text = _core_text_preprocessor(text)
    if not text: return ""
    tokens = list(t.tokenize(text))
    processed_tokens = []
    i = 0
    while i < len(tokens):
        token1 = tokens[i]
        base1 = token1.base_form if token1.base_form != '*' else token1.surface
        if base1 in STOP_WORDS: i += 1; continue
        pos1 = token1.part_of_speech.split(',')
        if len(base1) < 2 and pos1[0] != 'åè©': i += 1; continue
        if pos1[0] == 'åè©' and (len(pos1) > 1 and pos1[1] == 'æ•°'): i += 1; continue
        if (i + 1) < len(tokens):
            token2 = tokens[i+1]
            base2 = token2.base_form if token2.base_form != '*' else token2.surface
            pos2 = token2.part_of_speech.split(',')
            if pos1[0] == 'åè©' and pos2[0] == 'åè©' and base2 not in STOP_WORDS and (len(pos2) > 1 and pos2[1] != 'æ•°'):
                processed_tokens.append(base1 + base2); i += 2; continue
        if pos1[0] == 'åè©' or (pos1[0] in ['å‹•è©', 'å½¢å®¹è©'] and len(pos1)>1 and pos1[1] == 'è‡ªç«‹'):
            processed_tokens.append(base1)
        i += 1
    return " ".join(processed_tokens)

# --- CORE æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ ---
def build_regex_pattern(keyword): return re.escape(keyword)
def build_near_regex(a, b, n): return r'(?:{}.{{0,{}}}?{}|{}.{{0,{}}}?{})'.format(a, n, b, b, n, a)
def build_adj_regex(a, b, n): return r'{}.{{0,{}}}?{}'.format(a, n, b)
def build_or_regex(a, b): return r'(?:{}|{})'.format(a, b)

def split_by_operator(text, operator):
    parts = []; balance = 0; current_chunk_start = 0
    for i, char in enumerate(text):
        if char == '(': balance += 1
        elif char == ')': balance -= 1
        elif char == operator and balance == 0:
            parts.append(text[current_chunk_start:i].strip()); current_chunk_start = i + 1
    parts.append(text[current_chunk_start:].strip())
    return parts

@st.cache_data
def parse_core_rule(rule_str):
    tokens = re.findall(r'\(|\)|' r'\bnear\d+\b|' r'\badj\d+\b|' r'[\+]|' r'[^()\s\+]+', rule_str, re.IGNORECASE)
    tokens = [t.strip() for t in tokens if t and t.strip()]
    output_queue, op_stack = [], []; op_precedence = {}
    for op in tokens:
        if op.lower() == '+': op_precedence[op] = 1
        elif op.lower().startswith(('near', 'adj')): op_precedence[op] = 3
    for token in tokens:
        if token == '(': op_stack.append(token)
        elif token == ')':
            while op_stack and op_stack[-1] != '(': output_queue.append(op_stack.pop())
            if op_stack: op_stack.pop()
        elif token.lower() in op_precedence:
            while (op_stack and op_stack[-1] != '(' and op_precedence.get(op_stack[-1].lower(), 0) >= op_precedence[token.lower()]):
                output_queue.append(op_stack.pop())
            op_stack.append(token)
        else: output_queue.append(token)
    while op_stack: output_queue.append(op_stack.pop())
    
    regex_stack = []
    for token in output_queue:
        if token.lower() not in op_precedence and token not in '()':
            norm = unicodedata.normalize('NFKC', token).lower()
            regex_stack.append(build_regex_pattern(norm))
        else:
            if len(regex_stack) < 2: raise ValueError(f"Invalid rule: {rule_str}")
            b, a = regex_stack.pop(), regex_stack.pop()
            tl = token.lower()
            if tl == '+': regex_stack.append(build_or_regex(a, b))
            elif tl.startswith('near'): regex_stack.append(build_near_regex(a, b, int(re.findall(r'\d+', tl)[0])))
            elif tl.startswith('adj'): regex_stack.append(build_adj_regex(a, b, int(re.findall(r'\d+', tl)[0])))
    if len(regex_stack) != 1: raise ValueError(f"Invalid rule: {rule_str}")
    return re.compile(regex_stack[0], re.IGNORECASE | re.DOTALL)

@st.cache_data
def prepare_axis_data_core(df, axis_col_name, delimiter):
    df_processed = df.copy()
    if axis_col_name not in df_processed.columns: return pd.DataFrame()
    df_processed[axis_col_name] = df_processed[axis_col_name].fillna('N/A')
    if axis_col_name == 'year':
        df_processed[axis_col_name] = df_processed[axis_col_name].apply(lambda x: str(int(x)) if pd.notna(x) else 'N/A')
    if delimiter:
        df_processed[axis_col_name] = df_processed[axis_col_name].astype(str).str.split(delimiter)
        df_processed = df_processed.explode(axis_col_name)
    df_processed[axis_col_name] = df_processed[axis_col_name].astype(str).str.strip().replace('', 'N/A')
    return df_processed

@st.cache_data
def convert_df_to_csv_core(df): return df.to_csv(encoding='utf-8-sig').encode('utf-8-sig')

# ==================================================================
# --- 4. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆæœŸåŒ– & UIæ§‹æˆ ---
# ==================================================================
with st.sidebar:
    st.title("APOLLO") 
    st.markdown("Advanced Patent & Overall Landscape-analytics Logic Orbiter")
    st.markdown("**v.3**")
    st.markdown("---")
    st.subheader("Home"); st.page_link("Home.py", label="Mission Control", icon="ğŸ›°ï¸")
    st.subheader("Modules")
    st.page_link("pages/1_ğŸŒ_ATLAS.py", label="ATLAS", icon="ğŸŒ")
    st.page_link("pages/2_ğŸ’¡_CORE.py", label="CORE", icon="ğŸ’¡")
    st.page_link("pages/3_ğŸš€_Saturn_V.py", label="Saturn V", icon="ğŸš€")
    st.page_link("pages/4_ğŸ“ˆ_MEGA.py", label="MEGA", icon="ğŸ“ˆ")
    st.page_link("pages/5_ğŸ§­_Explorer.py", label="Explorer", icon="ğŸ§­")
    st.page_link("pages/6_ğŸ”—_CREW.py", label="CREW", icon="ğŸ”—")
    st.markdown("---")
    st.caption("ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³:")
    st.caption("1. Mission Control ã§ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€å‰å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
    st.caption("2. ä¸Šã®ãƒªã‚¹ãƒˆã‹ã‚‰åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é¸æŠã—ã¾ã™ã€‚")
    st.markdown("---")
    st.caption("Â© 2025 ã—ã°ã‚„ã¾")

st.title("ğŸ’¡ CORE")
st.markdown("Contextual Operator & Rule Engine: **è«–ç†å¼ãƒ™ãƒ¼ã‚¹ã®ç‰¹è¨±åˆ†é¡ãƒ„ãƒ¼ãƒ«**ã§ã™ã€‚")

col_theme, _ = st.columns([1, 3])
with col_theme:
    selected_theme = st.selectbox("è¡¨ç¤ºãƒ†ãƒ¼ãƒ:", ["APOLLO Standard", "Modern Presentation"], key="core_theme_selector")
theme_config = get_theme_config(selected_theme)
st.markdown(f"<style>{theme_config['css']}</style>", unsafe_allow_html=True)

if not st.session_state.get("preprocess_done", False):
    st.error("åˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"); st.stop()
else:
    df_main = st.session_state.df_main
    col_map = st.session_state.col_map

if "core_classification_rules" not in st.session_state: st.session_state.core_classification_rules = {}
if "core_df_classified" not in st.session_state: st.session_state.core_df_classified = None
if "core_current_axis" not in st.session_state: st.session_state.core_current_axis = ""
if "core_reanalyze_result" not in st.session_state: st.session_state.core_reanalyze_result = ""

# ==================================================================
# --- 5. CORE ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ ---
# ==================================================================
current_phase = st.radio("ãƒ•ã‚§ãƒ¼ã‚ºé¸æŠ:", ["ãƒ•ã‚§ãƒ¼ã‚º 1: AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ (KMeans)", "ãƒ•ã‚§ãƒ¼ã‚º 2: åˆ†é¡ãƒ«ãƒ¼ãƒ«å®šç¾©", "ãƒ•ã‚§ãƒ¼ã‚º 3: åˆ†é¡å®Ÿè¡Œ", "ãƒ•ã‚§ãƒ¼ã‚º 4: ç‰¹è¨±ãƒãƒƒãƒ—ä½œæˆ"], horizontal=True, key="core_phase_selector")
st.markdown("---")

# --- ãƒ•ã‚§ãƒ¼ã‚º 1: AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ ---
if current_phase.startswith("ãƒ•ã‚§ãƒ¼ã‚º 1"):
    st.subheader("ãƒ•ã‚§ãƒ¼ã‚º 1: AIã«ã‚ˆã‚‹åˆ†é¡ã‚µã‚¸ã‚§ã‚¹ãƒˆ (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)")
    col_map_options = [v for k, v in col_map.items() if k in ['title', 'abstract', 'claim']]
    target_column = st.selectbox("åˆ†æå¯¾è±¡ã‚«ãƒ©ãƒ :", options=col_map_options, key="core_target_col")
    
    col1, col2 = st.columns(2)
    with col1: ai_k_w = st.number_input("ãƒˆãƒ”ãƒƒã‚¯æ•° (K)", min_value=2, value=8, key="core_k")
    with col2: ai_n_w = st.number_input("ã‚µãƒ³ãƒ—ãƒ«æ•° (N)", min_value=1, value=5, key="core_n")
    
    use_mece = st.checkbox("MECEãƒ¢ãƒ¼ãƒ‰ (è‡ªå‹•æ±ºå®š)", value=True, key="core_use_mece")
    
    if not use_mece:
        st.markdown("<b>ç”Ÿæˆã™ã‚‹åˆ†é¡ã®æ•° (æ‰‹å‹•è¨­å®š):</b>", unsafe_allow_html=True)
        col_c1, col_c2, col_c3 = st.columns(3)
        with col_c1: ai_cat_count_tech = st.number_input("æŠ€è¡“åˆ†é¡:", min_value=1, value=6, key="core_cat_tech")
        with col_c2: ai_cat_count_prob = st.number_input("èª²é¡Œåˆ†é¡:", min_value=1, value=6, key="core_cat_prob")
        with col_c3: ai_cat_count_sol = st.number_input("è§£æ±ºæ‰‹æ®µåˆ†é¡:", min_value=1, value=6, key="core_cat_sol")

    if st.button("AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ", key="core_run_ai"):
        with st.spinner("åˆ†æä¸­..."):
            try:
                texts_raw = df_main[target_column].astype(str).fillna('')
                tokenized_texts = texts_raw.apply(advanced_tokenize_core)
                vec = TfidfVectorizer(min_df=1, max_df=0.9, token_pattern=r"(?u)\b\w+\b")
                tfidf = vec.fit_transform(tokenized_texts)
                km = KMeans(n_clusters=int(ai_k_w), random_state=42, n_init=10).fit(tfidf)
                
                sampled_docs = []
                for i in range(int(ai_k_w)):
                    c_idx = np.where(km.labels_ == i)[0]
                    if len(c_idx) == 0: continue
                    dists = euclidean_distances(tfidf[c_idx], km.cluster_centers_[i].reshape(1,-1))
                    top_idx = c_idx[dists.flatten().argsort()[:int(ai_n_w)]]
                    sampled_docs.append(f"\n--- Cluster {i} ---\n" + "\n".join([f"ãƒ»{_core_text_preprocessor(texts_raw.iloc[idx])}" for idx in top_idx]))
                
                if use_mece:
                    instruction_text = (
                        "ã“ã®ç‰¹è¨±æ¯é›†å›£å…¨ä½“ã‚’ç¶²ç¾…çš„ã«åˆ†é¡ã™ã‚‹ãŸã‚ã®ã€**ã€ŒæŠ€è¡“åˆ†é¡ã€ã€Œèª²é¡Œåˆ†é¡ã€ã€Œè§£æ±ºæ‰‹æ®µåˆ†é¡ã€**ã®3ã¤ã®åˆ†é¡è»¸ã«ã¤ã„ã¦ã€**åˆ†é¡å®šç¾©**ï¼ˆåˆ†é¡åã€å®šç¾©ã€COREè«–ç†å¼ã®ã‚»ãƒƒãƒˆï¼‰ã‚’è¨­è¨ˆã—ã¦ãã ã•ã„ã€‚\n"
                        "\n# é‡è¦: MECE (Mutually Exclusive, Collectively Exhaustive) ã®åŸå‰‡\n"
                        "- ç”Ÿæˆã™ã‚‹å„åˆ†é¡è»¸å†…ã®ã‚«ãƒ†ã‚´ãƒªã¯ã€ç›¸äº’ã«æ’ä»–çš„ï¼ˆãƒ€ãƒ–ã‚ŠãŒãªã„ï¼‰ã§ã‚ã‚Šã€ã‹ã¤å…¨ä½“ã¨ã—ã¦ç¶²ç¾…çš„ï¼ˆãƒ¢ãƒ¬ãŒãªã„ï¼‰ã§ã‚ã‚‹ã‚ˆã†ã«è¨­è¨ˆã—ã¦ãã ã•ã„ã€‚\n"
                        "- å„è»¸ã®ã‚«ãƒ†ã‚´ãƒªæ•°ã¯ã€MECEã‚’æº€ãŸã™ã®ã«æœ€é©ã ã¨ã‚ãªãŸãŒåˆ¤æ–­ã™ã‚‹æ•°ï¼ˆç›®å®‰ã¨ã—ã¦5ã€œ10å€‹ç¨‹åº¦ï¼‰ã«ã—ã¦ãã ã•ã„ã€‚"
                    )
                else:
                    instruction_text = "\n".join([
                        "ã“ã®ç‰¹è¨±æ¯é›†å›£å…¨ä½“ã‚’ç¶²ç¾…çš„ã«åˆ†é¡ã™ã‚‹ãŸã‚ã®ã€ä»¥ä¸‹ã®3ã¤ã®åˆ†é¡è»¸ã«ã¤ã„ã¦ã€æŒ‡å®šã•ã‚ŒãŸå€‹æ•°ã§**åˆ†é¡å®šç¾©**ã‚’è¨­è¨ˆã—ã¦ãã ã•ã„ã€‚",
                        f"- **æŠ€è¡“åˆ†é¡**: {ai_cat_count_tech}å€‹",
                        f"- **èª²é¡Œåˆ†é¡**: {ai_cat_count_prob}å€‹",
                        f"- **è§£æ±ºæ‰‹æ®µåˆ†é¡**: {ai_cat_count_sol}å€‹"
                    ])

                sampled_docs_str = "".join(sampled_docs)

                prompt = f"""
ã‚ãªãŸã¯å„ªç§€ãªç‰¹è¨±æƒ…å ±ã‚¹ãƒˆãƒ©ãƒ†ã‚¸ã‚¹ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®ã€Œä»£è¡¨æ–‡çŒ®ã‚µãƒ³ãƒ—ãƒ«ã€ã¯ã€ã‚ã‚‹ç‰¹è¨±æ¯é›†å›£ï¼ˆ{len(df_main)}ä»¶ï¼‰ã‚’K-Meansæ³•ã§{ai_k_w}å€‹ã®ã‚¯ãƒ©ã‚¹ã‚¿ã«åˆ†é¡ã—ã€å„ã‚¯ãƒ©ã‚¹ã‚¿ã‹ã‚‰ä»£è¡¨çš„ãªæ–‡çŒ®ã®ã€Œ{target_column}ã€ã‚’{ai_n_w}ä»¶ãšã¤æŠ½å‡ºã—ãŸã‚‚ã®ã§ã™ã€‚

# ä¾é ¼å†…å®¹
{instruction_text}

ä»¥ä¸‹ã®å½¢å¼ã® **JSONãƒ‡ãƒ¼ã‚¿ã®ã¿** ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚è§£èª¬ã¯ä¸è¦ã§ã™ã€‚
JSONã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ã‚·ã‚¹ãƒ†ãƒ ã«ãã®ã¾ã¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚

# JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ (å³å®ˆ)
{{
  "æŠ€è¡“åˆ†é¡": [
    {{
      "name": "ã‚«ãƒ†ã‚´ãƒªå (ä¾‹: CO2åˆ†é›¢è†œ)",
      "definition": "ã‚«ãƒ†ã‚´ãƒªã®å®šç¾©...",
      "rule": "COREè«–ç†å¼ (ä¾‹: (CO2 + äºŒé…¸åŒ–ç‚­ç´ ) * (è†œ + ãƒ¡ãƒ³ãƒ–ãƒ¬ãƒ³))"
    }},
    ...
  ],
  "èª²é¡Œåˆ†é¡": [ ... ],
  "è§£æ±ºæ‰‹æ®µåˆ†é¡": [ ... ]
}}

# COREè«–ç†å¼æ–‡æ³• (å³å®ˆ)
- `A + B` (OR): A ã¾ãŸã¯ B
- `A * B` (AND): A ã‹ã¤ B (é †åºå•ã‚ãš)
- `A nearN B` (è¿‘å‚): Aã¨BãŒ**Næ–‡å­—**ä»¥å†…ã§å‡ºç¾ (é †åºå•ã‚ãš)ã€‚Nã¯10ã€œ40ç¨‹åº¦ã‚’æ¨å¥¨ã€‚
- `A adjN B` (é †åºæŒ‡å®šè¿‘å‚): AãŒBã®**Næ–‡å­—**ä»¥å†…ã«Aâ†’Bã®é †ã§å‡ºç¾ã€‚Nã¯1ã€œ10ç¨‹åº¦ã‚’æ¨å¥¨ã€‚
- **é‡è¦:** ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯ã‚¹ãƒšãƒ¼ã‚¹ã‚’å«ã¾ãªã„å˜ä¸€èªï¼ˆä¾‹: `äºŒé…¸åŒ–ç‚­ç´ `ï¼‰ã«ã—ã¦ãã ã•ã„ã€‚ã‚¹ãƒšãƒ¼ã‚¹ã‚’å«ã‚€ãƒ•ãƒ¬ãƒ¼ã‚ºã¯ `adj1` ã§è¡¨ç¾ã—ã¦ãã ã•ã„ã€‚

# æœ€é‡è¦ãƒ«ãƒ¼ãƒ« (ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ‹¡å¼µã¨è¡¨è¨˜ã‚†ã‚Œ)
- ã‚µãƒ³ãƒ—ãƒ«ã«å­˜åœ¨ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãã®ã¾ã¾ä½¿ã†ã ã‘ã§ã¯ä¸ååˆ†ã§ã™ã€‚
- AIã®çŸ¥è­˜ã‚’æ´»ç”¨ã—ã€ãã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®**é¡ç¾©èªã€é–¢é€£èªã€ä¸Šä½/ä¸‹ä½æ¦‚å¿µã€ç‰¹è¨±ç‰¹æœ‰ã®è¡¨ç¾ã€è¡¨è¨˜ã‚†ã‚Œï¼ˆã‚«ã‚¿ã‚«ãƒŠã€ã²ã‚‰ãŒãªã€æ¼¢å­—ï¼‰**ã‚’ã€ã‚ãªãŸã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ç¶²ç¾…çš„ã«æƒ³èµ·ã—ã¦ãã ã•ã„ã€‚
- **ç‰¹è¨±ç”¨èªã®ç¶²ç¾…:** ï¼ˆä¾‹: ã€Œä¿æŒã€â†’ã€Œæ‹…æŒã€ã€Œå›ºç€ã€ã€Œä¿‚æ­¢ã€ãªã©ã€ç‰¹è¨±ã§ä½¿ã‚ã‚Œã‚‹è¨€ã„æ›ãˆã‚’ç¶²ç¾…ï¼‰
- **æ¦‚å¿µã®éšå±¤åŒ–:** ä¸Šä½æ¦‚å¿µï¼ˆä¾‹: ã€Œè»Šä¸¡ã€ï¼‰ã¨ä¸‹ä½æ¦‚å¿µï¼ˆä¾‹: ã€Œè‡ªå‹•è»Šã€ã€ŒäºŒè¼ªè»Šã€ï¼‰ã®ä¸¡æ–¹ã‚’å«ã‚ã€å–ã‚Šã“ã¼ã—ã‚’é˜²ãã¾ã™ã€‚
- **ã‚«ã‚¿ã‚«ãƒŠ:** ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ã‚«ã‚¿ã‚«ãƒŠã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€**å¿…ãšå…¨è§’ï¼ˆä¾‹: `ãƒãƒªãƒãƒ¼`ï¼‰**ã‚’ä½¿ç”¨ã—ã€**åŠè§’ï¼ˆä¾‹: `ï¾ï¾Ÿï¾˜ï¾ï½°`ï¼‰ã¯çµ¶å¯¾ã«ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„**ã€‚

# ä»£è¡¨æ–‡çŒ®ã‚µãƒ³ãƒ—ãƒ«
{sampled_docs_str}
"""
                st.success("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚å³ä¸Šã®ã‚³ãƒ”ãƒ¼ãƒœã‚¿ãƒ³ã§ã‚³ãƒ”ãƒ¼ã—ã¦ãã ã•ã„ã€‚")
                st.code(prompt, language='markdown')
            except Exception as e: st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

# --- ãƒ•ã‚§ãƒ¼ã‚º 2: åˆ†é¡ãƒ«ãƒ¼ãƒ«å®šç¾© ---
elif current_phase.startswith("ãƒ•ã‚§ãƒ¼ã‚º 2"):
    st.subheader("ãƒ•ã‚§ãƒ¼ã‚º 2: åˆ†é¡ãƒ«ãƒ¼ãƒ«å®šç¾©")
    
    tab_manual, tab_json = st.tabs(["æ‰‹å‹•è¿½åŠ ãƒ»ä¿®æ­£", "JSONä¸€æ‹¬ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"])
    existing = list(st.session_state.core_classification_rules.keys())
    
    with tab_manual:
        is_edit_mode = "core_edit_target" in st.session_state and st.session_state.core_edit_target is not None
        
        mode = st.radio("è»¸ã®æŒ‡å®š:", ["æ–°è¦ä½œæˆ", "æ—¢å­˜ã«è¿½åŠ "], horizontal=True, index=1 if is_edit_mode else 0)
        
        if mode == "æ—¢å­˜ã«è¿½åŠ " and existing:
            default_idx = 0
            if is_edit_mode:
                try: default_idx = existing.index(st.session_state.core_edit_target["axis"])
                except: pass
            elif st.session_state.core_current_axis in existing:
                try: default_idx = existing.index(st.session_state.core_current_axis)
                except: pass
            axis = st.selectbox("è¿½åŠ /ä¿®æ­£å…ˆã®è»¸:", existing, index=default_idx)
        else:
            axis = st.text_input("æ–°è¦è»¸å:", value=st.session_state.core_edit_target["axis"] if is_edit_mode else "", placeholder="ä¾‹: èª²é¡Œåˆ†é¡")
            
        c_name = st.text_input("åˆ†é¡å:", value=st.session_state.core_edit_target["cat"] if is_edit_mode else "", placeholder="ä¾‹: è€ä¹…æ€§å‘ä¸Š")
        c_def = st.text_area("å®šç¾©:", value=st.session_state.core_edit_target["def"] if is_edit_mode else "", height=68)
        c_rule = st.text_input("è«–ç†å¼:", value=st.session_state.core_edit_target["rule"] if is_edit_mode else "", placeholder="(è€ä¹…æ€§ + å¯¿å‘½) * å‘ä¸Š")
        
        btn_label = "ãƒ«ãƒ¼ãƒ«ã‚’æ›´æ–°" if is_edit_mode else "ãƒ«ãƒ¼ãƒ«ã‚’è¿½åŠ "
        
        if st.button(btn_label, key="add_manual"):
            if all([axis, c_name, c_rule]):
                try:
                    parse_core_rule(c_rule)
                    if axis not in st.session_state.core_classification_rules:
                        st.session_state.core_classification_rules[axis] = {}
                    st.session_state.core_classification_rules[axis][c_name] = {'rule': c_rule, 'definition': c_def}
                    st.session_state.core_current_axis = axis
                    if is_edit_mode: del st.session_state.core_edit_target
                    st.success(f"{btn_label}ã—ã¾ã—ãŸ: {c_name}")
                    st.rerun()
                except Exception as e: st.error(f"æ–‡æ³•ã‚¨ãƒ©ãƒ¼: {e}")
        
        if is_edit_mode:
            if st.button("ç·¨é›†ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«"):
                del st.session_state.core_edit_target
                st.rerun()
    
    with tab_json:
        st.markdown("AIãŒç”Ÿæˆã—ãŸJSONã‚’ã“ã“ã«è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„ã€‚æ—¢å­˜ã®ãƒ«ãƒ¼ãƒ«ã¯ç¶­æŒã•ã‚Œã€æ–°ã—ã„è»¸ãŒè¿½åŠ ã•ã‚Œã¾ã™ã€‚")
        json_input = st.text_area("JSONå…¥åŠ›:", height=300)
        if st.button("JSONã‚’ä¸€æ‹¬ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"):
            try:
                cleaned_json = re.sub(r'^```json\s*|\s*```$', '', json_input.strip(), flags=re.MULTILINE)
                data = json.loads(cleaned_json)
                count = 0
                for axis_name, categories in data.items():
                    if axis_name not in st.session_state.core_classification_rules:
                        st.session_state.core_classification_rules[axis_name] = {}
                    for cat in categories:
                        name = cat.get('name'); rule = cat.get('rule'); defn = cat.get('definition', '')
                        if name and rule:
                            st.session_state.core_classification_rules[axis_name][name] = {'rule': rule, 'definition': defn}
                            count += 1
                st.success(f"{count} å€‹ã®ãƒ«ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã—ãŸï¼")
                st.rerun()
            except Exception as e: st.error(f"JSONãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")

    st.markdown("---")
    st.subheader("ç¾åœ¨ã®ãƒ«ãƒ¼ãƒ«ä¸€è¦§")
    
    if st.button("å…¨ãƒ«ãƒ¼ãƒ«ã‚’å‰Šé™¤", type="primary"):
        st.session_state.core_classification_rules = {}
        st.rerun()
        
    for ax, cats in st.session_state.core_classification_rules.items():
        with st.expander(f"è»¸: {ax} ({len(cats)}ä»¶)"):
            for cn, cd in cats.items():
                r = cd['rule'] if isinstance(cd, dict) else cd[0]
                d = cd.get('definition', '') if isinstance(cd, dict) else ""
                
                c1, c2, c3 = st.columns([1, 4, 1])
                with c1:
                    if st.button("ç·¨é›†", key=f"edit_{ax}_{cn}"):
                        st.session_state.core_edit_target = {"axis": ax, "cat": cn, "rule": r, "def": d}
                        st.rerun()
                with c2:
                    st.text(f"ã€{cn}ã€‘ {r}")
                with c3:
                    if st.button("å‰Šé™¤", key=f"del_{ax}_{cn}"):
                        del st.session_state.core_classification_rules[ax][cn]
                        if not st.session_state.core_classification_rules[ax]:
                            del st.session_state.core_classification_rules[ax]
                        st.rerun()

# --- ãƒ•ã‚§ãƒ¼ã‚º 3: åˆ†é¡å®Ÿè¡Œ ---
elif current_phase.startswith("ãƒ•ã‚§ãƒ¼ã‚º 3"):
    st.subheader("ãƒ•ã‚§ãƒ¼ã‚º 3: åˆ†é¡å®Ÿè¡Œ")
    
    st.info("â€» æ¢ç´¢ç¯„å›²ã¯è‡ªå‹•çš„ã«ã€Œç™ºæ˜ã®åç§° + è¦ç´„ + è«‹æ±‚é …ã€ã®çµåˆãƒ†ã‚­ã‚¹ãƒˆã¨ãªã‚Šã¾ã™ã€‚")
    
    if st.button("ã™ã¹ã¦ã®åˆ†é¡ã‚’å®Ÿè¡Œ", type="primary"):
        if not st.session_state.core_classification_rules:
            st.error("ãƒ«ãƒ¼ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            with st.spinner("å®Ÿè¡Œä¸­..."):
                try:
                    df_res = df_main.copy()
                    
                    search_cols = []
                    if col_map.get('title') in df_res.columns: search_cols.append(df_res[col_map['title']].fillna(''))
                    if col_map.get('abstract') in df_res.columns: search_cols.append(df_res[col_map['abstract']].fillna(''))
                    if col_map.get('claim') in df_res.columns: search_cols.append(df_res[col_map['claim']].fillna(''))
                    
                    combined_text = search_cols[0]
                    for s in search_cols[1:]:
                        combined_text = combined_text + " " + s
                    
                    rules = st.session_state.core_classification_rules
                    compiled_rules = {}
                    for ax, cats in rules.items():
                        compiled_rules[ax] = []
                        for cn, cd in cats.items():
                            r_str = cd['rule'] if isinstance(cd, dict) else cd[0]
                            or_parts = split_by_operator(r_str, '+')
                            comp_or = []
                            for op in or_parts:
                                and_parts = split_by_operator(op, '*')
                                comp_and = [parse_core_rule(ap.strip()) for ap in and_parts]
                                comp_or.append(comp_and)
                            compiled_rules[ax].append((cn, comp_or))
                    
                    def apply_rules(text, ax_rules):
                        text = _core_text_preprocessor(str(text))
                        hits = []
                        for c_name, c_logic in ax_rules:
                            match_or = False
                            for and_block in c_logic:
                                match_and = True
                                for regex in and_block:
                                    if not regex.search(text): match_and = False; break
                                if match_and: match_or = True; break
                            if match_or: hits.append(c_name)
                        return ";".join(hits) if hits else "ãã®ä»–"

                    bar = st.progress(0)
                    for i, ax in enumerate(rules.keys()):
                        df_res[ax] = combined_text.apply(lambda x: apply_rules(x, compiled_rules[ax]))
                        bar.progress((i+1)/len(rules))
                    
                    st.session_state.core_df_classified = df_res
                    st.success("å®Œäº†ï¼")
                    
                    st.subheader("åˆ†é¡çµæœã‚µãƒãƒªãƒ¼")
                    cols = st.columns(len(rules))
                    for i, ax in enumerate(rules.keys()):
                        with cols[i]:
                            st.markdown(f"**{ax}**")
                            counts = df_res[ax].str.split(';').explode().value_counts()
                            st.dataframe(counts)
                    
                    csv_core = convert_df_to_csv_core(df_res)
                    st.download_button("åˆ†é¡çµæœCSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", csv_core, "CORE_classified.csv", "text/csv")
                    
                except Exception as e: st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

    st.markdown("---")
    st.subheader("ğŸ” æœªåˆ†é¡ãƒ‡ãƒ¼ã‚¿ã®å†åˆ†æ (ã€ãã®ä»–ã€ã‚’æ¸›ã‚‰ã™)")
    if st.session_state.core_df_classified is not None:
        rules = st.session_state.core_classification_rules
        if rules:
            col_re1, col_re2 = st.columns(2)
            with col_re1: reanalyze_axis = st.selectbox("å†åˆ†æã™ã‚‹è»¸ã‚’é¸æŠ:", list(rules.keys()), key="core_reanalyze_axis")
            
            col_k, col_n = st.columns(2)
            with col_k: re_k = st.number_input("æŠ½å‡ºãƒˆãƒ”ãƒƒã‚¯æ•° (K)", value=5, key="re_k")
            with col_n: re_n = st.number_input("1ãƒˆãƒ”ãƒƒã‚¯ã‚ãŸã‚Šã®ã‚µãƒ³ãƒ—ãƒ«æ•° (N)", value=3, key="re_n")
            
            re_mece = st.checkbox("MECEãƒ¢ãƒ¼ãƒ‰ (è‡ªå‹•)", value=True, key="re_mece")
            re_cnt = 3 if re_mece else st.number_input("è¿½åŠ ã™ã‚‹ã‚«ãƒ†ã‚´ãƒªæ•°", value=3, key="re_cnt")

            if st.button("ã€ãã®ä»–ã€ã‚’åˆ†æã—ã¦æ–°ãƒ«ãƒ¼ãƒ«ã‚’ææ¡ˆ", key="core_btn_reanalyze"):
                try:
                    df_c = st.session_state.core_df_classified
                    others_df = df_c[df_c[reanalyze_axis] == 'ãã®ä»–']
                    if others_df.empty:
                        st.info("ã€ãã®ä»–ã€ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
                    else:
                        with st.spinner(f"ã€ãã®ä»–ã€({len(others_df)}ä»¶) ã‚’åˆ†æä¸­..."):
                            search_cols = []
                            if col_map.get('title') in others_df.columns: search_cols.append(others_df[col_map['title']].fillna(''))
                            if col_map.get('abstract') in others_df.columns: search_cols.append(others_df[col_map['abstract']].fillna(''))
                            if col_map.get('claim') in others_df.columns: search_cols.append(others_df[col_map['claim']].fillna(''))
                            texts = search_cols[0]
                            for s in search_cols[1:]: texts = texts + " " + s
                            
                            toks = texts.apply(advanced_tokenize_core)
                            vec = TfidfVectorizer(min_df=1, max_df=0.9, token_pattern=r"(?u)\b\w+\b")
                            tfidf = vec.fit_transform(toks)
                            
                            actual_k = min(int(re_k), len(others_df))
                            if actual_k < 2: actual_k = 1
                            km = KMeans(n_clusters=actual_k, random_state=42).fit(tfidf)
                            
                            s_docs = []
                            for i in range(actual_k):
                                c_idx = np.where(km.labels_ == i)[0]
                                if len(c_idx) == 0: continue
                                dists = euclidean_distances(tfidf[c_idx], km.cluster_centers_[i].reshape(1,-1))
                                top_idx = c_idx[dists.flatten().argsort()[:int(re_n)]]
                                s_docs.append(f"\n--- ãã®ä»–ã‚°ãƒ«ãƒ¼ãƒ— {i} ---\n" + "\n".join([f"ãƒ»{_core_text_preprocessor(texts.iloc[idx])}" for idx in top_idx]))
                            
                            s_docs_str = "".join(s_docs)
                            exist_rules = [f"- {cat}: {d['rule']}" for cat, d in rules[reanalyze_axis].items()]
                            exist_rules_str = "\n".join(exist_rules)
                            
                            instruction_part = "MECEã‚’æ„è­˜ã—ã€ã‚«ãƒ†ã‚´ãƒªæ•°ã¯è‡ªå‹•ã§æœ€é©åŒ–ã—ã¦ãã ã•ã„ã€‚" if re_mece else f"**{re_cnt}å€‹** ã®æ–°ã—ã„ã‚«ãƒ†ã‚´ãƒªã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚"
                            
                            p_re = f"""
ã‚ãªãŸã¯ç‰¹è¨±æƒ…å ±ã‚¹ãƒˆãƒ©ãƒ†ã‚¸ã‚¹ãƒˆã§ã™ã€‚
ç¾åœ¨ã€åˆ†é¡è»¸ã€Œ{reanalyze_axis}ã€ã‚’ä½œæˆä¸­ã§ã™ãŒã€ä»¥ä¸‹ã®ã€Œæ—¢å­˜ã®åˆ†é¡ã€ã«å½“ã¦ã¯ã¾ã‚‰ãªã„ç‰¹è¨±ãŒã€Œãã®ä»–ã€ã¨ã—ã¦æ®‹ã£ã¦ã„ã¾ã™ã€‚

# æ—¢å­˜ã®åˆ†é¡ãƒªã‚¹ãƒˆ
{exist_rules_str}

# ä¾é ¼å†…å®¹
ä»¥ä¸‹ã®ã€Œæœªåˆ†é¡ç‰¹è¨±ã®ã‚µãƒ³ãƒ—ãƒ«ã€ã‚’åˆ†æã—ã€**æ—¢å­˜ã®åˆ†é¡ã¨ã¯æ¦‚å¿µçš„ã«é‡è¤‡ã—ãªã„ã€æ–°ã—ã„åˆ†é¡ã‚«ãƒ†ã‚´ãƒª**ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚
å‡ºåŠ›ã¯ **JSONå½¢å¼ã®ã¿** ã¨ã—ã¦ãã ã•ã„ã€‚
{instruction_part}

# JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
{{
  "{reanalyze_axis}": [
    {{
      "name": "æ–°ã‚«ãƒ†ã‚´ãƒªå",
      "definition": "...",
      "rule": "è«–ç†å¼"
    }}, ...
  ]
}}

# æœªåˆ†é¡ç‰¹è¨±ã®ã‚µãƒ³ãƒ—ãƒ«
{s_docs_str}
"""
                            st.session_state.core_reanalyze_result = p_re
                except Exception as e: st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
        
        if st.session_state.core_reanalyze_result:
            st.success("å†åˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚"); st.code(st.session_state.core_reanalyze_result, language='markdown')

# --- ãƒ•ã‚§ãƒ¼ã‚º 4: ç‰¹è¨±ãƒãƒƒãƒ— ---
elif current_phase.startswith("ãƒ•ã‚§ãƒ¼ã‚º 4"):
    st.subheader("ãƒ•ã‚§ãƒ¼ã‚º 4: ç‰¹è¨±ãƒãƒƒãƒ—ä½œæˆ")
    
    if st.session_state.core_df_classified is None:
        st.warning("å…ˆã«åˆ†é¡ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    else:
        df_c = st.session_state.core_df_classified
        axes = list(st.session_state.core_classification_rules.keys())
        meta_axes = []
        if 'year' in df_c.columns: meta_axes.append('å‡ºé¡˜å¹´')
        if col_map.get('applicant') in df_c.columns: meta_axes.append('å‡ºé¡˜äºº')
        all_axes = axes + meta_axes
        
        c1, c2, c3 = st.columns(3)
        with c1: x_ax = st.selectbox("Xè»¸", all_axes, index=0)
        with c2: y_ax = st.selectbox("Yè»¸", all_axes, index=min(1, len(all_axes)-1))
        with c3: chart_type = st.radio("ã‚°ãƒ©ãƒ•ã‚¿ã‚¤ãƒ—", ["ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—", "ãƒãƒ–ãƒ«ãƒãƒ£ãƒ¼ãƒˆ"])
        
        col_f1, col_f2 = st.columns(2)
        with col_f1: exclude_other = st.checkbox("ã€Œãã®ä»–ã€ã‚’é™¤å¤–ã™ã‚‹", value=True)
        
        if st.button("æç”»"):
            def get_col_data(ax_name):
                if ax_name == 'å‡ºé¡˜å¹´': return df_c['year'].fillna(0).astype(int).astype(str), None
                if ax_name == 'å‡ºé¡˜äºº': return df_c[col_map['applicant']].fillna('Unknown'), ';' 
                if ax_name in axes: return df_c[ax_name], ';'
                return None, None

            x_data, x_sep = get_col_data(x_ax); y_data, y_sep = get_col_data(y_ax)
            temp_df = pd.DataFrame({'X': x_data, 'Y': y_data})
            if x_sep: temp_df['X'] = temp_df['X'].astype(str).str.split(x_sep); temp_df = temp_df.explode('X')
            if y_sep: temp_df['Y'] = temp_df['Y'].astype(str).str.split(y_sep); temp_df = temp_df.explode('Y')
            
            temp_df = temp_df.replace({'nan': np.nan, 'None': np.nan}).dropna()
            if exclude_other:
                temp_df = temp_df[(temp_df['X'] != 'ãã®ä»–') & (temp_df['Y'] != 'ãã®ä»–')]
            
            if temp_df.empty: st.warning("ãƒ‡ãƒ¼ã‚¿ãªã—")
            else:
                ct = pd.crosstab(temp_df['Y'], temp_df['X'])
                
                if x_ax == 'å‡ºé¡˜å¹´': x_ord = sorted(ct.columns, key=lambda x: int(x) if x.isdigit() else x)
                else: x_ord = ct.sum(axis=0).sort_values(ascending=False).index.tolist()
                
                if y_ax == 'å‡ºé¡˜å¹´': y_ord = sorted(ct.index, key=lambda x: int(x) if x.isdigit() else x)
                else: y_ord = ct.sum(axis=1).sort_values(ascending=False).index.tolist()
                
                ct = ct.reindex(index=y_ord, columns=x_ord).fillna(0)
                
                if chart_type == "ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—":
                    fig = px.imshow(
                        ct, 
                        labels=dict(x=x_ax, y=y_ax, color="ä»¶æ•°"),
                        x=ct.columns,
                        y=ct.index,
                        aspect="auto",
                        color_continuous_scale='YlGnBu',
                        text_auto=True
                    )
                    
                    fig.update_layout(
                        title=f"{x_ax} Ã— {y_ax}",
                        height=max(600, len(ct)*40),
                        yaxis=dict(title=y_ax),
                        xaxis=dict(title=x_ax, side='bottom')
                    )
                    st.plotly_chart(fig, use_container_width=True)
                    
                else: # ãƒãƒ–ãƒ«ãƒãƒ£ãƒ¼ãƒˆ
                    ct_long = ct.reset_index().melt(id_vars='Y', var_name='X', value_name='Count')
                    ct_long = ct_long[ct_long['Count'] > 0]
                    atlas_colors = theme_config["color_sequence"]
                    
                    fig = px.scatter(
                        ct_long, x='X', y='Y', size='Count', color='Y',
                        size_max=60, color_discrete_sequence=atlas_colors,
                        category_orders={'X': x_ord, 'Y': y_ord} 
                    )
                    fig.update_yaxes(categoryorder='array', categoryarray=y_ord, autorange='reversed', title=y_ax, type='category')
                    fig.update_xaxes(categoryorder='array', categoryarray=x_ord, title=x_ax, side='bottom', type='category')
                    
                    fig.update_layout(title=f"{x_ax} Ã— {y_ax}", height=max(600, len(ct)*40), showlegend=False)
                    st.plotly_chart(fig, use_container_width=True)
        
        # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        st.markdown("---")
        csv_core = convert_df_to_csv_core(df_c)
        st.download_button("åˆ†é¡çµæœä»˜ãå…¨ãƒ‡ãƒ¼ã‚¿CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", csv_core, "CORE_classified_full.csv", "text/csv")