import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
import plotly.io as pio
import warnings
import unicodedata
import re
import json
import traceback

from janome.tokenizer import Tokenizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import euclidean_distances
import utils

# ==================================================================
# --- 1. ãƒšãƒ¼ã‚¸è¨­å®š ---
# ==================================================================
st.set_page_config(
    page_title="APOLLO | CORE", 
    page_icon="ğŸ’¡", 
    layout="wide"
)

pio.templates.default = "plotly_white"
warnings.filterwarnings('ignore')

# ==================================================================
# --- 2. ãƒ‡ã‚¶ã‚¤ãƒ³ãƒ†ãƒ¼ãƒè¨­å®š & å…±é€šCSS ---
# ==================================================================




# ==================================================================
# --- 3. ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° & ãƒªã‚½ãƒ¼ã‚¹ãƒ­ãƒ¼ãƒ‰ ---
# ==================================================================
@st.cache_resource
def load_tokenizer_core(): return Tokenizer()
t = load_tokenizer_core()

if "stopwords" in st.session_state and st.session_state["stopwords"]:
    STOP_WORDS = st.session_state["stopwords"]
else:
    STOP_WORDS = utils.get_stopwords()

@st.cache_data
def _core_text_preprocessor(text):
    if not isinstance(text, str): return ""
    text = unicodedata.normalize('NFKC', text).lower()
    text = re.sub(r'[ï¼ˆ(][^ï¼‰)]{1,80}[ï¼‰)]', ' ', text)
    text = re.sub(r'(?:å›³|Fig|FIG|fig)[. ã€€]*\d+', ' ', text)
    text = re.sub(r'[!ï¼?"â€œâ€#$%ï¼†&\'()ï¼ˆï¼‰*ï¼‹+,\-ï¼.\:ï¼š;ï¼›<=>?ï¼Ÿ@\[\]ï¼»ï¼½\\^_`{|}~ã€œã€œï¼/]', ' ', text)
    return text

@st.cache_data
def advanced_tokenize_core(text):
    text = _core_text_preprocessor(text)
    if not text: return ""
    tokens = list(t.tokenize(text))
    processed_tokens = []
    i = 0
    while i < len(tokens):
        token1 = tokens[i]
        base1 = token1.base_form if token1.base_form != '*' else token1.surface
        if base1 in STOP_WORDS: i += 1; continue
        pos1 = token1.part_of_speech.split(',')
        if len(base1) < 2 and pos1[0] != 'åè©': i += 1; continue
        if pos1[0] == 'åè©' and (len(pos1) > 1 and pos1[1] == 'æ•°'): i += 1; continue
        if (i + 1) < len(tokens):
            token2 = tokens[i+1]
            base2 = token2.base_form if token2.base_form != '*' else token2.surface
            pos2 = token2.part_of_speech.split(',')
            if pos1[0] == 'åè©' and pos2[0] == 'åè©' and base2 not in STOP_WORDS and (len(pos2) > 1 and pos2[1] != 'æ•°'):
                processed_tokens.append(base1 + base2); i += 2; continue
        if pos1[0] == 'åè©' or (pos1[0] in ['å‹•è©', 'å½¢å®¹è©'] and len(pos1)>1 and pos1[1] == 'è‡ªç«‹'):
            processed_tokens.append(base1)
        i += 1
    return " ".join(processed_tokens)

# --- CORE æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ (Recursive Descent Parser) ---
class LogicNode:
    def evaluate(self, text): raise NotImplementedError

class AndNode(LogicNode):
    def __init__(self, children): self.children = children
    def evaluate(self, text): return all(c.evaluate(text) for c in self.children)

class OrNode(LogicNode):
    def __init__(self, children): self.children = children
    def evaluate(self, text): return any(c.evaluate(text) for c in self.children)

class RegexNode(LogicNode):
    def __init__(self, pattern): 
        try: self.pattern = re.compile(pattern, re.IGNORECASE | re.DOTALL)
        except: self.pattern = None
    def evaluate(self, text): return bool(self.pattern.search(text)) if self.pattern else False

class CoreLogicParser:
    def __init__(self):
        self.tokens = []
        self.pos = 0

    def tokenize(self, rule_str):
        # Tokenize: (, ), +, *, nearN, adjN, or literals
        raw_tokens = re.findall(r'\(|\)|' r'\bnear\d+\b|' r'\badj\d+\b|' r'[\+\*]' r'|' r'[^\(\)\+\*\s]+', rule_str, re.IGNORECASE)
        self.tokens = [t.strip() for t in raw_tokens if t.strip()]
        self.pos = 0

    def parse(self, rule_str):
        self.tokenize(rule_str)
        if not self.tokens: return None
        node = self.expression()
        if self.pos < len(self.tokens):
            raise ValueError(f"Unexpected token at end: {self.tokens[self.pos]}")
        return node

    def expression(self):
        # Expression -> Term { + Term }  (OR)
        nodes = [self.term()]
        while self.pos < len(self.tokens) and self.tokens[self.pos] == '+':
            self.pos += 1
            nodes.append(self.term())
        return OrNode(nodes) if len(nodes) > 1 else nodes[0]

    def term(self):
        # Term -> Factor { * Factor } (AND)
        nodes = [self.factor()]
        while self.pos < len(self.tokens) and self.tokens[self.pos] == '*':
            self.pos += 1
            nodes.append(self.factor())
        return AndNode(nodes) if len(nodes) > 1 else nodes[0]

    def factor(self):
        # Factor -> Atom { (nearN|adjN) Atom }
        # Note: near/adj are treated as binary ops here, but strictly they form a single Regex Node in old Logic.
        # To support (A+B) near C, we need to compile sub-parts to regex strings if possible.
        # Limitation: near/adj can only apply to "Regex-compatible" nodes (Leaf or OR of Leafs). NO ANDs allowed inside near/adj.
        
        left = self.atom()
        
        while self.pos < len(self.tokens) and re.match(r'^(near|adj)\d+$', self.tokens[self.pos], re.IGNORECASE):
            op = self.tokens[self.pos].lower()
            self.pos += 1
            right = self.atom()
            
            # recursive constraint check: Left and Right must be convertible to Regex String
            l_rex = self.to_regex_string(left)
            r_rex = self.to_regex_string(right)
            n = int(re.findall(r'\d+', op)[0])
            
            if op.startswith('near'): pattern = r'(?:{}.{{0,{}}}?{}|{}.{{0,{}}}?{})'.format(l_rex, n, r_rex, r_rex, n, l_rex)
            else: pattern = r'{}.{{0,{}}}?{}'.format(l_rex, n, r_rex) # adj
            
            left = RegexNode(pattern)
            
        return left

    def atom(self):
        if self.pos < len(self.tokens) and self.tokens[self.pos] == '(':
            self.pos += 1
            node = self.expression()
            if self.pos < len(self.tokens) and self.tokens[self.pos] == ')':
                self.pos += 1
                return node
            else:
                raise ValueError("Missing closing parenthesis")
        elif self.pos < len(self.tokens):
            t = self.tokens[self.pos]
            self.pos += 1
            # Literal
            norm = unicodedata.normalize('NFKC', t).lower()
            return RegexNode(re.escape(norm))
        else:
            raise ValueError("Unexpected end of rule")

    def to_regex_string(self, node):
        # Helper to convert a Node back to regex string if it contains only OR/Literal
        if isinstance(node, RegexNode): 
            # Pattern inside RegexNode is already compiled or string? 
            # In our class, it's compiled. We need the source string. 
            # Implementation trick: Store source pattern in RegexNode
            if hasattr(node, 'pattern') and node.pattern: return node.pattern.pattern
            # If it was constructed blindly? 
            # Let's modifying RegexNode to store source.
            return "" 
        if isinstance(node, OrNode):
            parts = [self.to_regex_string(c) for c in node.children]
            return r'(?:' + '|'.join(parts) + r')'
        if isinstance(node, AndNode):
             raise ValueError("Cannot use AND (*) inside a NEAR/ADJ condition. Use OR (+) only.")
        return ""

# Patch RegexNode to store source for recursion
class RegexNode(LogicNode):
    def __init__(self, pattern): 
        self.source = pattern
        try: self.pattern = re.compile(pattern, re.IGNORECASE | re.DOTALL)
        except: self.pattern = None
    def evaluate(self, text): return bool(self.pattern.search(text)) if self.pattern else False

@st.cache_resource
def parse_core_rule(rule_str):
    try:
        parser = CoreLogicParser()
        return parser.parse(rule_str)
    except Exception as e:
        # st.error(f"Rule Parse Error: {e}") # Suppress during cache
        return None

@st.cache_data
def prepare_axis_data_core(df, axis_col_name, delimiter):
    df_processed = df.copy()
    if axis_col_name not in df_processed.columns: return pd.DataFrame()
    df_processed[axis_col_name] = df_processed[axis_col_name].fillna('N/A')
    if axis_col_name == 'year':
        df_processed[axis_col_name] = df_processed[axis_col_name].apply(lambda x: str(int(x)) if pd.notna(x) else 'N/A')
    if delimiter:
        df_processed[axis_col_name] = df_processed[axis_col_name].astype(str).str.split(delimiter)
        df_processed = df_processed.explode(axis_col_name)
    df_processed[axis_col_name] = df_processed[axis_col_name].astype(str).str.strip().replace('', 'N/A')
    return df_processed

@st.cache_data
def convert_df_to_csv_core(df): return df.to_csv(encoding='utf-8-sig').encode('utf-8-sig')



# ==================================================================
# --- 4. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆæœŸåŒ– & UIæ§‹æˆ ---
# ==================================================================
utils.render_sidebar()

st.title("ğŸ’¡ CORE")
st.markdown("Contextual Operator & Rule Engine: **è«–ç†å¼ãƒ™ãƒ¼ã‚¹ã®ç‰¹è¨±åˆ†é¡ãƒ„ãƒ¼ãƒ«**ã§ã™ã€‚")

col_theme, _ = st.columns([1, 3])
with col_theme:
    selected_theme = st.selectbox("è¡¨ç¤ºãƒ†ãƒ¼ãƒ:", ["APOLLO Standard", "Modern Presentation"], key="core_theme_selector")
theme_config = utils.get_theme_config(selected_theme)
st.markdown(f"<style>{theme_config['css']}</style>", unsafe_allow_html=True)

if not st.session_state.get("preprocess_done", False):
    st.error("åˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"); st.stop()
else:
    df_main = st.session_state.df_main
    col_map = st.session_state.col_map

if "core_classification_rules" not in st.session_state: st.session_state.core_classification_rules = {}
if "core_df_classified" not in st.session_state: st.session_state.core_df_classified = None
if "core_current_axis" not in st.session_state: st.session_state.core_current_axis = ""
if "core_reanalyze_result" not in st.session_state: st.session_state.core_reanalyze_result = ""

# ==================================================================
# --- 5. CORE ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ ---
# ==================================================================
current_phase = st.radio("ãƒ•ã‚§ãƒ¼ã‚ºé¸æŠ:", ["ãƒ•ã‚§ãƒ¼ã‚º 1: AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ (KMeans)", "ãƒ•ã‚§ãƒ¼ã‚º 2: åˆ†é¡ãƒ«ãƒ¼ãƒ«å®šç¾©", "ãƒ•ã‚§ãƒ¼ã‚º 3: åˆ†é¡å®Ÿè¡Œ", "ãƒ•ã‚§ãƒ¼ã‚º 4: ç‰¹è¨±ãƒãƒƒãƒ—ä½œæˆ"], horizontal=True, key="core_phase_selector")
st.markdown("---")

# --- ãƒ•ã‚§ãƒ¼ã‚º 1: AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ ---
if current_phase.startswith("ãƒ•ã‚§ãƒ¼ã‚º 1"):
    st.subheader("ãƒ•ã‚§ãƒ¼ã‚º 1: AIã«ã‚ˆã‚‹åˆ†é¡ã‚µã‚¸ã‚§ã‚¹ãƒˆ (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)")
    col_map_options = [v for k, v in col_map.items() if k in ['title', 'abstract', 'claim']]
    target_column = st.selectbox("åˆ†æå¯¾è±¡ã‚«ãƒ©ãƒ :", options=col_map_options, key="core_target_col")
    
    col1, col2 = st.columns(2)
    with col1: ai_k_w = st.number_input("ãƒˆãƒ”ãƒƒã‚¯æ•° (K)", min_value=2, value=8, key="core_k")
    with col2: ai_n_w = st.number_input("ã‚µãƒ³ãƒ—ãƒ«æ•° (N)", min_value=1, value=5, key="core_n")
    
    use_mece = st.checkbox("MECEãƒ¢ãƒ¼ãƒ‰ (è‡ªå‹•æ±ºå®š)", value=True, key="core_use_mece")
    
    if not use_mece:
        st.markdown("<b>ç”Ÿæˆã™ã‚‹åˆ†é¡ã®æ•° (æ‰‹å‹•è¨­å®š):</b>", unsafe_allow_html=True)
        col_c1, col_c2, col_c3 = st.columns(3)
        with col_c1: ai_cat_count_tech = st.number_input("æŠ€è¡“åˆ†é¡:", min_value=1, value=6, key="core_cat_tech")
        with col_c2: ai_cat_count_prob = st.number_input("èª²é¡Œåˆ†é¡:", min_value=1, value=6, key="core_cat_prob")
        with col_c3: ai_cat_count_sol = st.number_input("è§£æ±ºæ‰‹æ®µåˆ†é¡:", min_value=1, value=6, key="core_cat_sol")

    if st.button("AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ", key="core_run_ai"):
        with st.spinner("åˆ†æä¸­..."):
            try:
                texts_raw = df_main[target_column].astype(str).fillna('')
                tokenized_texts = texts_raw.apply(advanced_tokenize_core)
                vec = TfidfVectorizer(min_df=1, max_df=0.9, token_pattern=r"(?u)\b\w+\b")
                tfidf = vec.fit_transform(tokenized_texts)
                km = KMeans(n_clusters=int(ai_k_w), random_state=42, n_init=10).fit(tfidf)
                
                sampled_docs = []
                for i in range(int(ai_k_w)):
                    c_idx = np.where(km.labels_ == i)[0]
                    if len(c_idx) == 0: continue
                    dists = euclidean_distances(tfidf[c_idx], km.cluster_centers_[i].reshape(1,-1))
                    top_idx = c_idx[dists.flatten().argsort()[:int(ai_n_w)]]
                    sampled_docs.append(f"\n--- Cluster {i} ---\n" + "\n".join([f"ãƒ»{_core_text_preprocessor(texts_raw.iloc[idx])}" for idx in top_idx]))
                
                if use_mece:
                    instruction_text = (
                        "ã“ã®ç‰¹è¨±æ¯é›†å›£å…¨ä½“ã‚’ç¶²ç¾…çš„ã«åˆ†é¡ã™ã‚‹ãŸã‚ã®ã€**ã€ŒæŠ€è¡“åˆ†é¡ã€ã€Œèª²é¡Œåˆ†é¡ã€ã€Œè§£æ±ºæ‰‹æ®µåˆ†é¡ã€**ã®3ã¤ã®åˆ†é¡è»¸ã«ã¤ã„ã¦ã€**åˆ†é¡å®šç¾©**ï¼ˆåˆ†é¡åã€å®šç¾©ã€COREè«–ç†å¼ã®ã‚»ãƒƒãƒˆï¼‰ã‚’è¨­è¨ˆã—ã¦ãã ã•ã„ã€‚\n"
                        "\n# é‡è¦: MECE (Mutually Exclusive, Collectively Exhaustive) ã®åŸå‰‡\n"
                        "- ç”Ÿæˆã™ã‚‹å„åˆ†é¡è»¸å†…ã®ã‚«ãƒ†ã‚´ãƒªã¯ã€ç›¸äº’ã«æ’ä»–çš„ï¼ˆãƒ€ãƒ–ã‚ŠãŒãªã„ï¼‰ã§ã‚ã‚Šã€ã‹ã¤å…¨ä½“ã¨ã—ã¦ç¶²ç¾…çš„ï¼ˆãƒ¢ãƒ¬ãŒãªã„ï¼‰ã§ã‚ã‚‹ã‚ˆã†ã«è¨­è¨ˆã—ã¦ãã ã•ã„ã€‚\n"
                        "- å„è»¸ã®ã‚«ãƒ†ã‚´ãƒªæ•°ã¯ã€MECEã‚’æº€ãŸã™ã®ã«æœ€é©ã ã¨ã‚ãªãŸãŒåˆ¤æ–­ã™ã‚‹æ•°ï¼ˆç›®å®‰ã¨ã—ã¦5ã€œ10å€‹ç¨‹åº¦ï¼‰ã«ã—ã¦ãã ã•ã„ã€‚"
                    )
                else:
                    instruction_text = "\n".join([
                        "ã“ã®ç‰¹è¨±æ¯é›†å›£å…¨ä½“ã‚’ç¶²ç¾…çš„ã«åˆ†é¡ã™ã‚‹ãŸã‚ã®ã€ä»¥ä¸‹ã®3ã¤ã®åˆ†é¡è»¸ã«ã¤ã„ã¦ã€æŒ‡å®šã•ã‚ŒãŸå€‹æ•°ã§**åˆ†é¡å®šç¾©**ã‚’è¨­è¨ˆã—ã¦ãã ã•ã„ã€‚",
                        f"- **æŠ€è¡“åˆ†é¡**: {ai_cat_count_tech}å€‹",
                        f"- **èª²é¡Œåˆ†é¡**: {ai_cat_count_prob}å€‹",
                        f"- **è§£æ±ºæ‰‹æ®µåˆ†é¡**: {ai_cat_count_sol}å€‹"
                    ])

                sampled_docs_str = "".join(sampled_docs)

                prompt = f"""
ã‚ãªãŸã¯å„ªç§€ãªç‰¹è¨±æƒ…å ±ã‚¹ãƒˆãƒ©ãƒ†ã‚¸ã‚¹ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®ã€Œä»£è¡¨æ–‡çŒ®ã‚µãƒ³ãƒ—ãƒ«ã€ã¯ã€ã‚ã‚‹ç‰¹è¨±æ¯é›†å›£ï¼ˆ{len(df_main)}ä»¶ï¼‰ã‚’K-Meansæ³•ã§{ai_k_w}å€‹ã®ã‚¯ãƒ©ã‚¹ã‚¿ã«åˆ†é¡ã—ã€å„ã‚¯ãƒ©ã‚¹ã‚¿ã‹ã‚‰ä»£è¡¨çš„ãªæ–‡çŒ®ã®ã€Œ{target_column}ã€ã‚’{ai_n_w}ä»¶ãšã¤æŠ½å‡ºã—ãŸã‚‚ã®ã§ã™ã€‚

# ä¾é ¼å†…å®¹
{instruction_text}

ä»¥ä¸‹ã®å½¢å¼ã® **JSONãƒ‡ãƒ¼ã‚¿ã®ã¿** ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚è§£èª¬ã¯ä¸è¦ã§ã™ã€‚
JSONã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ã‚·ã‚¹ãƒ†ãƒ ã«ãã®ã¾ã¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚

# JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ (å³å®ˆ)
{{
  "æŠ€è¡“åˆ†é¡": [
    {{
      "name": "ã‚«ãƒ†ã‚´ãƒªå (ä¾‹: CO2åˆ†é›¢è†œ)",
      "definition": "ã‚«ãƒ†ã‚´ãƒªã®å®šç¾©...",
      "rule": "COREè«–ç†å¼ (ä¾‹: (CO2 + äºŒé…¸åŒ–ç‚­ç´ ) * (è†œ + ãƒ¡ãƒ³ãƒ–ãƒ¬ãƒ³))"
    }},
    ...
  ],
  "èª²é¡Œåˆ†é¡": [ ... ],
  "è§£æ±ºæ‰‹æ®µåˆ†é¡": [ ... ]
}}

# COREè«–ç†å¼æ–‡æ³• (å³å®ˆ)
- `A + B` (OR): A ã¾ãŸã¯ B
- `A * B` (AND): A ã‹ã¤ B (é †åºå•ã‚ãš)
- `()`: æ‹¬å¼§ã‚’ä½¿ã£ã¦å„ªå…ˆé †ä½ã‚’åˆ¶å¾¡ã§ãã¾ã™ã€‚å…¥ã‚Œå­ã‚‚å¯èƒ½ã§ã™ã€‚
    - ä¾‹: `(æ°´ç´  * (å¸è”µ + è²¯è”µ) * (åˆé‡‘ + ææ–™))`
    - ä¾‹: `(A * B) + (C * D)`
- `A nearN B` (è¿‘å‚): Aã¨BãŒNæ–‡å­—ä»¥å†… (é †åºä¸å•)ã€‚
- `A adjN B` (é †åºæŒ‡å®š): Aâ†’BãŒNæ–‡å­—ä»¥å†…ã€‚
- **é‡è¦:** `near` ã‚„ `adj` ã®æ¡ä»¶ã®å†…éƒ¨ã«ã¯ `*` (AND) ã‚’å«ã‚ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ï¼ˆ`+` (OR) ã¯å¯èƒ½ï¼‰ã€‚
    - OK: `(A + B) near10 C`
    - NG: `(A * B) near10 C`

# æœ€é‡è¦ãƒ«ãƒ¼ãƒ« (ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ‹¡å¼µã¨è¡¨è¨˜ã‚†ã‚Œ)
- ã‚µãƒ³ãƒ—ãƒ«ã«å­˜åœ¨ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãã®ã¾ã¾ä½¿ã†ã ã‘ã§ã¯ä¸ååˆ†ã§ã™ã€‚
- AIã®çŸ¥è­˜ã‚’æ´»ç”¨ã—ã€ãã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®**é¡ç¾©èªã€é–¢é€£èªã€ä¸Šä½/ä¸‹ä½æ¦‚å¿µã€ç‰¹è¨±ç‰¹æœ‰ã®è¡¨ç¾ã€è¡¨è¨˜ã‚†ã‚Œï¼ˆã‚«ã‚¿ã‚«ãƒŠã€ã²ã‚‰ãŒãªã€æ¼¢å­—ï¼‰**ã‚’ã€ã‚ãªãŸã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ç¶²ç¾…çš„ã«æƒ³èµ·ã—ã¦ãã ã•ã„ã€‚
- **ç‰¹è¨±ç”¨èªã®ç¶²ç¾…:** ï¼ˆä¾‹: ã€Œä¿æŒã€â†’ã€Œæ‹…æŒã€ã€Œå›ºç€ã€ã€Œä¿‚æ­¢ã€ãªã©ã€ç‰¹è¨±ã§ä½¿ã‚ã‚Œã‚‹è¨€ã„æ›ãˆã‚’ç¶²ç¾…ï¼‰
- **æ¦‚å¿µã®éšå±¤åŒ–:** ä¸Šä½æ¦‚å¿µï¼ˆä¾‹: ã€Œè»Šä¸¡ã€ï¼‰ã¨ä¸‹ä½æ¦‚å¿µï¼ˆä¾‹: ã€Œè‡ªå‹•è»Šã€ã€ŒäºŒè¼ªè»Šã€ï¼‰ã®ä¸¡æ–¹ã‚’å«ã‚ã€å–ã‚Šã“ã¼ã—ã‚’é˜²ãã¾ã™ã€‚
- **ã‚«ã‚¿ã‚«ãƒŠ:** ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ã‚«ã‚¿ã‚«ãƒŠã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€**å¿…ãšå…¨è§’ï¼ˆä¾‹: `ãƒãƒªãƒãƒ¼`ï¼‰**ã‚’ä½¿ç”¨ã—ã€**åŠè§’ï¼ˆä¾‹: `ï¾ï¾Ÿï¾˜ï¾ï½°`ï¼‰ã¯çµ¶å¯¾ã«ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„**ã€‚

# ä»£è¡¨æ–‡çŒ®ã‚µãƒ³ãƒ—ãƒ«
{sampled_docs_str}
"""
                st.success("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚å³ä¸Šã®ã‚³ãƒ”ãƒ¼ãƒœã‚¿ãƒ³ã§ã‚³ãƒ”ãƒ¼ã—ã¦ãã ã•ã„ã€‚")
                st.code(prompt, language='markdown')
            except Exception as e: st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

# --- ãƒ•ã‚§ãƒ¼ã‚º 2: åˆ†é¡ãƒ«ãƒ¼ãƒ«å®šç¾© ---
elif current_phase.startswith("ãƒ•ã‚§ãƒ¼ã‚º 2"):
    st.subheader("ãƒ•ã‚§ãƒ¼ã‚º 2: åˆ†é¡ãƒ«ãƒ¼ãƒ«å®šç¾©")
    
    tab_manual, tab_json = st.tabs(["æ‰‹å‹•è¿½åŠ ãƒ»ä¿®æ­£", "JSONä¸€æ‹¬ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"])
    existing = list(st.session_state.core_classification_rules.keys())
    
    with tab_manual:
        is_edit_mode = "core_edit_target" in st.session_state and st.session_state.core_edit_target is not None
        
        mode = st.radio("è»¸ã®æŒ‡å®š:", ["æ–°è¦ä½œæˆ", "æ—¢å­˜ã«è¿½åŠ "], horizontal=True, index=1 if is_edit_mode else 0)
        
        if mode == "æ—¢å­˜ã«è¿½åŠ " and existing:
            default_idx = 0
            if is_edit_mode:
                try: default_idx = existing.index(st.session_state.core_edit_target["axis"])
                except: pass
            elif st.session_state.core_current_axis in existing:
                try: default_idx = existing.index(st.session_state.core_current_axis)
                except: pass
            axis = st.selectbox("è¿½åŠ /ä¿®æ­£å…ˆã®è»¸:", existing, index=default_idx)
        else:
            axis = st.text_input("æ–°è¦è»¸å:", value=st.session_state.core_edit_target["axis"] if is_edit_mode else "", placeholder="ä¾‹: èª²é¡Œåˆ†é¡")
            
        c_name = st.text_input("åˆ†é¡å:", value=st.session_state.core_edit_target["cat"] if is_edit_mode else "", placeholder="ä¾‹: è€ä¹…æ€§å‘ä¸Š")
        c_def = st.text_area("å®šç¾©:", value=st.session_state.core_edit_target["def"] if is_edit_mode else "", height=68)
        c_rule = st.text_input("è«–ç†å¼:", value=st.session_state.core_edit_target["rule"] if is_edit_mode else "", placeholder="(è€ä¹…æ€§ + å¯¿å‘½) * å‘ä¸Š")
        
        btn_label = "ãƒ«ãƒ¼ãƒ«ã‚’æ›´æ–°" if is_edit_mode else "ãƒ«ãƒ¼ãƒ«ã‚’è¿½åŠ "
        
        if st.button(btn_label, key="add_manual"):
            if all([axis, c_name, c_rule]):
                try:
                    parse_core_rule(c_rule)
                    if axis not in st.session_state.core_classification_rules:
                        st.session_state.core_classification_rules[axis] = {}
                    st.session_state.core_classification_rules[axis][c_name] = {'rule': c_rule, 'definition': c_def}
                    st.session_state.core_current_axis = axis
                    if is_edit_mode: del st.session_state.core_edit_target
                    st.success(f"{btn_label}ã—ã¾ã—ãŸ: {c_name}")
                    st.rerun()
                except Exception as e: st.error(f"æ–‡æ³•ã‚¨ãƒ©ãƒ¼: {e}")
        
        if is_edit_mode:
            if st.button("ç·¨é›†ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«"):
                del st.session_state.core_edit_target
                st.rerun()
    
    with tab_json:
        st.markdown("AIãŒç”Ÿæˆã—ãŸJSONã‚’ã“ã“ã«è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„ã€‚æ—¢å­˜ã®ãƒ«ãƒ¼ãƒ«ã¯ç¶­æŒã•ã‚Œã€æ–°ã—ã„è»¸ãŒè¿½åŠ ã•ã‚Œã¾ã™ã€‚")
        json_input = st.text_area("JSONå…¥åŠ›:", height=300)
        if st.button("JSONã‚’ä¸€æ‹¬ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"):
            try:
                cleaned_json = re.sub(r'^```json\s*|\s*```$', '', json_input.strip(), flags=re.MULTILINE)
                data = json.loads(cleaned_json)
                count = 0
                for axis_name, categories in data.items():
                    if axis_name not in st.session_state.core_classification_rules:
                        st.session_state.core_classification_rules[axis_name] = {}
                    for cat in categories:
                        name = cat.get('name'); rule = cat.get('rule'); defn = cat.get('definition', '')
                        if name and rule:
                            st.session_state.core_classification_rules[axis_name][name] = {'rule': rule, 'definition': defn}
                            count += 1
                st.success(f"{count} å€‹ã®ãƒ«ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã—ãŸï¼")
                st.rerun()
            except Exception as e: st.error(f"JSONãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")

    st.markdown("---")
    st.subheader("ç¾åœ¨ã®ãƒ«ãƒ¼ãƒ«ä¸€è¦§")
    
    if st.button("å…¨ãƒ«ãƒ¼ãƒ«ã‚’å‰Šé™¤", type="primary"):
        st.session_state.core_classification_rules = {}
        st.rerun()
        
    for ax, cats in st.session_state.core_classification_rules.items():
        with st.expander(f"è»¸: {ax} ({len(cats)}ä»¶)"):
            for cn, cd in cats.items():
                r = cd['rule'] if isinstance(cd, dict) else cd[0]
                d = cd.get('definition', '') if isinstance(cd, dict) else ""
                
                c1, c2, c3 = st.columns([1, 4, 1])
                with c1:
                    if st.button("ç·¨é›†", key=f"edit_{ax}_{cn}"):
                        st.session_state.core_edit_target = {"axis": ax, "cat": cn, "rule": r, "def": d}
                        st.rerun()
                with c2:
                    st.text(f"ã€{cn}ã€‘ {r}")
                with c3:
                    if st.button("å‰Šé™¤", key=f"del_{ax}_{cn}"):
                        del st.session_state.core_classification_rules[ax][cn]
                        if not st.session_state.core_classification_rules[ax]:
                            del st.session_state.core_classification_rules[ax]
                        st.rerun()

# --- ãƒ•ã‚§ãƒ¼ã‚º 3: åˆ†é¡å®Ÿè¡Œ ---
elif current_phase.startswith("ãƒ•ã‚§ãƒ¼ã‚º 3"):
    st.subheader("ãƒ•ã‚§ãƒ¼ã‚º 3: åˆ†é¡å®Ÿè¡Œ")
    
    st.info("â€» æ¢ç´¢ç¯„å›²ã¯è‡ªå‹•çš„ã«ã€Œç™ºæ˜ã®åç§° + è¦ç´„ + è«‹æ±‚é …ã€ã®çµåˆãƒ†ã‚­ã‚¹ãƒˆã¨ãªã‚Šã¾ã™ã€‚")
    
    if st.button("ã™ã¹ã¦ã®åˆ†é¡ã‚’å®Ÿè¡Œ", type="primary"):
        if not st.session_state.core_classification_rules:
            st.error("ãƒ«ãƒ¼ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            with st.spinner("å®Ÿè¡Œä¸­..."):
                try:
                    df_res = df_main.copy()
                    
                    search_cols = []
                    if col_map.get('title') in df_res.columns: search_cols.append(df_res[col_map['title']].fillna(''))
                    if col_map.get('abstract') in df_res.columns: search_cols.append(df_res[col_map['abstract']].fillna(''))
                    if col_map.get('claim') in df_res.columns: search_cols.append(df_res[col_map['claim']].fillna(''))
                    
                    combined_text = search_cols[0]
                    for s in search_cols[1:]:
                        combined_text = combined_text + " " + s
                    
                    rules = st.session_state.core_classification_rules
                    compiled_rule_nodes = {}
                    for ax, cats in rules.items():
                        compiled_rule_nodes[ax] = []
                        for cn, cd in cats.items():
                            r_str = cd['rule'] if isinstance(cd, dict) else cd[0]
                            # Try parse
                            node = parse_core_rule(r_str)
                            if node:
                                compiled_rule_nodes[ax].append((cn, node))
                            else:
                                st.warning(f"Failed to parse rule for {cn}: {r_str}")

                    def apply_rules(text, ax_nodes):
                        text = _core_text_preprocessor(str(text))
                        hits = []
                        for c_name, node in ax_nodes:
                            if node.evaluate(text):
                                hits.append(c_name)
                        return ";".join(hits) if hits else "ãã®ä»–"

                    bar = st.progress(0)
                    for i, ax in enumerate(rules.keys()):
                        df_res[ax] = combined_text.apply(lambda x: apply_rules(x, compiled_rule_nodes[ax]))
                        bar.progress((i+1)/len(rules))
                    
                    st.session_state.core_df_classified = df_res
                    st.success("å®Œäº†ï¼")
                    
                    st.subheader("åˆ†é¡çµæœã‚µãƒãƒªãƒ¼")
                    cols = st.columns(len(rules))
                    for i, ax in enumerate(rules.keys()):
                        with cols[i]:
                            st.markdown(f"**{ax}**")
                            counts = df_res[ax].str.split(';').explode().value_counts()
                            st.dataframe(counts)
                    
                    csv_core = convert_df_to_csv_core(df_res)
                    st.download_button("åˆ†é¡çµæœCSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", csv_core, "CORE_classified.csv", "text/csv")
                    
                except Exception as e: st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

    st.markdown("---")
    st.subheader("ğŸ” æœªåˆ†é¡ãƒ‡ãƒ¼ã‚¿ã®å†åˆ†æ (ã€ãã®ä»–ã€ã‚’æ¸›ã‚‰ã™)")
    if st.session_state.core_df_classified is not None:
        rules = st.session_state.core_classification_rules
        if rules:
            col_re1, col_re2 = st.columns(2)
            with col_re1: reanalyze_axis = st.selectbox("å†åˆ†æã™ã‚‹è»¸ã‚’é¸æŠ:", list(rules.keys()), key="core_reanalyze_axis")
            
            col_k, col_n = st.columns(2)
            with col_k: re_k = st.number_input("æŠ½å‡ºãƒˆãƒ”ãƒƒã‚¯æ•° (K)", value=5, key="re_k")
            with col_n: re_n = st.number_input("1ãƒˆãƒ”ãƒƒã‚¯ã‚ãŸã‚Šã®ã‚µãƒ³ãƒ—ãƒ«æ•° (N)", value=3, key="re_n")
            
            re_mece = st.checkbox("MECEãƒ¢ãƒ¼ãƒ‰ (è‡ªå‹•)", value=True, key="re_mece")
            re_cnt = 3 if re_mece else st.number_input("è¿½åŠ ã™ã‚‹ã‚«ãƒ†ã‚´ãƒªæ•°", value=3, key="re_cnt")

            if st.button("ã€ãã®ä»–ã€ã‚’åˆ†æã—ã¦æ–°ãƒ«ãƒ¼ãƒ«ã‚’ææ¡ˆ", key="core_btn_reanalyze"):
                try:
                    df_c = st.session_state.core_df_classified
                    others_df = df_c[df_c[reanalyze_axis] == 'ãã®ä»–']
                    if others_df.empty:
                        st.info("ã€ãã®ä»–ã€ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
                    else:
                        with st.spinner(f"ã€ãã®ä»–ã€({len(others_df)}ä»¶) ã‚’åˆ†æä¸­..."):
                            search_cols = []
                            if col_map.get('title') in others_df.columns: search_cols.append(others_df[col_map['title']].fillna(''))
                            if col_map.get('abstract') in others_df.columns: search_cols.append(others_df[col_map['abstract']].fillna(''))
                            if col_map.get('claim') in others_df.columns: search_cols.append(others_df[col_map['claim']].fillna(''))
                            texts = search_cols[0]
                            for s in search_cols[1:]: texts = texts + " " + s
                            
                            toks = texts.apply(advanced_tokenize_core)
                            vec = TfidfVectorizer(min_df=1, max_df=0.9, token_pattern=r"(?u)\b\w+\b")
                            tfidf = vec.fit_transform(toks)
                            
                            actual_k = min(int(re_k), len(others_df))
                            if actual_k < 2: actual_k = 1
                            km = KMeans(n_clusters=actual_k, random_state=42).fit(tfidf)
                            
                            s_docs = []
                            for i in range(actual_k):
                                c_idx = np.where(km.labels_ == i)[0]
                                if len(c_idx) == 0: continue
                                dists = euclidean_distances(tfidf[c_idx], km.cluster_centers_[i].reshape(1,-1))
                                top_idx = c_idx[dists.flatten().argsort()[:int(re_n)]]
                                s_docs.append(f"\n--- ãã®ä»–ã‚°ãƒ«ãƒ¼ãƒ— {i} ---\n" + "\n".join([f"ãƒ»{_core_text_preprocessor(texts.iloc[idx])}" for idx in top_idx]))
                            
                            s_docs_str = "".join(s_docs)
                            exist_rules = [f"- {cat}: {d['rule']}" for cat, d in rules[reanalyze_axis].items()]
                            exist_rules_str = "\n".join(exist_rules)
                            
                            instruction_part = "MECEã‚’æ„è­˜ã—ã€ã‚«ãƒ†ã‚´ãƒªæ•°ã¯è‡ªå‹•ã§æœ€é©åŒ–ã—ã¦ãã ã•ã„ã€‚" if re_mece else f"**{re_cnt}å€‹** ã®æ–°ã—ã„ã‚«ãƒ†ã‚´ãƒªã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚"
                            
                            p_re = f"""
ã‚ãªãŸã¯ç‰¹è¨±æƒ…å ±ã‚¹ãƒˆãƒ©ãƒ†ã‚¸ã‚¹ãƒˆã§ã™ã€‚
ç¾åœ¨ã€åˆ†é¡è»¸ã€Œ{reanalyze_axis}ã€ã‚’ä½œæˆä¸­ã§ã™ãŒã€ä»¥ä¸‹ã®ã€Œæ—¢å­˜ã®åˆ†é¡ã€ã«å½“ã¦ã¯ã¾ã‚‰ãªã„ç‰¹è¨±ãŒã€Œãã®ä»–ã€ã¨ã—ã¦æ®‹ã£ã¦ã„ã¾ã™ã€‚

# æ—¢å­˜ã®åˆ†é¡ãƒªã‚¹ãƒˆ
{exist_rules_str}

# ä¾é ¼å†…å®¹
ä»¥ä¸‹ã®ã€Œæœªåˆ†é¡ç‰¹è¨±ã®ã‚µãƒ³ãƒ—ãƒ«ã€ã‚’åˆ†æã—ã€**æ—¢å­˜ã®åˆ†é¡ã¨ã¯æ¦‚å¿µçš„ã«é‡è¤‡ã—ãªã„ã€æ–°ã—ã„åˆ†é¡ã‚«ãƒ†ã‚´ãƒª**ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚
å‡ºåŠ›ã¯ **JSONå½¢å¼ã®ã¿** ã¨ã—ã¦ãã ã•ã„ã€‚
{instruction_part}

# JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
{{
  "{reanalyze_axis}": [
    {{
      "name": "æ–°ã‚«ãƒ†ã‚´ãƒªå",
      "definition": "...",
      "rule": "è«–ç†å¼ (Allowed: `(A * B) + C`, `(A+B) near10 C` etc)"
    }}, ...
  ]
}}

# æœªåˆ†é¡ç‰¹è¨±ã®ã‚µãƒ³ãƒ—ãƒ«
{s_docs_str}
"""
                            st.session_state.core_reanalyze_result = p_re
                except Exception as e: st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
        
        if st.session_state.core_reanalyze_result:
            st.success("å†åˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚"); st.code(st.session_state.core_reanalyze_result, language='markdown')

# --- ãƒ•ã‚§ãƒ¼ã‚º 4: ç‰¹è¨±ãƒãƒƒãƒ— ---
elif current_phase.startswith("ãƒ•ã‚§ãƒ¼ã‚º 4"):
    st.subheader("ãƒ•ã‚§ãƒ¼ã‚º 4: ç‰¹è¨±ãƒãƒƒãƒ—ä½œæˆ")
    
    if st.session_state.core_df_classified is None:
        st.warning("å…ˆã«åˆ†é¡ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    else:
        df_c = st.session_state.core_df_classified
        axes = list(st.session_state.core_classification_rules.keys())
        meta_axes = []
        if 'year' in df_c.columns: meta_axes.append('å‡ºé¡˜å¹´')
        if col_map.get('applicant') in df_c.columns: meta_axes.append('å‡ºé¡˜äºº')
        all_axes = axes + meta_axes
        
        c1, c2, c3 = st.columns(3)
        with c1: x_ax = st.selectbox("Xè»¸", all_axes, index=0)
        with c2: y_ax = st.selectbox("Yè»¸", all_axes, index=min(1, len(all_axes)-1))
        with c3: chart_type = st.radio("ã‚°ãƒ©ãƒ•ã‚¿ã‚¤ãƒ—", ["ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—", "ãƒãƒ–ãƒ«ãƒãƒ£ãƒ¼ãƒˆ"])
        
        col_f1, col_f2 = st.columns(2)
        with col_f1: exclude_other = st.checkbox("ã€Œãã®ä»–ã€ã‚’é™¤å¤–ã™ã‚‹", value=True)
        
        if st.button("æç”» (åˆ†æå®Ÿè¡Œ)", type="primary"):
            st.session_state.core_phase4_run = True

        if st.session_state.get("core_phase4_run"):
            st.markdown("---")
            
            # --- 1. ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªè»¸ã®é †åºã‚’æ±ºå®š (æ¯é›†å›£å…¨ä½“ã§å›ºå®š) ---
            def get_col_data_global(target_df, ax_name):
                if ax_name == 'å‡ºé¡˜å¹´': return target_df['year'].fillna(0).astype(int).astype(str), None
                if ax_name == 'å‡ºé¡˜äºº': return target_df[col_map['applicant']].fillna('Unknown'), ';' 
                if ax_name in axes: return target_df[ax_name], ';'
                return None, None

            # å…¨ä½“ãƒ‡ãƒ¼ã‚¿ã§ã‚¯ãƒ­ã‚¹é›†è¨ˆã—ã¦è»¸é †åºã‚’æ±ºå®š
            x_data_g, x_sep_g = get_col_data_global(df_c, x_ax)
            y_data_g, y_sep_g = get_col_data_global(df_c, y_ax)
            temp_df_g = pd.DataFrame({'X': x_data_g, 'Y': y_data_g})
            
            if x_sep_g: temp_df_g['X'] = temp_df_g['X'].astype(str).str.split(x_sep_g); temp_df_g = temp_df_g.explode('X')
            if y_sep_g: temp_df_g['Y'] = temp_df_g['Y'].astype(str).str.split(y_sep_g); temp_df_g = temp_df_g.explode('Y')
            
            temp_df_g = temp_df_g.replace({'nan': np.nan, 'None': np.nan}).dropna()
            if exclude_other:
                temp_df_g = temp_df_g[(temp_df_g['X'] != 'ãã®ä»–') & (temp_df_g['Y'] != 'ãã®ä»–')]
            
            # --- è»¸ãŒå‡ºé¡˜äººã®å ´åˆã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° (Top N / æ‰‹å‹•) ---
            if 'å‡ºé¡˜äºº' in [x_ax, y_ax]:
                # å…¨ãƒ‡ãƒ¼ã‚¿ã®å‡ºé¡˜äººé »åº¦ã‚’è¨ˆç®—
                app_s_all = df_c[col_map['applicant']].fillna('Unknown').astype(str).str.split(';')
                app_counts_all = app_s_all.explode().str.strip().value_counts()
                
                st.markdown("##### ğŸ‘¥ å‡ºé¡˜äººè»¸ã®è¡¨ç¤ºè¨­å®š")
                app_filter_mode = st.radio("è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰:", ["ä¸Šä½æŒ‡å®š (Top N)", "æ‰‹å‹•é¸æŠ (Manual)"], horizontal=True, key="core_app_axis_mode")
                
                target_apps_set = set()
                if app_filter_mode == "ä¸Šä½æŒ‡å®š (Top N)":
                    top_n_val = st.number_input("è¡¨ç¤ºä»¶æ•° (ä¸Šä½Nç¤¾):", min_value=5, max_value=200, value=10, step=5, key="core_app_axis_n")
                    target_apps_set = set(app_counts_all.head(top_n_val).index)
                    st.info(f"ä¸Šä½ {top_n_val} ç¤¾ã‚’è¡¨ç¤ºã—ã¾ã™ï¼ˆå…¨ {len(app_counts_all)} ç¤¾ä¸­ï¼‰")
                else:
                    target_apps_set = set(st.multiselect("è¡¨ç¤ºã™ã‚‹å‡ºé¡˜äººã‚’é¸æŠ:", app_counts_all.index.tolist(), default=app_counts_all.head(10).index.tolist(), key="core_app_axis_manual"))
                
                # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é©ç”¨
                if x_ax == 'å‡ºé¡˜äºº':
                    temp_df_g = temp_df_g[temp_df_g['X'].isin(target_apps_set)]
                if y_ax == 'å‡ºé¡˜äºº':
                    temp_df_g = temp_df_g[temp_df_g['Y'].isin(target_apps_set)]
            
            if temp_df_g.empty:
                st.warning("æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            else:
                ct_g = pd.crosstab(temp_df_g['Y'], temp_df_g['X'])
                
                # Sorting Global
                if x_ax == 'å‡ºé¡˜å¹´': x_ord_global = sorted(ct_g.columns, key=lambda x: int(x) if x.isdigit() else x)
                else: x_ord_global = ct_g.sum(axis=0).sort_values(ascending=False).index.tolist()
                
                if y_ax == 'å‡ºé¡˜å¹´': y_ord_global = sorted(ct_g.index, key=lambda x: int(x) if x.isdigit() else x)
                else: y_ord_global = ct_g.sum(axis=1).sort_values(ascending=False).index.tolist()

                # --- Sorting Adjustment: Force 'Others' to the end ---
                if 'ãã®ä»–' in x_ord_global:
                    x_ord_global.remove('ãã®ä»–')
                    x_ord_global.append('ãã®ä»–')
                
                if 'ãã®ä»–' in y_ord_global:
                    y_ord_global.remove('ãã®ä»–')
                    y_ord_global.append('ãã®ä»–')

                # --- 2. å‡ºé¡˜äººé¸æŠ (ãƒ—ãƒ«ãƒ€ã‚¦ãƒ³) ---
                target_applicant_options = ["å…¨ä½“ (Overall)"]
                app_name_map = {} # label -> app_name
                
                if col_map.get('applicant') in df_c.columns:
                    app_s = df_c[col_map['applicant']].fillna('Unknown').astype(str).str.split(';')
                    app_exploded = app_s.explode().str.strip()
                    top_apps = app_exploded.value_counts() # All applicants
                    
                    for app_name, count in top_apps.items():
                        if app_name and app_name != 'nan':
                            label = f"{app_name} ({count})"
                            target_applicant_options.append(label)
                            app_name_map[label] = app_name
                
                selected_app_label = st.selectbox("å‡ºé¡˜äººã§çµã‚Šè¾¼ã¿ (Focus Applicant):", target_applicant_options)

                # --- 3. ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° ---
                if selected_app_label == "å…¨ä½“ (Overall)":
                    df_target = df_c
                else:
                    target_app_name = app_name_map[selected_app_label]
                    mask = df_c[col_map['applicant']].fillna('').astype(str).apply(lambda x: target_app_name in [s.strip() for s in x.split(';')])
                    df_target = df_c[mask]
                
                st.markdown(f"**åˆ†æå¯¾è±¡: {selected_app_label}**")

                # --- 4. æç”»é–¢æ•° (Global Axisã‚’é©ç”¨) ---
                def render_core_chart(sub_df, wrapper_key):
                    # Local Data Prep
                    x_d, x_s = get_col_data_global(sub_df, x_ax)
                    y_d, y_s = get_col_data_global(sub_df, y_ax)
                    t_df = pd.DataFrame({'X': x_d, 'Y': y_d})
                    if x_s: t_df['X'] = t_df['X'].astype(str).str.split(x_s); t_df = t_df.explode('X')
                    if y_s: t_df['Y'] = t_df['Y'].astype(str).str.split(y_s); t_df = t_df.explode('Y')
                    
                    t_df = t_df.replace({'nan': np.nan, 'None': np.nan}).dropna()
                    if exclude_other:
                        t_df = t_df[(t_df['X'] != 'ãã®ä»–') & (t_df['Y'] != 'ãã®ä»–')]
                    
                    # Create Crosstab
                    ct_local = pd.crosstab(t_df['Y'], t_df['X'])
                    
                    # Reindex with Global Orders (Forces matrix structure)
                    ct_final = ct_local.reindex(index=y_ord_global, columns=x_ord_global).fillna(0)
                    
                    if chart_type == "ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—":
                        fig = px.imshow(
                            ct_final, 
                            labels=dict(x=x_ax, y=y_ax, color="ä»¶æ•°"),
                            x=ct_final.columns,
                            y=ct_final.index,
                            aspect="auto",
                            color_continuous_scale='YlGnBu',
                            text_auto=True
                        )
                        fig.update_layout(
                            height=max(600, len(ct_final)*40),
                            yaxis=dict(title=y_ax),
                            xaxis=dict(title=x_ax, side='bottom')
                        )
                        st.plotly_chart(fig, use_container_width=True, config={'editable': False}, key=f"core_chart_{wrapper_key}")
                        
                    else: # ãƒãƒ–ãƒ«ãƒãƒ£ãƒ¼ãƒˆ
                        ct_long = ct_final.reset_index().melt(id_vars='Y', var_name='X', value_name='Count')
                        ct_long = ct_long[ct_long['Count'] > 0] 
                        
                        atlas_colors = theme_config["color_sequence"]
                        
                        fig = px.scatter(
                            ct_long, x='X', y='Y', size='Count', color='Y',
                            size_max=60, color_discrete_sequence=atlas_colors,
                            category_orders={'X': x_ord_global, 'Y': y_ord_global} 
                        )
                        
                        # Explicitly FORCE Range to show all categories, even empty ones
                        x_range = [-0.5, len(x_ord_global) - 0.5]
                        # For Y, Plotly usually plots bottom-up for Scatter, but we want Matrix style (Top-down)?
                        # `autorange='reversed'` does Top-down.
                        # If reversed, range should be [len-0.5, -0.5]? Or just rely on autorange='reversed' with fixed categoryarray.
                        # Let's try specifying range explicitly with reversed effect if needed, but 'reversed' + categoryarray works best usually.
                        # The issue "missing parts" implies missing ticks.
                        # We will force tickvals to be 0..N-1 to ensure all labels appear?
                        # Or simply setting the range is robust.
                        
                        fig.update_yaxes(
                            categoryorder='array', 
                            categoryarray=y_ord_global, 
                            title=y_ax, 
                            type='category',
                            range=[len(y_ord_global) - 0.5, -0.5] # Top-down Matrix Style
                        )
                        fig.update_xaxes(
                            categoryorder='array', 
                            categoryarray=x_ord_global, 
                            title=x_ax, 
                            side='bottom', 
                            type='category',
                            range=[-0.5, len(x_ord_global) - 0.5] # Ensure full width
                        )
                        
                        fig.update_layout(height=max(600, len(ct_final)*40), showlegend=False)
                        st.plotly_chart(fig, use_container_width=True, config={'editable': False}, key=f"core_chart_{wrapper_key}")


                render_core_chart(df_target, "main_display")

        # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        st.markdown("---")
        csv_core = convert_df_to_csv_core(df_c)
        st.download_button("åˆ†é¡çµæœä»˜ãå…¨ãƒ‡ãƒ¼ã‚¿CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", csv_core, "CORE_classified_full.csv", "text/csv")