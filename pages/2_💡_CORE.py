%%writefile pages/2_ğŸ’¡_CORE.py
# ==================================================================
# --- 1. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
# ==================================================================
import streamlit as st
import pandas as pd
import numpy as np
import io
import datetime
import warnings
import unicodedata
import re

# Janome / Sklearn
from janome.tokenizer import Tokenizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import euclidean_distances

# Plotly
import plotly.graph_objects as go
import plotly.express as px
import plotly.io as pio

pio.templates.default = "plotly_white"

# è­¦å‘Šã‚’éè¡¨ç¤º
warnings.filterwarnings('ignore')

# ==================================================================
# --- 2. COREå°‚ç”¨ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ---
# ==================================================================

# COREã¯ç‹¬è‡ªã®Tokenizerã¨StopWordsã‚’æŒã¤
@st.cache_resource
def load_tokenizer_core():
    print("... CORE: Janome Tokenizerã‚’ãƒ­ãƒ¼ãƒ‰ä¸­ ...")
    return Tokenizer()

t = load_tokenizer_core()

# COREå°‚ç”¨ã®ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰
stop_words = {
    "ã™ã‚‹","ã‚ã‚‹","ãªã‚‹","ãŸã‚","ã“ã¨","ã‚ˆã†","ã‚‚ã®","ã“ã‚Œ","ãã‚Œ","ã‚ã‚Œ","ã“ã“","ãã“","ã©ã‚Œ","ã©ã®","ã“ã®","ãã®","å½“è©²","è©²","ãŠã‚ˆã³","åŠã³","ã¾ãŸã¯","ã¾ãŸ","ä¾‹ãˆã°","ä¾‹ãˆã°ã¯","ã«ãŠã„ã¦","ã«ã‚ˆã‚Š","ã«å¯¾ã—ã¦","ã«é–¢ã—ã¦","ã«ã¤ã„ã¦","ã¨ã—ã¦","ã¨ã—ã¦ã¯","å ´åˆ","ä¸€æ–¹","ä»–æ–¹","ã•ã‚‰ã«","ãã—ã¦","ãŸã ã—","ãªãŠ","ç­‰","ãªã©","ç­‰ã€…","ã„ã‚ã‚†ã‚‹","æ‰€è¬‚","åŒæ§˜","åŒæ™‚","å‰è¨˜","æœ¬","åŒ","å„","å„ç¨®","æ‰€å®š","æ‰€æœ›","ä¸€ä¾‹","ä»–","ä¸€éƒ¨","ä¸€ã¤","è¤‡æ•°","å°‘ãªãã¨ã‚‚","å°‘ãªãã¨ã‚‚ä¸€ã¤","ä¸Šè¨˜","ä¸‹è¨˜","å‰è¿°","å¾Œè¿°","æ—¢è¿°","é–¢ã™ã‚‹","åŸºã¥ã","ç”¨ã„ã‚‹","ä½¿ç”¨","åˆ©ç”¨","æœ‰ã™ã‚‹","å«ã‚€","å‚™ãˆã‚‹","è¨­ã‘ã‚‹","ã™ãªã‚ã¡","å¾“ã£ã¦","ã—ã‹ã—ãªãŒã‚‰","æ¬¡ã«","ç‰¹ã«","å…·ä½“çš„ã«","è©³ç´°ã«","ã„ãšã‚Œ","ã†ã¡","ãã‚Œãã‚Œ","ã¨ã","ã‹ã‹ã‚‹","ã‹ã‚ˆã†ãª","ã‹ã‹ã‚‹å ´åˆ","æœ¬ä»¶","æœ¬é¡˜","æœ¬å‡ºé¡˜","æœ¬æ˜ç´°æ›¸",
    "ã§ãã‚‹", "ã„ã‚‹", "æä¾›", "æ˜ç´°æ›¸", 
    "æœ¬ç™ºæ˜","ç™ºæ˜","å®Ÿæ–½ä¾‹","å®Ÿæ–½å½¢æ…‹","å¤‰å½¢ä¾‹","è«‹æ±‚","è«‹æ±‚é …","å›³","å›³é¢","ç¬¦å·","ç¬¦å·ã®èª¬æ˜","å›³é¢ã®ç°¡å˜ãªèª¬æ˜","ç™ºæ˜ã®è©³ç´°ãªèª¬æ˜","æŠ€è¡“åˆ†é‡","èƒŒæ™¯æŠ€è¡“","å¾“æ¥æŠ€è¡“","ç™ºæ˜ãŒè§£æ±ºã—ã‚ˆã†ã¨ã™ã‚‹èª²é¡Œ","èª²é¡Œ","è§£æ±ºæ‰‹æ®µ","åŠ¹æœ","è¦ç´„","ç™ºæ˜ã®åŠ¹æœ","ç›®çš„","æ‰‹æ®µ", "å®Ÿæ–½ã®å½¢æ…‹","å®Ÿæ–½ã®æ…‹æ§˜","æ…‹æ§˜","å¤‰å½¢","ä¿®æ­£ä¾‹","å›³ç¤º","å›³ç¤ºä¾‹","å›³ç¤ºã—ãªã„","å‚ç…§","å‚ç…§ç¬¦å·","æ®µè½","è©³ç´°èª¬æ˜","è¦æ—¨","ä¸€å®Ÿæ–½å½¢æ…‹","ä»–ã®å®Ÿæ–½å½¢æ…‹","ä¸€å®Ÿæ–½ä¾‹","åˆ¥ã®å´é¢","ä»˜è¨˜","é©ç”¨ä¾‹","ç”¨èªã®å®šç¾©","é–‹ç¤º","æœ¬é–‹ç¤º","é–‹ç¤ºå†…å®¹",
    "å‡ºé¡˜","å‡ºé¡˜äºº","å‡ºé¡˜ç•ªå·","å‡ºé¡˜æ—¥","å‡ºé¡˜æ›¸","å‡ºé¡˜å…¬é–‹","å…¬é–‹","å…¬é–‹ç•ªå·","å…¬é–‹å…¬å ±","å…¬å ±","å…¬å ±ç•ªå·","ç‰¹è¨±","ç‰¹è¨±ç•ªå·","ç‰¹è¨±æ–‡çŒ®","éç‰¹è¨±æ–‡çŒ®","å¼•ç”¨","å¼•ç”¨æ–‡çŒ®","å…ˆè¡ŒæŠ€è¡“","å¯©æŸ»","å¯©æŸ»å®˜","æ‹’çµ¶","æ„è¦‹æ›¸","è£œæ­£æ›¸","å„ªå…ˆ","å„ªå…ˆæ—¥","åˆ†å‰²å‡ºé¡˜","ç¶™ç¶šå‡ºé¡˜","å›½å†…ç§»è¡Œ","å›½éš›å‡ºé¡˜","å›½éš›å…¬é–‹","PCT","ç™»éŒ²","å…¬é–‹æ—¥","å¯©æŸ»è«‹æ±‚","æ‹’çµ¶ç†ç”±","è£œæ­£","è¨‚æ­£","ç„¡åŠ¹å¯©åˆ¤","ç•°è­°","å–æ¶ˆ","å–ä¸‹ã’","äº‹ä»¶ç•ªå·","ä»£ç†äºº","å¼ç†å£«","ä¿‚å±","çµŒé",
    "ç¬¬","ç¬¬ä¸€","ç¬¬äºŒ","ç¬¬ä¸‰","ç¬¬1","ç¬¬ï¼’","ç¬¬ï¼“","ç¬¬ï¼‘","ç¬¬ï¼’","ç¬¬ï¼“","ä¸€","äºŒ","ä¸‰","å››","äº”","å…­","ä¸ƒ","å…«","ä¹","é›¶","æ•°","è¤‡åˆ","å¤šæ•°","å°‘æ•°","å›³1","å›³2","å›³3","å›³4","å›³5","å›³6","å›³7","å›³8","å›³9","è¡¨1","è¡¨2","è¡¨3","å¼1","å¼2","å¼3",
    "%","ï¼…","wt%","vol%","è³ªé‡%","é‡é‡%","å®¹é‡%","mol","mol%","mol/L","M","mm","cm","m","nm","Î¼m","Î¼","rpm","Pa","kPa","MPa","GPa","N","W","V","A","mA","Hz","kHz","MHz","GHz","â„ƒ","Â°C","K","mL","L","g","kg","mg","wt","vol","h","hr","hrs","min","s","sec","ppm","ppb","bar","Î©","ohm","J","kJ","Wh","kWh",
    "æ ªå¼ä¼šç¤¾","æœ‰é™ä¼šç¤¾","åˆè³‡ä¼šç¤¾","åˆåä¼šç¤¾","åˆåŒä¼šç¤¾","Inc","Inc.","Ltd","Ltd.","Co","Co.","Corp","Corp.","LLC", "GmbH","AG","BV","B.V.","S.A.","S.p.A.","ï¼ˆæ ªï¼‰","ãˆ±","ï¼ˆæœ‰ï¼‰"
}

@st.cache_data
def _core_text_preprocessor(text):
    """KMeans(ãƒ•ã‚§ãƒ¼ã‚º1)ã¨åˆ†é¡å®Ÿè¡Œ(ãƒ•ã‚§ãƒ¼ã‚º3)ã§å…±é€šã®ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†"""
    if not isinstance(text, str): return ""
    text = unicodedata.normalize('NFKC', text).lower()
    text = re.sub(r'[ï¼ˆ(][^ï¼‰)]{1,80}[ï¼‰)]', ' ', text) # æ‹¬å¼§å†…ã‚’é™¤å»
    text = re.sub(r'(?:å›³|Fig|FIG|fig)[. ã€€]*\d+', ' ', text) # å›³ç•ªã‚’é™¤å»
    text = re.sub(r'[!ï¼?"â€œâ€#$%ï¼†&\'()ï¼ˆï¼‰*ï¼‹+,\-ï¼.\:ï¼š;ï¼›<=>?ï¼Ÿ@\[\]ï¼»ï¼½\\^_`{|}~ã€œã€œï¼/]', ' ', text) # è¨˜å·ã‚’é™¤å»
    return text

@st.cache_data
def advanced_tokenize_core(text):
    text = _core_text_preprocessor(text)
    if not text: return ""

    tokens = list(t.tokenize(text))
    processed_tokens = []
    i = 0
    while i < len(tokens):
        token1 = tokens[i]
        base1 = token1.base_form if token1.base_form != '*' else token1.surface
        if base1 in stop_words:
            i += 1
            continue
        part_of_speech = token1.part_of_speech.split(',')
        pos_major = part_of_speech[0]
        pos_minor = part_of_speech[1] if len(part_of_speech) > 1 else ''
        if len(base1) < 2 and pos_major != 'åè©':
            i += 1
            continue
        if pos_major == 'åè©' and pos_minor == 'æ•°':
            i += 1
            continue
        if (i + 1) < len(tokens):
            token2 = tokens[i+1]
            base2 = token2.base_form if token2.base_form != '*' else token2.surface
            part_of_speech_2 = token2.part_of_speech.split(',')
            pos_major_2 = part_of_speech_2[0]
            pos_minor_2 = part_of_speech_2[1] if len(part_of_speech_2) > 1 else ''
            if pos_major == 'åè©' and pos_major_2 == 'åè©' and \
               base2 not in stop_words and pos_minor_2 != 'æ•°':
                compound_word = base1 + base2
                processed_tokens.append(compound_word)
                i += 2
                continue
        if pos_major == 'åè©':
            processed_tokens.append(base1)
        elif pos_major == 'å‹•è©' and pos_minor == 'è‡ªç«‹':
            processed_tokens.append(base1)
        elif pos_major == 'å½¢å®¹è©' and pos_minor == 'è‡ªç«‹':
            processed_tokens.append(base1)
        i += 1
    return " ".join(processed_tokens)

# CORE æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³
def build_regex_pattern(keyword):
    return re.escape(keyword)
def build_near_regex(a, b, n):
    a_b = r'{}.{{0,{}}}?{}'.format(a, n, b); b_a = r'{}.{{0,{}}}?{}'.format(b, n, a); return r'(?:{}|{})'.format(a_b, b_a)
def build_adj_regex(a, b, n):
    return r'{}.{{0,{}}}?{}'.format(a, n, b)
def build_or_regex(a, b):
    return r'(?:{}|{})'.format(a, b)

def split_by_operator(text, operator):
    """æ‹¬å¼§ã®å¤–å´ã«ã‚ã‚‹æ¼”ç®—å­ã§ã®ã¿åˆ†å‰²ã™ã‚‹"""
    parts = []
    balance = 0
    current_chunk_start = 0
    for i, char in enumerate(text):
        if char == '(':
            balance += 1
        elif char == ')':
            balance -= 1
        elif char == operator and balance == 0:
            parts.append(text[current_chunk_start:i].strip())
            current_chunk_start = i + 1
    parts.append(text[current_chunk_start:].strip())
    return parts

@st.cache_data
def parse_core_rule(rule_str):
    # ã“ã®ãƒ‘ãƒ¼ã‚µãƒ¼ã¯ `+`, `near`, `adj`, `()` ã®ã¿ã‚’å‡¦ç†
    # `*` ã¯ä¸Šä½ã® `split_by_operator` ã§å‡¦ç†ã•ã‚Œã‚‹
    tokens = re.findall(r'\(|\)|' r'\bnear\d+\b|' r'\badj\d+\b|' r'[\+]|' r'[^()\s\+]+', rule_str, re.IGNORECASE)
    tokens = [t.strip() for t in tokens if t and t.strip()]
    output_queue, op_stack = [], []
    op_precedence = {}
    for op in tokens:
        op_lower = op.lower()
        if op_lower == '+': op_precedence[op] = 1
        elif op_lower.startswith('near'): op_precedence[op] = 3
        elif op_lower.startswith('adj'): op_precedence[op] = 3
    for token in tokens:
        token_lower = token.lower()
        if token == '(': op_stack.append(token)
        elif token == ')':
            while op_stack and op_stack[-1] != '(': output_queue.append(op_stack.pop())
            if not op_stack: raise ValueError(f"æ–‡æ³•ã‚¨ãƒ©ãƒ¼: æ‹¬å¼§ã®å¯¾å¿œãŒå–ã‚Œã¾ã›ã‚“ (ã€Œ{rule_str}ã€)")
            op_stack.pop() 
        elif token_lower in op_precedence:
            while (op_stack and op_stack[-1] != '(' and op_precedence.get(op_stack[-1].lower(), 0) >= op_precedence[token_lower]):
                output_queue.append(op_stack.pop())
            op_stack.append(token)
        else: output_queue.append(token)
    while op_stack:
        op = op_stack.pop();
        if op == '(': raise ValueError(f"æ–‡æ³•ã‚¨ãƒ©ãƒ¼: æ‹¬å¼§ã®å¯¾å¿œãŒå–ã‚Œã¾ã›ã‚“ (ã€Œ{rule_str}ã€)");
        output_queue.append(op)
    regex_stack = []
    for token in output_queue:
        token_lower = token.lower()
        if token_lower not in op_precedence and token not in '()':
            normalized_token = unicodedata.normalize('NFKC', token).lower()
            if not normalized_token:
                raise ValueError(f"æ–‡æ³•ã‚¨ãƒ©ãƒ¼: ç©ºã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ (ã€Œ{rule_str}ã€)")
            regex_stack.append(build_regex_pattern(normalized_token))
        else:
            if len(regex_stack) < 2: raise ValueError(f"æ–‡æ³•ã‚¨ãƒ©ãƒ¼: æ¼”ç®—å­ '{token}' ãŒä¸æ­£ã§ã™ (ã€Œ{rule_str}ã€)")
            b, a = regex_stack.pop(), regex_stack.pop()
            if token_lower == '+': regex_stack.append(build_or_regex(a, b))
            elif token_lower.startswith('near'):
                n = int(re.findall(r'(\d+)', token_lower)[0]); regex_stack.append(build_near_regex(a, b, n))
            elif token_lower.startswith('adj'):
                n = int(re.findall(r'(\d+)', token_lower)[0]); regex_stack.append(build_adj_regex(a, b, n))
    if len(regex_stack) != 1: raise ValueError(f"æ–‡æ³•ã‚¨ãƒ©ãƒ¼: æœ€çµ‚å¼ãŒä¸æ­£ã§ã™ (ã€Œ{rule_str}ã€)")
    return re.compile(regex_stack[0], re.IGNORECASE | re.DOTALL) 

@st.cache_data
def prepare_axis_data_core(df, axis_col_name, delimiter):
    """ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—å°‚ç”¨ã®ãƒ‡ãƒ¼ã‚¿æº–å‚™é–¢æ•°"""
    df_processed = df.copy()
    if axis_col_name not in df_processed.columns:
        st.error(f"ã‚¨ãƒ©ãƒ¼: ã‚«ãƒ©ãƒ  '{axis_col_name}' ãŒãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        return pd.DataFrame() # ç©ºã®DFã‚’è¿”ã™
    
    df_processed[axis_col_name] = df_processed[axis_col_name].fillna('N/A')
    
    # 'year' ã‚«ãƒ©ãƒ ã®å ´åˆ (floatã‚’intæ–‡å­—åˆ—ã«)
    if axis_col_name == 'year':
        df_processed[axis_col_name] = df_processed[axis_col_name].apply(
            lambda x: str(int(x)) if pd.notna(x) else 'N/A'
        )
    
    # 'å‡ºé¡˜äºº' ã¾ãŸã¯ 'åˆ†é¡è»¸' ã®å ´åˆ (ãƒ‡ãƒªãƒŸã‚¿ã§åˆ†å‰²)
    if delimiter:
        df_processed[axis_col_name] = df_processed[axis_col_name].astype(str).str.split(delimiter)
        df_processed = df_processed.explode(axis_col_name)
    
    # å…±é€šã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
    df_processed[axis_col_name] = df_processed[axis_col_name].astype(str).str.strip()
    df_processed[axis_col_name] = df_processed[axis_col_name].replace('', 'N/A')
    return df_processed


# ==================================================================
# --- 3. Streamlit UI ---
# ==================================================================
st.set_page_config(
    page_title="APOLLO | CORE", 
    page_icon="ğŸ’¡", 
    layout="wide"
)

st.title("ğŸ’¡ CORE")
st.markdown("Contextual Operator & Rule Engine: **è«–ç†å¼ãƒ™ãƒ¼ã‚¹ã®ç‰¹è¨±åˆ†é¡ãƒ„ãƒ¼ãƒ«**ã§ã™ã€‚")

# ==================================================================
# --- 4. ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®ç¢ºèªã¨åˆæœŸåŒ– ---
# ==================================================================
if not st.session_state.get("preprocess_done", False):
    st.error("åˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    st.warning("å…ˆã«ã€ŒMission Controlã€ï¼ˆãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ï¼‰ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€ã€Œåˆ†æã‚¨ãƒ³ã‚¸ãƒ³èµ·å‹•ã€ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    st.stop()
else:
    df_main = st.session_state.df_main
    col_map = st.session_state.col_map

if "core_classification_rules" not in st.session_state:
    st.session_state.core_classification_rules = {}
if "core_df_classified" not in st.session_state:
    st.session_state.core_df_classified = None
if "core_current_axis" not in st.session_state:
    st.session_state.core_current_axis = ""

# ==================================================================
# --- 5. CORE ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ ---
# ==================================================================

tab_ai, tab_rule, tab_run, tab_graph = st.tabs([
    "ãƒ•ã‚§ãƒ¼ã‚º 1: AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ (KMeans)",
    "ãƒ•ã‚§ãƒ¼ã‚º 2: åˆ†é¡ãƒ«ãƒ¼ãƒ«å®šç¾©",
    "ãƒ•ã‚§ãƒ¼ã‚º 3: åˆ†é¡å®Ÿè¡Œ",
    "ãƒ•ã‚§ãƒ¼ã‚º 4: ç‰¹è¨±ãƒãƒƒãƒ—ä½œæˆ"
])

# --- ãƒ•ã‚§ãƒ¼ã‚º 1: AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ ---
with tab_ai:
    st.subheader("ãƒ•ã‚§ãƒ¼ã‚º 1: AIã«ã‚ˆã‚‹åˆ†é¡ã‚µã‚¸ã‚§ã‚¹ãƒˆ (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)")
    st.markdown("K-Meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã«åŸºã¥ãã€åˆ†é¡ãƒ«ãƒ¼ãƒ«ä½œæˆã®ãŸã‚ã®AIãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚")
    
    col_map_options = [v for k, v in col_map.items() if k in ['title', 'abstract', 'claim']]
    target_column = st.selectbox(
        "åˆ†æå¯¾è±¡ã‚«ãƒ©ãƒ :",
        options=col_map_options,
        key="core_target_col"
    )
    
    col1, col2, col3 = st.columns(3)
    with col1:
        ai_k_w = st.number_input("ãƒˆãƒ”ãƒƒã‚¯æ•° (K):", min_value=2, value=8, key="core_k")
    with col2:
        ai_n_w = st.number_input("å„ãƒˆãƒ”ãƒƒã‚¯ã®ä»£è¡¨æ–‡çŒ®æ•° (N):", min_value=1, value=5, key="core_n")
    with col3:
        ai_cat_count_w = st.number_input("AIãŒç”Ÿæˆã™ã‚‹åˆ†é¡åã®æ•°:", min_value=1, value=6, key="core_cat_count")
        
    if st.button("AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ", key="core_run_ai"):
        if not target_column or target_column not in df_main.columns:
            st.error("ã‚¨ãƒ©ãƒ¼: åˆ†æå¯¾è±¡ã‚«ãƒ©ãƒ ã‚’æ­£ã—ãé¸æŠã—ã¦ãã ã•ã„ã€‚")
        else:
            try:
                k = int(ai_k_w)
                n = int(ai_n_w)
                cat_count = int(ai_cat_count_w)
                
                with st.spinner(f"K-Means (K={k}) ã¨ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° (N={n}) ã‚’å®Ÿè¡Œä¸­..."):
                    texts_raw = df_main[target_column].astype(str).fillna('')
                    tokenized_texts = texts_raw.apply(advanced_tokenize_core)
                    
                    vectorizer = TfidfVectorizer(min_df=1, max_df=0.9, token_pattern=r"(?u)\b\w+\b")
                    tfidf_matrix = vectorizer.fit_transform(tokenized_texts)
                    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
                    clusters = kmeans.fit_predict(tfidf_matrix)
                    centroids = kmeans.cluster_centers_
                    
                    sampled_abstracts = []
                    for cluster_id in range(k):
                        cluster_indices = np.where(clusters == cluster_id)[0]
                        if len(cluster_indices) == 0: continue
                        centroid = centroids[cluster_id]
                        distances = euclidean_distances(tfidf_matrix[cluster_indices], centroid.reshape(1, -1))
                        closest_indices_in_cluster = distances.flatten().argsort()[:n]
                        original_indices = cluster_indices[closest_indices_in_cluster]
                        sampled_abstracts.append(f"\n--- (AIã«ã‚ˆã‚‹æ¨å®š) ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id} ã®ä»£è¡¨æ–‡çŒ® ---")
                        for original_index in original_indices:
                            abstract_original = texts_raw.iloc[original_index]
                            abstract_processed = _core_text_preprocessor(abstract_original)
                            sampled_abstracts.append(f"ãƒ» {abstract_processed}")
                    
                    prompt_parts = [
                        "ã‚ãªãŸã¯å„ªç§€ãªç‰¹è¨±æƒ…å ±ã‚¹ãƒˆãƒ©ãƒ†ã‚¸ã‚¹ãƒˆã§ã™ã€‚",
                        "\n# ä¾é ¼å†…å®¹",
                        f"ä»¥ä¸‹ã®ã€Œä»£è¡¨æ–‡çŒ®ã‚µãƒ³ãƒ—ãƒ«ã€ã¯ã€ã‚ã‚‹ç‰¹è¨±æ¯é›†å›£ï¼ˆ{len(df_main)}ä»¶ï¼‰ã‚’K-Meansæ³•ã§{k}å€‹ã®ã‚¯ãƒ©ã‚¹ã‚¿ã«åˆ†é¡ã—ã€å„ã‚¯ãƒ©ã‚¹ã‚¿ã‹ã‚‰ä»£è¡¨çš„ãªæ–‡çŒ®ã®ã€Œ{target_column}ã€ã‚’{n}ä»¶ãšã¤æŠ½å‡ºã—ãŸã‚‚ã®ã§ã™ã€‚",
                        f"ã“ã®ç‰¹è¨±æ¯é›†å›£å…¨ä½“ã‚’ç¶²ç¾…çš„ã«åˆ†é¡ã™ã‚‹ãŸã‚ã®ã€**ã€ŒæŠ€è¡“åˆ†é¡ã€ã€Œèª²é¡Œåˆ†é¡ã€ã€Œè§£æ±ºæ‰‹æ®µåˆ†é¡ã€**ã®3ã¤ã®åˆ†é¡è»¸ã«ã¤ã„ã¦ã€**åˆ†é¡å®šç¾©**ï¼ˆåˆ†é¡åã€å®šç¾©ã€COREè«–ç†å¼ã®ã‚»ãƒƒãƒˆï¼‰ã‚’ãã‚Œãã‚Œ**{cat_count}å€‹**ãšã¤è¨­è¨ˆã—ã¦ãã ã•ã„ã€‚",
                        "\n# ã‚ãªãŸï¼ˆAIï¼‰ã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹",
                        "1. **ç†Ÿèª­:** ã¾ãšã€`# ä»£è¡¨æ–‡çŒ®ã‚µãƒ³ãƒ—ãƒ«` ã‚’**ã™ã¹ã¦**ç†Ÿèª­ã—ã€ã“ã®æŠ€è¡“åˆ†é‡ã®å…¨ä½“åƒï¼ˆã©ã®ã‚ˆã†ãªæŠ€è¡“ãƒˆãƒ”ãƒƒã‚¯ãŒã‚ã‚Šã€ã©ã®ã‚ˆã†ãªèª²é¡ŒãŒè­°è«–ã•ã‚Œã¦ã„ã‚‹ã‹ï¼‰ã‚’æŠŠæ¡ã—ã¾ã™ã€‚",
                        "2. **åˆ†é¡:** æ¬¡ã«ã€å„æ–‡çŒ®ã®æ–‡è„ˆã‹ã‚‰ã€æŠ€è¡“ã®ã€Œç›®çš„ï¼ˆèª²é¡Œï¼‰ã€ã¨ã€Œæ‰‹æ®µï¼ˆè§£æ±ºç­–ï¼‰ã€ã¨ã€Œæ ¸ã¨ãªã‚‹æŠ€è¡“è¦ç´ ã€ã‚’å¿ƒã®ä¸­ã§åˆ†é¡ã—ã¾ã™ã€‚",
                        "3. **ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é¸å®š:** å„åˆ†é¡è»¸ï¼ˆæŠ€è¡“ãƒ»èª²é¡Œãƒ»è§£æ±ºæ‰‹æ®µï¼‰ã«ãµã•ã‚ã—ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’é¸å®šã—ã¾ã™ã€‚",
                        "4. **ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ‹¡å¼µ:** ã€Œ**æœ€é‡è¦ãƒ«ãƒ¼ãƒ«**ã€ã«åŸºã¥ãã€é¸å®šã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®**é¡ç¾©èªã€é–¢é€£èªã€è¡¨è¨˜ã‚†ã‚Œï¼ˆã‚«ã‚¿ã‚«ãƒŠã€ã²ã‚‰ãŒãªã€æ¼¢å­—ï¼‰**ã‚’ã€ã‚ãªãŸã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ç¶²ç¾…çš„ã«æƒ³èµ·ã—ã¾ã™ã€‚",
                        "5. **è«–ç†å¼æ§‹ç¯‰:** ã“ã‚Œã‚‰ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç¾¤ã‚’ã€Œ**COREè«–ç†å¼æ–‡æ³•**ã€ã‚’é§†ä½¿ã—ã¦çµ„ã¿åˆã‚ã›ã€**ãƒã‚¤ã‚ºã«å¼·ãã€ã‹ã¤ç¶²ç¾…çš„ï¼ˆãƒ¢ãƒ¬ãŒå°‘ãªã„ï¼‰**ãªè«–ç†å¼ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚",
                        f"6. **å‡ºåŠ›:** æœ€å¾Œã«ã€ã€Œ### è‰¯ã„å‡ºåŠ›ä¾‹ã€ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å³å¯†ã«å¾“ã£ã¦ã€3ã¤ã®åˆ†é¡è»¸ã‚’ï¼ˆãã‚Œãã‚Œ{cat_count}å€‹ãšã¤ï¼‰ç”Ÿæˆã—ã¾ã™ã€‚",
                        "\n# COREè«–ç†å¼æ–‡æ³• (å³å®ˆ)",
                        "- `A + B` (OR): A ã¾ãŸã¯ B",
                        "- `A * B` (AND): A ã‹ã¤ B (é †åºå•ã‚ãš)",
                        "- `A nearN B` (è¿‘å‚): Aã¨BãŒ**Næ–‡å­—**ä»¥å†…ã§å‡ºç¾ (é †åºå•ã‚ãš)ã€‚Nã¯10ã€œ40ç¨‹åº¦ã‚’æ¨å¥¨ã€‚",
                        "- `A adjN B` (é †åºæŒ‡å®šè¿‘å‚): AãŒBã®**Næ–‡å­—**ä»¥å†…ã«Aâ†’Bã®é †ã§å‡ºç¾ã€‚Nã¯1ã€œ10ç¨‹åº¦ã‚’æ¨å¥¨ã€‚",
                        "- **é‡è¦:** ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯ã‚¹ãƒšãƒ¼ã‚¹ã‚’å«ã¾ãªã„å˜ä¸€èªï¼ˆä¾‹: `äºŒé…¸åŒ–ç‚­ç´ `ï¼‰ã«ã—ã¦ãã ã•ã„ã€‚ã‚¹ãƒšãƒ¼ã‚¹ã‚’å«ã‚€ãƒ•ãƒ¬ãƒ¼ã‚ºï¼ˆä¾‹: `AI agent`ï¼‰ã¯ã€`AI adj1 agent` ã®ã‚ˆã†ã«æ¼”ç®—å­ã§è¡¨ç¾ã—ã¦ãã ã•ã„ã€‚",
                        "\n# æœ€é‡è¦ãƒ«ãƒ¼ãƒ« (ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ‹¡å¼µã¨è¡¨è¨˜ã‚†ã‚Œ)",
                        "- ã‚µãƒ³ãƒ—ãƒ«ã«å­˜åœ¨ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãã®ã¾ã¾ä½¿ã†ã ã‘ã§ã¯ä¸ååˆ†ã§ã™ã€‚",
                        "- AIã®çŸ¥è­˜ã‚’æ´»ç”¨ã—ã€ãã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®**é¡ç¾©èªã€é–¢é€£èªã€ä¸Šä½/ä¸‹ä½æ¦‚å¿µ**ã‚’æƒ³èµ·ã—ã¦ãã ã•ã„ã€‚",
                        "- **ç‰¹ã«ã€ã‚«ã‚¿ã‚«ãƒŠï¼ˆä¾‹: `ãƒãƒªãƒãƒ¼`ï¼‰ã€ã²ã‚‰ãŒãªï¼ˆä¾‹: `ã°ã­`ï¼‰ã€æ¼¢å­—ï¼ˆä¾‹: `æ¨¹è„‚`ï¼‰**ã¨ã„ã£ãŸ**æ—¥æœ¬èªã®**è¡¨è¨˜ã‚†ã‚Œã‚’ `+` æ¼”ç®—å­ã§ç¶²ç¾…ã—ã¦ãã ã•ã„ã€‚",
                        "- **æ³¨æ„:** è«–ç†å¼ã«è‹±èªï¼ˆè‹±å˜èªï¼‰ã¯å«ã‚ãšã€æ—¥æœ¬èªï¼ˆæ¼¢å­—ã€ã‚«ã‚¿ã‚«ãƒŠã€ã²ã‚‰ãŒãªï¼‰ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚",
                        "- **ã‚«ã‚¿ã‚«ãƒŠ:** ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ã‚«ã‚¿ã‚«ãƒŠã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€**å¿…ãšå…¨è§’ï¼ˆä¾‹: `ãƒãƒªãƒãƒ¼`ï¼‰**ã‚’ä½¿ç”¨ã—ã€**åŠè§’ï¼ˆä¾‹: `ï¾ï¾Ÿï¾˜ï¾ï½°`ï¼‰ã¯çµ¶å¯¾ã«ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„**ã€‚",
                        "\n### è‰¯ã„å‡ºåŠ›ä¾‹",
                        "```",
                        "## æŠ€è¡“åˆ†é¡",
                        "1.  **CO2åˆ†é›¢è†œ**",
                        "    * **å®šç¾©:** CO2ã‚’åˆ†é›¢ãƒ»å›åã™ã‚‹ãŸã‚ã®è†œï¼ˆä¸­ç©ºç³¸è†œã€é«˜åˆ†å­è†œãªã©ï¼‰ã«é–¢é€£ã™ã‚‹æŠ€è¡“ã€‚",
                        "    * **è«–ç†å¼:** (CO2 + äºŒé…¸åŒ–ç‚­ç´  + ç‚­é…¸ã‚¬ã‚¹) * (è†œ + åˆ†é›¢è†œ + ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ + ä¸­ç©ºç³¸)",
                        "2.  **ã‚¢ãƒŸãƒ³å¸åæ¶²**",
                        "    * **å®šç¾©:** ã‚¢ãƒŸãƒ³åŒ–åˆç‰©ï¼ˆMEA, MDEAç­‰ï¼‰ã‚’ç”¨ã„ãŸåŒ–å­¦å¸åæ¶²ã«ã‚ˆã‚‹CO2å›åæŠ€è¡“ã€‚",
                        "    * **è«–ç†å¼:** (ã‚¢ãƒŸãƒ³ + å¸åæ¶²) + (MEA + MDEA + ãƒ¢ãƒã‚¨ã‚¿ãƒãƒ¼ãƒ«ã‚¢ãƒŸãƒ³)",
                        "\n## èª²é¡Œåˆ†é¡",
                        "1.  **è€ä¹…æ€§ã®å‘ä¸Š**",
                        "    * **å®šç¾©:** è†œã‚„å¸åæ¶²ã®åŠ£åŒ–ã‚’æŠ‘åˆ¶ã—ã€é•·æœŸé–“å®‰å®šã—ã¦ä½¿ç”¨å¯èƒ½ã«ã™ã‚‹ã“ã¨ã€‚",
                        "    * **è«–ç†å¼:** (è€ä¹…æ€§ +ä¿¡é ¼æ€§ + åŠ£åŒ– + å¯¿å‘½ + å®‰å®šæ€§) * (å‘ä¸Š + æ”¹å–„ + æŠ‘åˆ¶ + é«˜ã‚ã‚‹)",
                        "2.  **ã‚³ã‚¹ãƒˆã®å‰Šæ¸›**",
                        "    * **å®šç¾©:** è£½é€ ã‚³ã‚¹ãƒˆã‚„é‹ç”¨ã‚³ã‚¹ãƒˆã‚’ä½æ¸›ã—ã€çµŒæ¸ˆæ€§ã‚’é«˜ã‚ã‚‹ã“ã¨ã€‚",
                        "    * **è«–ç†å¼:** (ã‚³ã‚¹ãƒˆ + è£½é€ è²»ç”¨ + å®‰ä¾¡ + ä½å»‰ + çµŒæ¸ˆæ€§) * (å‰Šæ¸› + ä½æ¸› + å®‰ã)",
                        "\n## è§£æ±ºæ‰‹æ®µåˆ†é¡",
                        "1.  **å¤šå­”è³ªæ‹…ä½“ã®åˆ©ç”¨**",
                        "    * **å®šç¾©:** ã‚¼ã‚ªãƒ©ã‚¤ãƒˆã€MOFã€æ´»æ€§ç‚­ãªã©ã®å¤šå­”è³ªãªæ‹…ä½“ã«æ©Ÿèƒ½æ€§ææ–™ã‚’æ‹…æŒã•ã›ã‚‹æ‰‹æ³•ã€‚",
                        "    * **è«–ç†å¼:** (å¤šå­”è³ª + ãƒãƒ¼ãƒ©ã‚¹ + æ‹…ä½“ + ç´°å­”) + (ã‚¼ã‚ªãƒ©ã‚¤ãƒˆ + MOF + æ´»æ€§ç‚­)",
                        "2.  **æ–°è¦ã‚¢ãƒŸãƒ³ã®æ·»åŠ **",
                        "    * **å®šç¾©:** æ—¢å­˜ã®ã‚¢ãƒŸãƒ³å¸åæ¶²ã«ã€æ€§èƒ½å‘ä¸Šã®ãŸã‚ã®æ–°è¦ã‚¢ãƒŸãƒ³åŒ–åˆç‰©ã‚’æ·»åŠ ã™ã‚‹æ‰‹æ³•ã€‚",
                        "    * **è«–ç†å¼:** (ã‚¢ãƒŸãƒ³ + æº¶å‰¤) adj10 (æ–°è¦ + æ·»åŠ  + æ··åˆ + é–‹ç™º)",
                        "```",
                        "\n# ä»£è¡¨æ–‡çŒ®ã‚µãƒ³ãƒ—ãƒ«",
                        "\n".join(sampled_abstracts)
                    ]
                    final_prompt = "\n".join(prompt_parts)
                    
                    st.success("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
                    st.text_area("ä»¥ä¸‹ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼ã—ã€AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã«è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„ã€‚", final_prompt, height=400)

            except Exception as e:
                st.error(f"åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                import traceback
                st.exception(traceback.format_exc())

# --- ãƒ•ã‚§ãƒ¼ã‚º 2: åˆ†é¡ãƒ«ãƒ¼ãƒ«å®šç¾© ---
with tab_rule:
    st.subheader("ãƒ•ã‚§ãƒ¼ã‚º 2: åˆ†é¡ãƒ«ãƒ¼ãƒ«å®šç¾©")
    st.markdown("AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å‡ºåŠ›ã‚’å‚è€ƒã«ã€åˆ†é¡ãƒ«ãƒ¼ãƒ«ã‚’å®šç¾©ã—ã¾ã™ã€‚")
    
    axis_name_text = st.text_input("åˆ†é¡è»¸ã®åå‰:", placeholder="ä¾‹: èª²é¡Œã€è§£æ±ºæ‰‹æ®µã€æŠ€è¡“è¦ç´ ãªã©", key="core_axis_name")
    category_name_text = st.text_input("åˆ†é¡å:", placeholder="ä¾‹: è€ä¹…æ€§ã€ã‚³ã‚¹ãƒˆãƒ€ã‚¦ãƒ³ãªã©", key="core_category_name")

    st.markdown("""
    <b>è«–ç†å¼æ–‡æ³• (N = æ–‡å­—æ•°)</b>
    <ul>
        <li><b><code>A + B</code></b> (OR): A ã¾ãŸã¯ B</li>
        <li><b><code>A * B</code></b> (AND): A ã‹ã¤ B (é †åºå•ã‚ãš)</li>
        <li><b><code>A nearN B</code></b> (è¿‘å‚): Aã¨BãŒ<b>Næ–‡å­—</b>ä»¥å†…ã§å‡ºç¾ (é †åºå•ã‚ãš)</li>
        <li><b><code>A adjN B</code></b> (é †åºæŒ‡å®šè¿‘å‚): AãŒBã®<b>Næ–‡å­—</b>ä»¥å†…ã«Aâ†’Bã®é †ã§å‡ºç¾</li>
        <li><b><code>( )</code></b> (æ‹¬å¼§): æ¼”ç®—ã®å„ªå…ˆé †ä½ã‚’æŒ‡å®š</li>
    </ul>
    """, unsafe_allow_html=True) 

    keywords_text = st.text_input("è«–ç†å¼:", placeholder="ä¾‹: (æ¨¹è„‚ + ãƒãƒªãƒãƒ¼) * (é«˜å¼·åº¦ near50 è€ä¹…æ€§)", key="core_keywords")
    
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ã“ã®åˆ†é¡ãƒ«ãƒ¼ãƒ«ã‚’è¿½åŠ ", key="core_add_rule"):
            axis_name = axis_name_text
            category_name = category_name_text
            rule_str = keywords_text
            
            if not all([axis_name, category_name, rule_str]):
                st.warning("ã€Œåˆ†é¡è»¸ã®åå‰ã€ã€Œåˆ†é¡åã€ã€Œè«–ç†å¼ã€ã®ã™ã¹ã¦ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            else:
                try:
                    or_clauses_str = split_by_operator(rule_str, '+')
                    compiled_or_clauses = []
                    
                    for or_part_str in or_clauses_str:
                        and_clauses_str = split_by_operator(or_part_str, '*')
                        compiled_and_clauses = []
                        
                        for and_part_str in and_clauses_str:
                            sub_rule = and_part_str.strip()
                            if not sub_rule:
                                raise ValueError("'*' ã¾ãŸã¯ '+' æ¼”ç®—å­ã®é–“ã«ç©ºã®ãƒ«ãƒ¼ãƒ«ãŒã‚ã‚Šã¾ã™ã€‚")
                            compiled_and_clauses.append(parse_core_rule(sub_rule))
                        
                        compiled_or_clauses.append(compiled_and_clauses)
                    
                    if axis_name not in st.session_state.core_classification_rules:
                        st.session_state.core_classification_rules[axis_name] = {}
                    
                    st.session_state.core_classification_rules[axis_name][category_name] = (rule_str, compiled_or_clauses)
                    st.success(f"[è»¸: {axis_name}] ã‚«ãƒ†ã‚´ãƒª '{category_name}' ã«è«–ç†å¼ '{rule_str}' ã‚’ç™»éŒ²ã—ã¾ã—ãŸã€‚")
                    st.session_state.core_current_axis = axis_name
                except Exception as e:
                    st.error(f"æ–‡æ³•ã‚¨ãƒ©ãƒ¼: {e}")

    with col2:
        if st.button("ã“ã®åˆ†é¡è»¸ã®å®šç¾©ã‚’å®Œäº† (æ¬¡ã®è»¸ã¸)", key="core_finish_axis"):
            axis_name = st.session_state.core_current_axis
            if not axis_name or axis_name not in st.session_state.core_classification_rules:
                st.warning(f"è»¸ '{axis_name}' ã«ãƒ«ãƒ¼ãƒ«ãŒ1ã¤ã‚‚ç™»éŒ²ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            else:
                st.success(f"åˆ†é¡è»¸ '{axis_name}' ã®å®šç¾©ã‚’å®Œäº†ã—ã¾ã—ãŸã€‚")
                st.session_state.core_current_axis = "" # ã‚¯ãƒªã‚¢

    st.markdown("---")
    st.subheader("å®šç¾©æ¸ˆã¿ãƒ«ãƒ¼ãƒ«ãƒ­ã‚°")
    if not st.session_state.core_classification_rules:
        st.info("ã¾ã ãƒ«ãƒ¼ãƒ«ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    else:
        for axis, rules in st.session_state.core_classification_rules.items():
            st.markdown(f"**è»¸: {axis}**")
            for category, (rule_str, _) in rules.items():
                st.code(f"  - {category}: {rule_str}", language="text")

# --- ãƒ•ã‚§ãƒ¼ã‚º 3: åˆ†é¡å®Ÿè¡Œ ---
with tab_run:
    st.subheader("ãƒ•ã‚§ãƒ¼ã‚º 3: åˆ†é¡å®Ÿè¡Œ")
    st.markdown("å®šç¾©ã—ãŸã™ã¹ã¦ã®åˆ†é¡ãƒ«ãƒ¼ãƒ«ã‚’å®Ÿè¡Œã—ã€ç‰¹è¨±ãƒªã‚¹ãƒˆã«åˆ†é¡ã‚’ä»˜ä¸ã—ã¾ã™ã€‚")
    
    if st.button("ã™ã¹ã¦ã®åˆ†é¡ã‚’å®Ÿè¡Œ", type="primary", key="core_run_classification"):
        if not st.session_state.core_classification_rules:
            st.error("ã‚¨ãƒ©ãƒ¼: ã€Œãƒ•ã‚§ãƒ¼ã‚º 2ã€ã§åˆ†é¡ãƒ«ãƒ¼ãƒ«ã‚’1ã¤ä»¥ä¸Šå®šç¾©ã—ã¦ãã ã•ã„ã€‚")
        elif not target_column or target_column not in df_main.columns:
            st.error("ã‚¨ãƒ©ãƒ¼: ã€Œãƒ•ã‚§ãƒ¼ã‚º 1ã€ã§åˆ†æå¯¾è±¡ã‚«ãƒ©ãƒ ã‚’æ­£ã—ãé¸æŠã—ã¦ãã ã•ã„ã€‚")
        else:
            with st.spinner("åˆ†é¡å‡¦ç†ã‚’å®Ÿè¡Œä¸­..."):
                try:
                    df_classified = df_main.copy()
                    
                    status_area = st.empty()
                    progress_bar = st.progress(0, "åˆ†é¡å‡¦ç†ä¸­...")
                    
                    rules = st.session_state.core_classification_rules
                    total_axes = len(rules)
                    
                    for i, (axis_name, ruleset) in enumerate(rules.items()):
                        status_area.write(f"è»¸ '{axis_name}' ã®åˆ†é¡å‡¦ç†ä¸­... ({i+1}/{total_axes})")
                        df_classified[axis_name] = ''
                        
                        target_texts = df_classified[target_column].astype(str).fillna("")
                        
                        def apply_rules_for_axis(search_text):
                            search_text_processed = _core_text_preprocessor(search_text)
                            found_categories = []
                            
                            for category, (rule_str, compiled_or_clauses) in ruleset.items():
                                
                                is_or_match = False
                                for compiled_and_clauses in compiled_or_clauses:
                                    
                                    is_and_match = True
                                    for sub_regex in compiled_and_clauses:
                                        if not sub_regex.search(search_text_processed): 
                                            is_and_match = False
                                            break 
                                    
                                    if is_and_match:
                                        is_or_match = True
                                        break
                                
                                if is_or_match:
                                    found_categories.append(category)

                            if found_categories:
                                return ";".join(found_categories)
                            else:
                                return 'ãã®ä»–'
                                
                        df_classified[axis_name] = target_texts.apply(apply_rules_for_axis)
                        progress_bar.progress((i + 1) / total_axes)

                    st.session_state.core_df_classified = df_classified.copy()
                    
                    status_area.empty()
                    progress_bar.empty()
                    st.success("ã™ã¹ã¦ã®åˆ†é¡ä»˜ä¸ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
                    
                    st.subheader("åˆ†é¡çµæœã‚µãƒãƒªãƒ¼")
                    total_docs = len(df_classified)
                    st.write(f"ç·å…¬å ±æ•°: {total_docs}ä»¶")
                    summary_text = []
                    for axis_name in rules.keys():
                        summary_text.append(f"\n--- è»¸: [{axis_name}] ---")
                        for category_name in rules[axis_name].keys():
                            count = df_classified[axis_name].str.contains(re.escape(category_name), na=False, regex=True).sum()
                            summary_text.append(f"  {category_name}: {count}ä»¶")
                        other_count = (df_classified[axis_name] == 'ãã®ä»–').sum()
                        summary_text.append(f"  ãã®ä»–: {other_count}ä»¶")
                    st.code("\n".join(summary_text), language="text")
                    
                    st.subheader("å‡¦ç†çµæœã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
                    preview_cols = list(rules.keys()) + [target_column]
                    st.dataframe(df_classified[preview_cols].head())
                    
                    @st.cache_data
                    def convert_df_to_csv_core(df):
                        return df.to_csv(encoding='utf-8-sig').encode('utf-8-sig')
                    
                    csv_core = convert_df_to_csv_core(df_classified)
                    st.download_button(
                        label="åˆ†é¡çµæœ (CORE_classified_output.csv) ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=csv_core,
                        file_name="CORE_classified_output.csv",
                        mime="text/csv",
                    )
                    
                except Exception as e:
                    st.error(f"åˆ†é¡å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                    import traceback
                    st.exception(traceback.format_exc())

# --- ãƒ•ã‚§ãƒ¼ã‚º 4: ç‰¹è¨±ãƒãƒƒãƒ—ä½œæˆ ---
with tab_graph:
    st.subheader("ãƒ•ã‚§ãƒ¼ã‚º 4: ç‰¹è¨±ãƒãƒƒãƒ—ä½œæˆ (ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—)")
    
    st.markdown("ã€Œãƒ•ã‚§ãƒ¼ã‚º 3ã€ã§åˆ†é¡ä»˜ä¸ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’å¯¾è±¡ã«ã€2è»¸ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’ä½œæˆã—ã¾ã™ã€‚")
    st.markdown("---")
    
    if st.session_state.core_df_classified is None:
        st.info("å…ˆã«ã€Œãƒ•ã‚§ãƒ¼ã‚º 3: åˆ†é¡å®Ÿè¡Œã€ã‚¿ãƒ–ã§åˆ†é¡ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    else:
        df_graph = st.session_state.core_df_classified
        
        st.subheader("ãƒãƒƒãƒ—è¨­å®š")
        
        # 1. è»¸ã®é¸æŠè‚¢ã‚’æº–å‚™
        core_axes = list(st.session_state.core_classification_rules.keys())
        app_py_axes = []
        
        if 'year' in df_graph.columns:
            app_py_axes.append("å‡ºé¡˜å¹´")
        
        if col_map.get('applicant') and col_map['applicant'] in df_graph.columns:
            app_py_axes.append("å‡ºé¡˜äºº")
        
        all_axis_options = core_axes + app_py_axes
        
        if len(all_axis_options) < 2:
            st.error("ã‚¨ãƒ©ãƒ¼: ã‚°ãƒ©ãƒ•åŒ–ã§ãã‚‹è»¸ï¼ˆåˆ†é¡è»¸ã€å‡ºé¡˜å¹´ã€å‡ºé¡˜äººã®ã†ã¡2ã¤ä»¥ä¸Šï¼‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            st.stop()
            
        # 2. UIã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã‚’å®šç¾©
        col1, col2 = st.columns(2)
        with col1:
            x_axis_name = st.selectbox(
                "Xè»¸:", 
                all_axis_options, 
                key="core_x_axis",
                index = min(0, len(all_axis_options)-1) 
            )
            x_top_n = st.number_input(
                "Xè»¸ è¡¨ç¤ºä»¶æ•° (Top N):", 
                min_value=1, 
                value=20, 
                key="core_x_top_n",
                help="ã€Œå‡ºé¡˜å¹´ã€ã‚’è»¸ã«ã—ãŸå ´åˆã¯ã€ã“ã®è¨­å®šã¯ç„¡è¦–ã•ã‚Œã¾ã™ã€‚"
            )
            x_exclude_other_w = st.checkbox("Xè»¸ã‹ã‚‰ã€Œãã®ä»–ã€ã‚’é™¤å¤–", value=False, key="core_x_exclude_other")
            
        with col2:
            y_axis_name = st.selectbox(
                "Yè»¸:", 
                all_axis_options, 
                key="core_y_axis",
                index= min(1, len(all_axis_options)-1) 
            )
            y_top_n = st.number_input(
                "Yè»¸ è¡¨ç¤ºä»¶æ•° (Top N):", 
                min_value=1, 
                value=20, 
                key="core_y_top_n",
                help="ã€Œå‡ºé¡˜å¹´ã€ã‚’è»¸ã«ã—ãŸå ´åˆã¯ã€ã“ã®è¨­å®šã¯ç„¡è¦–ã•ã‚Œã¾ã™ã€‚"
            )
            y_exclude_other_w = st.checkbox("Yè»¸ã‹ã‚‰ã€Œãã®ä»–ã€ã‚’é™¤å¤–", value=False, key="core_y_exclude_other")
        
        delimiter_w = st.text_input("åŒºåˆ‡ã‚Šæ–‡å­— (åˆ†é¡è»¸ãƒ»å‡ºé¡˜äººç”¨):", value=';', key="core_delimiter")

        x_is_year = (st.session_state.core_x_axis == "å‡ºé¡˜å¹´")
        y_is_year = (st.session_state.core_y_axis == "å‡ºé¡˜å¹´")
        
        if x_is_year or y_is_year:
            st.markdown("---")
            st.subheader("æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿è¨­å®š")
            st.info("Xè»¸ã¾ãŸã¯Yè»¸ã«ã€Œå‡ºé¡˜å¹´ã€ãŒé¸æŠã•ã‚ŒãŸãŸã‚ã€ä»¥ä¸‹ã®æœŸé–“ã§ãƒ‡ãƒ¼ã‚¿ã‚’çµã‚Šè¾¼ã¿ã¾ã™ã€‚")
            
            def callback_autoset_core_year():
                if 'year' in df_graph.columns and df_graph['year'].notna().any():
                    valid_years = df_graph['year'].dropna().astype(int)
                    st.session_state.core_start_year = int(valid_years.min())
                    st.session_state.core_end_year = int(valid_years.max())
                else:
                    st.session_state.core_start_year = 2010
                    st.session_state.core_end_year = 2024
            
            if 'core_start_year' not in st.session_state:
                callback_autoset_core_year()

            d_col1, d_col2, d_col3 = st.columns([1, 1, 2])
            with d_col1:
                st.number_input("é–‹å§‹å¹´:", key="core_start_year", step=1)
            with d_col2:
                st.number_input("çµ‚äº†å¹´:", key="core_end_year", step=1)
            with d_col3:
                st.button("ï¼ˆå…¨æœŸé–“ã‚’è‡ªå‹•è¨­å®šï¼‰", on_click=callback_autoset_core_year, key="core_autoset_year")
        
        st.markdown("---")
        st.subheader("ãƒãƒƒãƒ—ã®å®Ÿè¡Œã¨è¡¨ç¤º")
        
        if st.button("5. ç‰¹è¨±ãƒãƒƒãƒ—ã‚’ä½œæˆ", type="primary", key="core_run_graph"):
            
            x_axis_key = st.session_state.core_x_axis
            y_axis_key = st.session_state.core_y_axis
            
            if x_axis_key == y_axis_key:
                st.error("ã‚¨ãƒ©ãƒ¼: ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§ã¯Xè»¸ã¨Yè»¸ã«ç•°ãªã‚‹ã‚«ãƒ©ãƒ ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
            else:
                with st.spinner("ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’ä½œæˆä¸­..."):
                    try:
                        delimiter = delimiter_w.strip()
                        
                        df_filtered = df_graph.copy()
                        if x_is_year or y_is_year:
                            start_year_val = int(st.session_state.core_start_year)
                            end_year_val = int(st.session_state.core_end_year)
                            df_filtered = df_filtered[
                                (df_filtered['year'].notna()) &
                                (df_filtered['year'] >= start_year_val) & 
                                (df_filtered['year'] <= end_year_val)
                            ]

                        if x_axis_key == "å‡ºé¡˜å¹´":
                            x_col_name = 'year'
                            x_delimiter = None
                        elif x_axis_key == "å‡ºé¡˜äºº":
                            x_col_name = col_map['applicant']
                            x_delimiter = delimiter
                        else:
                            x_col_name = x_axis_key 
                            x_delimiter = delimiter

                        if y_axis_key == "å‡ºé¡˜å¹´":
                            y_col_name = 'year'
                            y_delimiter = None
                        elif y_axis_key == "å‡ºé¡˜äºº":
                            y_col_name = col_map['applicant']
                            y_delimiter = delimiter
                        else:
                            y_col_name = y_axis_key
                            y_delimiter = delimiter

                        df_plot_x = prepare_axis_data_core(df_filtered, x_col_name, x_delimiter)
                        if df_plot_x.empty: st.stop()
                        
                        df_plot_xy = prepare_axis_data_core(df_plot_x, y_col_name, y_delimiter)
                        if df_plot_xy.empty: st.stop()
                        
                        if st.session_state.core_x_exclude_other:
                            df_plot_xy = df_plot_xy[df_plot_xy[x_col_name] != 'ãã®ä»–']
                        
                        if st.session_state.core_y_exclude_other:
                            df_plot_xy = df_plot_xy[df_plot_xy[y_col_name] != 'ãã®ä»–']
                        
                        x_top_n_val = int(st.session_state.core_x_top_n)
                        y_top_n_val = int(st.session_state.core_y_top_n)

                        if x_axis_key != "å‡ºé¡˜å¹´":
                            x_top_labels = df_plot_xy[
                                (df_plot_xy[x_col_name] != 'N/A') & 
                                (df_plot_xy[x_col_name] != 'ãã®ä»–')
                            ][x_col_name].value_counts().head(x_top_n_val).index.tolist()
                            
                            x_allowed_labels = x_top_labels + ['N/A']
                            if not st.session_state.core_x_exclude_other:
                                x_allowed_labels.append('ãã®ä»–')
                                
                            df_plot_xy = df_plot_xy[df_plot_xy[x_col_name].isin(x_allowed_labels)]
                        
                        if y_axis_key != "å‡ºé¡˜å¹´":
                            y_top_labels = df_plot_xy[
                                (df_plot_xy[y_col_name] != 'N/A') & 
                                (df_plot_xy[y_col_name] != 'ãã®ä»–')
                            ][y_col_name].value_counts().head(y_top_n_val).index.tolist()
                            
                            y_allowed_labels = y_top_labels + ['N/A']
                            if not st.session_state.core_y_exclude_other:
                                y_allowed_labels.append('ãã®ä»–')

                            df_plot_xy = df_plot_xy[df_plot_xy[y_col_name].isin(y_allowed_labels)]

                        df_plot_final = df_plot_xy[
                            (df_plot_xy[x_col_name] != 'N/A') & 
                            (df_plot_xy[y_col_name] != 'N/A')
                        ]
                        
                        if df_plot_final.empty:
                            st.warning("è©²å½“ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ï¼ˆTop N ãƒ•ã‚£ãƒ«ã‚¿ã‚„ N/A é™¤å¤–ã®çµæœã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ï¼‰")
                        else:
                            matrix = pd.crosstab(df_plot_final[y_col_name], df_plot_final[x_col_name])
                            
                            if x_axis_key == "å‡ºé¡˜å¹´":
                                x_category_order = sorted(matrix.columns.astype(int))
                            else:
                                x_category_order = matrix.sum(axis=0).sort_values(ascending=False).index.tolist()
                            
                            if y_axis_key == "å‡ºé¡˜å¹´":
                                y_category_order = sorted(matrix.index.astype(int))
                            else:
                                y_category_order = matrix.sum(axis=1).sort_values(ascending=False).index.tolist()

                            cell_size_px = 35 
                            x_label_padding = 150 
                            y_label_padding = 200 
                            
                            fig_height = max(400, len(matrix.index) * cell_size_px + x_label_padding)
                            fig_width = max(600, len(matrix.columns) * cell_size_px + y_label_padding)

                            fig = px.imshow(matrix, 
                                            text_auto=True, 
                                            title=f"'{y_axis_key}' Ã— '{x_axis_key}' ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—",
                                            aspect=None,
                                            color_continuous_scale='YlGnBu',
                                            height=fig_height, 
                                            width=fig_width   
                                           )
                            fig.update_layout(
                                xaxis_title=x_axis_key,
                                yaxis_title=y_axis_key,
                                xaxis_tickangle=-90,
                                xaxis={'categoryarray': x_category_order},
                                yaxis={'categoryarray': y_category_order, 'autorange': 'reversed'}
                            )
                            
                            st.plotly_chart(fig, use_container_width=False)

                    except Exception as e:
                        st.error(f"ã‚°ãƒ©ãƒ•ä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                        import traceback
                        st.exception(traceback.format_exc())

# --- å…±é€šã‚µã‚¤ãƒ‰ãƒãƒ¼ãƒ•ãƒƒã‚¿ãƒ¼ ---
st.sidebar.markdown("---") 
st.sidebar.caption("ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³:")
st.sidebar.caption("1. Mission Control ã§ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€å‰å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
st.sidebar.caption("2. å·¦ã®ãƒªã‚¹ãƒˆã‹ã‚‰åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é¸æŠã—ã¾ã™ã€‚")
st.sidebar.markdown("---")
st.sidebar.caption("Â© 2025 ã—ã°ã‚„ã¾")